#!/usr/bin/env Rscript
#' Decompose treatment effect heterogeneity by image
#'
#' Implements the image heterogeneity decomposition analysis of Jerzak, Johansson, and Daoud (2023). Users
#' input in treatment and outcome data, along with a function specifying how to load in images
#' using keys referenced to each unit (since loading in all image data will usually not be possible due to memory limitations).
#' This function by default performs estimation, constructs salience maps, and can optionally perform
#' estimation for new areas outside the original study sites in a transportability analysis.
#'
#' @usage
#'
#' AnalyzeImageHeterogeneity(obsW, obsY, imageKeysOfUnits, acquireImageFxn, kClust_est, ...)
#'
#' @param obsW A numeric vector where `0`'s correspond to control units and `1`'s to treated units.
#' @param obsY A numeric vector containing observed outcomes.
#' @param kClust_est (default = `2L`) Integer specifying the number of clusters used in estimation.
#' @param file (default = `NULL`) Path to a tfrecord file generated by `WriteTfRecord`.
#' @param acquireImageFxn A function specifying how to load images representations associated with `imageKeysOfUnits` into memory. For example, if observation `3` has a value  of `"a34f"` in `imageKeysOfUnits`, `acquireImageFxn` should extract the image associated with the unique key `"a34f"`.
#' First argument should be image key values and second argument have be `training` (in case of behavior change in training/inference).
#' @param transportabilityMat (optional) A matrix with a column named `key` specifying keys to be used by `acquireImageFxn` for generating treatment effect predictions for out-of-sample points in earth observation data settings.
#' @param imageKeysOfUnits (default = `1:length(obsY)`) A vector of length `length(obsY)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into `acquireImageFxn` to call images into memory.
#' @param long,lat (optional) Vectors specifying longitude and latitude coordinates for units. Used only for describing highest and lowest probability neighorhood units if specified.
#' @param X (optional) A numeric matrix containing tabular information used if `orthogonalize = T`.
#' @param conda_env (default = `NULL`) A string specifying a conda environment wherein `tensorflow`, `tensorflow_probability`, and `gc` are installed.
#' @param conda_env_required (default = `F`) A Boolean stating whether use of the specified conda environment is required.
#' @param orthogonalize (default = `F`) A Boolean specifying whether to perform the image decomposition after orthogonalizing with respect to tabular covariates specified in `X`.
#' @param nMonte_variational (default = `5L`) An integer specifying how many Monte Carlo iterations to use in the
#' calculation of the expected likelihood in each training step.
#' @param nMonte_predictive (default = `20L`) An integer specifying how many Monte Carlo iterations to use in the calculation
#' of posterior means (e.g., mean cluster probabilities).
#' @param nMonte_salience (default = `100L`) An integer specifying how many Monte Carlo iterations to use in the calculation
#' of the salience maps (e.g., image gradients of expected cluster probabilities).
#' @param reparameterizationType (default = `"Flipout"`) Either `"Flipout"`, or `"Reparameterization"`. Specifies the estimator used in the Bayesian neural components. With `"Flipout"`, convolutions are performed via CPU; with `"Reparameterization", they are performed by GPU if available.
#' @param figuresTag (default = `""`) A string specifying an identifier that is appended to all figure names.
#' @param figuresPath (default = `"./"`) A string specifying file path for saved figures made in the analysis.
#' @param kernelSize (default = `5L`) Dimensions used in convolution kernels.
#' @param nSGD (default = `400L`) Number of stochastic gradient descent (SGD) iterations.
#' @param batchSize (default = `25L`) Batch size used in SGD optimization.
#' @param doConvLowerDimProj (default = `T`) Should we project the `nFilters` convolutional feature dimensions down to `nDimLowerDimConv` to reduce the number of required parameters.
#' @param nDimLowerDimConv (default = `3L`) If `doConvLowerDimProj = T`, then, in each convolutional layer, we project the `nFilters` feature dimensions down to `nDimLowerDimConv` to reduce the number of parameters needed.
#' @param nFilters (default = `32L`) Integer specifying the number of convolutional filters used.
#' @param nDenseWidth (default = `32L`) Width of dense projection layers post-convolutions.
#' @param nDepthHidden_conv (default = `3L`) Hidden depth of convolutional layer.
#' @param nDepthHidden_dense (default = `0L`) Hidden depth of dense layers. Default of `0L` means a single projection layer is performed after the convolutional layer (i.e., no hidden layers are used).
#' @param quiet (default = `F`) Should we suppress information about progress?
#' @param yDensity (default = `normal`) Specifies the density for the outcome. Current options include `normal` and `lognormal`.
#' @param maxPoolSize (default = `2L`) Integer specifying the max pooling size used in the convolutional layers.
#' @param strides (default = `2L`) Integer specifying the strides used in the convolutional layers.=
#' @param simMode (default = `F`) Should the analysis be performed in comparison with ground truth from simulation?
#' @param plotResults (default = `T`) Should analysis results be plotted?
#' @param plotBands (default = `1L`) An integer or vector specifying which band position (from the acquired image representation) should be plotted in the visual results. If a vector, `plotBands` should have 3 (and only 3) dimensions (corresponding to the 3 dimensions to be used in RBG plotting).
#' @param channelNormalize (default = `T`) Should channelwise image feature normalization be attempted? Default is `T`, as this improves training.
#'
#' @return A list consiting of \itemize{
#'   \item `clusterTaus_mean` default
#'   \item `clusterTaus_sd` Estimated image effect cluster standard deviations.
#'   \item `clusterProbs_mean`. Estimated mean image effect cluster probabilities.
#'   \item `clusterTaus_sd`. Estimated image effect cluster probability standard deviations.
#'   \item `clusterProbs_lowerConf`. Estimated lower confidence for effect cluster probabilities.
#'   \item `impliedATE`. Implied ATE.
#'   \item `individualTau_est`. Estimated individual-level image-based treatment effects.
#'   \item `transportabilityMat`. Transportability matrix withestimated cluster information.
#'   \item `plottedCoordinates`. List containing coordinates plotted in salience maps.
#'   \item `whichNA_dropped`. A vector containing observations dropped due to missingness.
#' }
#'
#' @section References:
#' \itemize{
#' \item Connor T. Jerzak, Fredrik Johansson, Adel Daoud. Image-based Treatment Effect Heterogeneity. Forthcoming in \emph{Proceedings of the Second Conference on Causal Learning and Reasoning (CLeaR), Proceedings of Machine Learning Research (PMLR)}, 2023.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @import tensorflow
#' @import keras
#' @export
#' @md
AnalyzeImageHeterogeneity <- function(obsW,
                                      obsY,
                                      X = NULL,
                                      orthogonalize = F,
                                      imageKeysOfUnits = 1:length(obsY),
                                      kClust_est = 2,
                                      acquireImageFxn = NULL ,
                                      transportabilityMat = NULL ,
                                      lat = NULL,
                                      long = NULL,
                                      conda_env = NULL,
                                      conda_env_required = F,

                                      figuresTag = "",
                                      figuresPath = "./",
                                      plotBands = 1L,
                                      modelType = "variational_minimal",
                                      simMode = F,
                                      plotResults = F,

                                      nDepthHidden_conv = 1L,
                                      nDepthHidden_dense = 0L,
                                      maxPoolSize = 2L,
                                      strides = 1L,
                                      yDensity = "normal",
                                      compile = T,
                                      nMonte_variational = 5L,
                                      nMonte_predictive = 20L,
                                      nMonte_salience = 100L,
                                      batchSize = 25L,
                                      kernelSize = 5L,
                                      nSGD  = 400L,
                                      nDenseWidth = 32L,
                                      reparameterizationType = "Reparameterization",
                                      doConvLowerDimProj = T,
                                      nDimLowerDimConv = 3L,
                                      nFilters = 32L,
                                      channelNormalize = T,
                                      printDiagnostics = F,
                                      quiet = F){
  if(T == T){
    library(tensorflow); library(keras)
    try(tensorflow::use_condaenv(conda_env, required = conda_env_required),T)
    Sys.sleep(1.); try(tf$square(1.),T); Sys.sleep(1.)
    try(tf$config$experimental$set_memory_growth(tf$config$list_physical_devices('GPU')[[1]],T),T)
    tf$config$set_soft_device_placement( T )
    tfp <- tf_probability()
    tfd <- tfp$distributions
    #tfa <- reticulate::import("tensorflow_addons")

    tf$random$set_seed(  c(1000L ) )
    tf$keras$utils$set_random_seed( 4L )

    py_gc <- reticulate::import("gc")
    gc(); py_gc$collect()
  }

  # coerce output to tf$constant
  environment(acquireImageFxn) <- environment()
  test_ <- acquireImageFxn(imageKeysOfUnits[1:5],training = F)
  if(!"tensorflow.tensor" %in% class(test_)){
    acquireImageFxn_as_input <- acquireImageFxn
    acquireImageFxn <- function(keys, training){
      m_ <- tf$constant(acquireImageFxn_as_input(keys, training),tf$float32)
      if(length(m_$shape) == 3){
        # expand across batch dimension if receiving no batch dimension
        m_ <- tf$expand_dims(m_,0L)
      }
      return( m_ )
    }
  }
  rm( test_ )
  if(channelNormalize == T){
    print("Getting channel normalization parameters...")
    acquireImageRepFxn_orig <- acquireImageFxn
    tmp <- replicate(30, {
        tmp <- acquireImageFxn( sample(unique(imageKeysOfUnits), batchSize),
                         training = F)
        list("NORM_MEAN" = apply(as.array(tmp),4,function(zer){mean(zer,na.rm=T)}),
             "NORM_SD" = apply(as.array(tmp),4,function(zer){sd(zer,na.rm=T)})  )
    })
    NORM_MEAN <- tf$expand_dims( tf$expand_dims(tf$expand_dims(tf$constant(colMeans(do.call(rbind,tmp["NORM_MEAN",]))),0L),0L), 0L)
    NORM_SD <- tf$expand_dims( tf$expand_dims(tf$expand_dims(colMeans(do.call(rbind,tmp["NORM_SD",])),0L),0L), 0L)
    acquireImageFxn <- function(keys, training){
        (acquireImageRepFxn_orig(keys, training) - NORM_MEAN) / NORM_SD
    }
    rm(  tmp  )
  }

  print2 <- function(x){if(!quiet){print(x)}}

  # set environment of image sampling functions
  if(is.null(acquireImageFxn)){ acquireImageFxn <- function(keys){acquireImageRepFxn_orig(keys, training = F)} }
  figuresPath <- paste(strsplit(figuresPath,split="/")[[1]],collapse = "/")
  nDimLowerDimConv <- as.integer( nDimLowerDimConv )
  windowCounter <- 0

  # orthogonalize if specified
  whichNA_dropped <- c()
  if(orthogonalize){
    print2("Orthogonalizing Potential Outcomes...")
    if(is.null(X)){stop("orthogonalize set to TRUE, but no X specified to perform orthogonalization!")}

    # drop observations with NAs in their orthogonalized outcomes
    whichNA_dropped <- which( is.na(  rowSums( X ) ) )
    if(length(whichNA_dropped) > 0){
      # note: transportabilityMat doesn't need to drop dropNAs
      obsW <- obsW[-whichNA_dropped]
      obsY <- obsY[-whichNA_dropped]
      X <- X[-whichNA_dropped,]
      imageKeysOfUnits <- imageKeysOfUnits[-whichNA_dropped]
      lat <- lat[ -whichNA_dropped ]
      long <- long[ -whichNA_dropped ]
    }
    Yobs_ortho <- resid(temp_lm <- lm(obsY ~ X))
    if(length(Yobs_ortho) != length(obsY)){
      stop("length(Yobs_ortho) != length(obsY)")
    }
    plot(obsY,Yobs_ortho)
    obsY <- Yobs_ortho
    #YandW_mat[f2n(names(my_resid)),]$Yobs_ortho <- as.numeric(YandW_mat[f2n(names(my_resid)),]$Yobs_ortho)
    #YandW_mat[["Yobs_ortho"]][f2n(names(my_resid))] <- my_resid
    #try({plot(YandW_mat$Yobs_ortho,YandW_mat$Yobs);abline(a=0,b=1)},T)
    #YandW_mat$Yobs_ortho[is.na(YandW_mat$Yobs_ortho)] <- mean(YandW_mat$Yobs_ortho,na.rm=T)
  }

  # specify some training parameters + helper functions
  rzip <- function(l1,l2){  fl<-list(); for(aia in 1:length(l1)){ fl[[aia]] <- list(l1[[aia]], l2[[aia]]) }; return( fl  ) }
  GlobalMax <- tf$keras$layers$GlobalMaxPool2D()
  GlobalAve <- tf$keras$layers$GlobalAveragePooling2D()
  GlobalFlatten <- tf$keras$layers$Flatten()
  FinalImageSummary <- function(x){tf$concat(list(GlobalMax(x),GlobalAve(x)),1L)}
  #GlobalSpatial <- tfa$layers$SpatialPyramidPooling2D(bins = list(4L,4L))
  #FinalImageSummary <- function(x){GlobalFlatten( GlobalSpatial( x ) )}
  #FinalImageSummary <- GlobalFlatten

  adaptiveMomentum <- F
  BNPreOutput <- F;
  BNPrePreOutput <- T;
  LowerDimActivation <- ConvActivation <- "swish"
  LowerDimInputDense <- F
  doBN_conv1 <- T; doBN_conv2 <- T
  kernelSize_est <- as.integer(  kernelSize )
  batchFracOut <- max(1/3*batchSize,3) / batchSize
  nMonte_variational <- as.integer( nMonte_variational  )
  LEARNING_RATE_BASE <- .005; widthCycle <- 50
  INV_TEMP_GLOBAL <- 1/0.5
  WhenPool <- c(1,2)
  #plot(as.matrix(do.call(rbind,replicate(10,tfd$RelaxedOneHotCategorical(temperature = 1/INV_TEMP_GLOBAL, probs = c(0.1,0.9))$sample(1L))))[,2],ylim = c(0,1))
  #points(as.matrix(do.call(rbind,replicate(10,tfd$RelaxedOneHotCategorical(temperature = 1/INV_TEMP_GLOBAL, probs = c(0.5,0.5))$sample(1L))))[,2],pch = 2,col="gray")
  #points(as.matrix(do.call(rbind,replicate(10,tfd$RelaxedOneHotCategorical(temperature = 1/INV_TEMP_GLOBAL, probs = c(0.1,0.9))$sample(1L))))[,2],pch = 1,col="black")
  BN_MOM <- 0.9
  BN_EP <- 0.01
  if(grepl(modelType,pattern = "variational")){ BN_MOM <- 0.90^(1/nMonte_variational) }
  ConvNormText <- function(dim_){"tf$keras$layers$BatchNormalization(axis = 3L, center = T, scale = T, momentum = BN_MOM, epsilon = BN_EP)"}
  #ConvNormText <- function(dim_){dim_ <- paste(as.character(dim_),"L",sep=""); sprintf("tfa$layers$GroupNormalization(groups = c(%s), center = T, scale = T, epsilon = BN_EP)", dim_) }
  #ConvNormText <- function(dim_){"tf$keras$layers$LayerNormalization(center = T, scale = T, epsilon = BN_EP)"}
  #tmp <- eval(parse(text = ConvNormText))
  # c(mean(c(as.array((tmp_[,,,1])))), sd(c(as.array((tmp_[,,,1])))))
  #DenseNormText <- function(){"tf$keras$layers$LayerNormalization(center = T, scale = T, epsilon = BN_EP)"}
  DenseNormText <- function(){"tf$keras$layers$BatchNormalization(center = T, scale = T, momentum = BN_MOM, epsilon = BN_EP)"}

  # set up some placeholders
  y0_true <- r2_y1_out <- r2_y0_out <- ClusterProbs_est <- NULL
  tau_i_est <- sd_tau1 <- sd_tau2 <- negELL <- y1_est <- y0_est <- y1_true <- y0_true <- NULL
  if(!"ClusterProbs" %in% ls() &
     !"ClusterProbs" %in% ls(envir=globalenv())){ClusterProbs<-NULL}

  # normalize outcomes for stability (estimates are re-normalized after training)
  if( yDensity == "normal"){
    Y_mean <- mean(obsY); Y_sd <- sd(obsY)
    obsY <- (obsY - Y_mean)  /  Y_sd
  }
  if( yDensity == "lognormal"){
    Y_mean <- -abs(min(obsY))-0.1; Y_sd <- sd(obsY)
    obsY <- (obsY - Y_mean)  /  Y_sd
  }
  Rescale <- function(x,doMean = F){ return( x*Y_sd + ifelse(doMean, yes = Y_mean, no = 0) ) }
  Tau_mean_init_prior <- Tau_mean_init <- mean(obsY[obsW==1]) - mean(obsY[obsW==0])
  Tau_sd_init <- sqrt( var(obsY[obsW==1]) + var( obsY[obsW==0]) )
  Y0_sd_vec <- na.omit(replicate(10000,{ top_ <- sample(1:length(obsY),batchSize); return(sd(obsY[top_][obsW[top_]==0])) }))
  Y1_sd_vec <- na.omit(replicate(10000,{ top_ <- sample(1:length(obsY),batchSize); sd(obsY[top_][obsW[top_]==1]) }))
  tau_vec <- na.omit(replicate(10000,{ top_ <- sample(1:length(obsY),batchSize); mean(obsY[top_][obsW[top_]==1]) - mean(obsY[top_][obsW[top_]==0]) }))
  Y0_mean_init_prior <- Y0_mean_init <- mean(obsY[obsW==0]); Y0_sd_init_prior <- Y0_sd_init <- max(0.01, median(Y0_sd_vec,na.rm=T))
  Y1_mean_init_prior <- Y1_mean_init <- mean(obsY[obsW==1]); Y1_sd_init_prior <- Y1_sd_init <- max(0.01, median(Y1_sd_vec,na.rm=T))
  Y0_sd_init_prior <- tfp$math$softplus_inverse(Y0_sd_init_prior)
  Y1_sd_init_prior <- tfp$math$softplus_inverse(Y1_sd_init_prior)

  for(BAYES_STEP in c(1,2)){
    if(BAYES_STEP == 1){ print2("Empirical Bayes Calibration Step (see  Krishnan et al. (2020))...") }
    if(BAYES_STEP == 2){ print2("Empirical Bayes Estimation Step...") }

    if(BAYES_STEP == 1){
      nSGD_ORIG <- nSGD
      nSGD <- nSGD
      L2_grad_scale <- 0.5
      SD_PRIOR_MODEL <- .01; KL_wt <- 0
      PRIOR_MODEL_FXN <- function(name_){
        eval(parse(text = 'function(dtype, shape, name, trainable, add_variable_fn){
      d_prior <- tfd$Normal(loc = tf$zeros(shape), scale = SD_PRIOR_MODEL)
      tfd$Independent(d_prior, reinterpreted_batch_ndims = tf$size(d_prior$batch_shape_tensor())) }'))
      }
      PRIOR_MODEL_FXN("hap")
    }
    if(BAYES_STEP == 2){
      nSGD <- nSGD_ORIG
      L2_grad_scale <- 0.5
      KL_wt <- batchSize / length(obsY)
      PRIOR_MODEL_FXN <- function(name_){
        prior_loc_name <- sprintf("%s_PRIOR_MEAN_HASH818",name_ )
        prior_SD_name <- sprintf("%s_PRIOR_SD_HASH818",name_ )
        ZERO_LEN_IN <- length( eval(parse(text = sprintf("%s$variables",name_)))) == 0
        if( ZERO_LEN_IN){
          prior_loc_name <- "tf$zeros(shape)"
          prior_SD_name <- "1"
        }
        #if( ZERO_LEN_IN & BAYES_STEP == 2){ browser()  }
        if( !ZERO_LEN_IN){
          z_name_ref <- eval(parse(text = sprintf("%s$variables[[1]]$name",name_)))
          z_name_ref <- gsub(z_name_ref, pattern = ":",replace = "XCOLX")
          z_name_ref <- gsub(z_name_ref, pattern = "/",replace = "XDASHX")

          # set mean
          eval.parent(parse(text = sprintf("%s <- tf$constant(%s$variables[[1]],tf$float32)",prior_loc_name,name_)))
          #eval.parent(parse(text = sprintf("%s <- tf$constant(RollMean_%s)",prior_loc_name,z_name_ref)))

          # assign variable to its mean - can speed up training, but generates instabilities in optimization
          #eval.parent(parse(text = sprintf("%s$variables[[1]]$assign( %s )",name_, prior_loc_name)))

          # set sd
          # different variance scaling elections
          #eval.parent(parse(text = sprintf("%s <- tf$constant(2*tf$sqrt(tf$math$reduce_variance(%s$variables[[1]])),tf$float32)",prior_SD_name,name_)))
          #eval.parent(parse(text = sprintf("%s <- tf$constant(1*tf$sqrt(tf$math$reduce_variance(%s$variables[[1]])),tf$float32)",prior_SD_name,name_)))
          #eval.parent(parse(text = sprintf("%s <- tf$constant(0.1*tf$sqrt(tf$math$reduce_variance(%s$variables[[1]])),tf$float32)",prior_SD_name,name_)))# previous use
          eval.parent(parse(text = sprintf("%s <- tf$maximum(0.001,tf$constant(0.1*tf$sqrt(tf$math$reduce_variance(%s$variables[[1]])),tf$float32))",prior_SD_name,name_)))
          #eval.parent(parse(text = sprintf("%s <- 0.1*tf$ones( tf$shape(%s$variables[[1]]),tf$float32)",prior_SD_name,name_)))
          #eval.parent(parse(text = sprintf("%s <- tf$constant(tf$sqrt(tf$maximum(1e-4,RollVar_%s)))",prior_SD_name,z_name_ref)))
        }
        eval(parse(text = sprintf('function(dtype, shape, name, trainable, add_variable_fn){
              d_prior <- tfd$Normal(loc = (%s),
                                  scale = (%s))
              tfd$Independent(d_prior, reinterpreted_batch_ndims = tf$size(d_prior$batch_shape_tensor())) }',
                                  prior_loc_name, prior_SD_name)))
      }

      # set other priors
      Tau_mean_init_prior <- as.vector(MeanDist_tau[k_,"Mean"][[1]])
      Y0_sd_init_prior <- as.vector(SDDist_Y0[k_,"Mean"][[1]])
      Y1_sd_init_prior <- as.vector(SDDist_Y1[k_,"Mean"][[1]])
    }
    print2("Building clustering model...")
    {
      BNLayer_Axis1_Clust <-  eval(parse(text = DenseNormText()))
      BNLayer_Axis1_Y0 <- eval(parse(text = DenseNormText()))
      BNLayer_Axis1_Proj <- eval(parse(text = DenseNormText()))
      BNLayer_Axis1_ProjY0 <- tf$keras$layers$BatchNormalization(axis = 1L, center = T, scale = T, momentum = BN_MOM, epsilon = BN_EP,
                                                                 beta_initializer = tf$constant_initializer(Y0_mean_init),
                                                                 gamma_initializer = tf$constant_initializer(Y0_sd_init),
                                                                 name = "BN_Y0")
      BNLayer_Axis1_ProjTau <- tf$keras$layers$BatchNormalization(axis = 1L, center = T, scale = T, momentum = BN_MOM, epsilon = BN_EP,
                                                                  beta_initializer = tf$constant_initializer(Tau_mean_init),
                                                                  gamma_initializer = tf$constant_initializer(Tau_sd_init))
      tmp_ <- 1*1/length(obsW[obsW==1])*var(obsY[obsW==1])+1/length(obsW[obsW==0])*var(obsY[obsW==0])
      if(modelType == "variational_CNN"){ for(k___ in 1:kClust_est){
        eval(parse(text = sprintf("BNLayer_Axis1_Tau%s <- %s", k___, DenseNormText())))
        eval(parse(text =
                     sprintf("BNLayer_Axis1_ProjTau%s <- tf$keras$layers$BatchNormalization(axis = 1L, center = T, scale = T, momentum = BN_MOM, epsilon = BN_EP,
                                                               beta_initializer = tf$constant_initializer(Tau_mean_init),
                                                               gamma_initializer = tf$constant_initializer(Tau_sd_init))",k___)))
        eval(parse(text=
                     sprintf("TauProj%s = ProbDenseType(as.integer(1L),
                          kernel_prior_fn = PRIOR_MODEL_FXN('TauProj%s'),
                          name = 'TauProj%s',
                          activation='linear')",k___, k___,k___)))
      }}
      if(reparameterizationType == "Flipout"){
        #ProbLayerExecutionDevice <- '/GPU:0' # currently unavailable with tensorflow 2.11
        ProbLayerExecutionDevice <- '/CPU:0'
        ProbConvType <- tfp$layers$Convolution2DFlipout # more efficient, must wrap execution in with(tf$device('/CPU:0'),{...})
        ProbDenseType <-  tfp$layers$DenseFlipout # more efficient, must wrap execution in with(tf$device('/CPU:0'),{...})
      }
      if(reparameterizationType == "Reparameterization"){
        ProbLayerExecutionDevice <- '/GPU:0'
        ProbConvType <- tfp$layers$Convolution2DReparameterization # less efficient
        ProbDenseType <-  tfp$layers$DenseReparameterization # less efficient
      }
      for(conv_ in 1:nDepthHidden_conv){
        eval(parse(text = sprintf("BNLayer_Axis3_Clust_%s <- %s",conv_,ConvNormText(nFilters) )))
        eval(parse(text = sprintf("BNLayer_Axis3_Y0_%s <- %s",conv_,ConvNormText(nFilters) )))
        tmp <- conv_ == nDepthHidden_conv & (LowerDimInputDense == F)
        ProjNormInput <- ifelse(tmp, yes = nFilters, no = nDimLowerDimConv)
        eval(parse(text = sprintf("BNLayer_Axis3_Clust_Proj_%s <- %s",conv_,ConvNormText(ProjNormInput) )))
        eval(parse(text = sprintf("BNLayer_Axis3_Y0_Proj_%s <- %s",conv_,ConvNormText(ProjNormInput)  )))
        eval(parse(text = sprintf("ClusterConv%s <- ProbConvType(filters = nFilters,
                                                   kernel_size = kernelSize_est,
                                                   activation = ConvActivation,
                                                   kernel_prior_fn = PRIOR_MODEL_FXN('ClusterConv%s'),
                                                   strides = strides,
                                                   name = 'ClusterConv%s',
                                                   padding = 'valid')",conv_,conv_,conv_)))
        eval(parse(text = sprintf("ClusterConvProj%s <- ProbDenseType(nDimLowerDimConv,
                                kernel_prior_fn = PRIOR_MODEL_FXN('ClusterConvProj%s'),
                                activation=LowerDimActivation,
                                name = 'ClusterConvProj%s')",conv_,conv_,conv_)))
        if(grepl(modelType,pattern="variational")){
          eval(parse(text = sprintf("Y0Conv%s <- ProbConvType(filters = nFilters,
                                                     kernel_size=c(kernelSize_est,kernelSize_est),
                                                     activation = ConvActivation,
                                                     kernel_prior_fn = PRIOR_MODEL_FXN('Y0Conv%s'),
                                                     strides = strides,
                                                     name = 'Y0Conv%s',
                                                     padding = 'valid')",conv_,conv_,conv_)))
          eval(parse(text = sprintf("Y0ConvProj%s <- ProbDenseType(nDimLowerDimConv,
                                  kernel_prior_fn = PRIOR_MODEL_FXN('Y0ConvProj%s'), activation=LowerDimActivation)",conv_,conv_)))
          if(modelType == "variational_CNN"){ for(k____ in 1:kClust_est){
            eval(parse(text = sprintf("TauConv%s_%s <- ProbConvType(filters = nFilters,
                                                     kernel_size=c(kernelSize_est,kernelSize_est),
                                                     activation=ConvActivation,
                                                     kernel_prior_fn = PRIOR_MODEL_FXN('TauConv%s_%s'),
                                                     strides = strides,
                                                     name = 'TauConv%s_%s',
                                                     padding = 'valid')",conv_,k____, conv_, k____,conv_, k____)))
            eval(parse(text = sprintf("TauConvProj%s_%s <- ProbDenseType(nDimLowerDimConv,
                                    kernel_prior_fn = PRIOR_MODEL_FXN('TauConvProj%s_%s'),
                                    activation=LowerDimActivation)",conv_,k____,conv_,k____)))
            eval(parse(text = sprintf("BNLayer_Axis3_Tau_%s_%s <- %s",conv_,k____,ConvNormText(nFilters))))
            eval(parse(text = sprintf("BNLayer_Axis3_Tau_Proj_%s_%s <- %s",conv_,k____,ConvNormText(nDimLowerDimConv))))
          }}
        }
        if(modelType == "tarnet"){
          eval(parse(text = sprintf("Y0Conv%s <- tf$keras$layers$Conv2D(filters = nFilters,
                                                       kernel_size=c(kernelSize_est,kernelSize_est),
                                                       activation=ConvActivation,
                                                       kernel_prior_fn = PRIOR_MODEL_FXN('Y0Conv%s'),
                                                       strides = strides,
                                                       name = 'Y0Conv%s',
                                                       padding = 'valid')",conv_,conv_,conv_)))
          eval(parse(text = sprintf("Y0ConvProj%s <- ProbDenseType(nDimLowerDimConv,
                                  kernel_prior_fn = PRIOR_MODEL_FXN('Y0ConvProj%s'),
                                  name = 'Y0ConvProj%s',
                                  activation=LowerDimActivation)",conv_,conv_,conv_)))
        }
      }
      if(nDepthHidden_dense > 0){
      for(dense_ in 1:nDepthHidden_dense){
        eval(parse(text = sprintf("BNLayer_Axis1_Clust_%s <- %s",dense_,DenseNormText())))
        eval(parse(text = sprintf("BNLayer_Axis1_Y0_%s <- %s",dense_,DenseNormText())))
        eval(parse(text = sprintf("DenseProj_Clust_%s <- ProbDenseType(as.integer(nDenseWidth),
                                kernel_prior_fn = PRIOR_MODEL_FXN('DenseProj_Clust_%s'),
                                activation='swish')",dense_,dense_)))
        eval(parse(text = sprintf("DenseProj_Y0_%s <- ProbDenseType(as.integer(nDenseWidth),
                                kernel_prior_fn = PRIOR_MODEL_FXN('DenseProj_Y0_%s'),
                                activation='swish')",dense_,dense_)))

      }
      }
      ClusterProj = ProbDenseType( as.integer(kClust_est-1L),
                                   kernel_prior_fn = PRIOR_MODEL_FXN('ClusterProj'), activation='linear' )
      if(grepl(modelType, pattern = "variational")){Y0Proj = ProbDenseType(as.integer(1L),
                                                                      kernel_prior_fn = PRIOR_MODEL_FXN('Y0Proj'),activation='linear')}
      BNLayer_Axis1_ProjY1 <- tf$keras$layers$BatchNormalization(axis = 1L, center = T, scale = T, momentum = BN_MOM, epsilon = BN_EP,
                                                                 beta_initializer = tf$constant_initializer( Y1_mean_init ),
                                                                 gamma_initializer = tf$constant_initializer( Y1_sd_init ) )
      if(modelType == "tarnet"){
        Y0Proj = tf$keras$layers$Dense(as.integer(1L), activation='linear')
        Y1Proj = tf$keras$layers$Dense(as.integer(1L), activation='linear')
      }
      SD_scaling <- 1
      Tau_mean_init <- mean(obsY[obsW==1]) - mean(obsY[obsW==0])
      Tau_means_init <- Tau_mean_init + .01*seq(-1,1,length.out=kClust_est)*max(0.01,abs(Tau_mean_init))
      Y0_sds_prior_mean <- tfp$math$softplus_inverse(SD_scaling*(Y0_sd_init))#<-SD_scaling*sd(obsY[obsW==0])))
      Y1_sds_prior_mean <- tfp$math$softplus_inverse(SD_scaling*(Y1_sd_init))#<-SD_scaling*sd(obsY[obsW==1])))

      base_mat <- as.data.frame( matrix(list(),nrow=kClust_est,ncol=3L) ); colnames( base_mat ) <- c("Mean","SD","Prior")
      SDDist_Y1 <- SDDist_Y0 <- MeanDist_tau <- base_mat
      for(k_ in 1:kClust_est){
        # prior SD - subject-matter knowledge informs this
        # set this to a small number so network starts off as nearly deterministic
        sd_init_trainableParams <- as.numeric(tfp$math$softplus_inverse(0.01))

        MeanDist_tau[k_,"Mean"][[1]] <- list( tf$Variable(Tau_means_init[k_],trainable=T,name=sprintf("MeanTau%s_mean",k_) ) )
        MeanDist_tau[k_,"SD"][[1]] <- list( tf$Variable(sd_init_trainableParams,trainable=T,name = sprintf("MeanTau%s_sd",k_) ) )
        MeanDist_tau[k_,"Prior"][[1]] <- list( tfd$Normal(Tau_mean_init_prior, 2*sd(tau_vec) ))

        # Y0
        SDDist_Y0[k_,"Mean"][[1]] <- list( tf$Variable(3*Y0_sds_prior_mean,trainable=T,name=sprintf("SDY0%s_mean",k_) ) )
        SDDist_Y0[k_,"SD"][[1]] <- list( tf$Variable(sd_init_trainableParams,trainable=T,name=sprintf("SDY0%s_sd",k_)) )
        SDDist_Y0[k_,"Prior"][[1]] <- list( tfd$Normal(Y0_sd_init_prior,2*sd(Y0_sd_vec)))

        # Y0
        SDDist_Y1[k_,"Mean"][[1]] <- list( tf$Variable(3*Y1_sds_prior_mean,trainable=T,name=sprintf("SDY1%s_mean",k_) ) )
        SDDist_Y1[k_,"SD"][[1]] <- list( tf$Variable(sd_init_trainableParams,trainable=T,name=sprintf("SDY1%s_sd",k_)) )
        SDDist_Y1[k_,"Prior"][[1]] <- list( tfd$Normal(Y1_sd_init_prior,2*sd(Y1_sd_vec)))

        # SD posterior checks
        #hist(as.numeric(tf$nn$softplus(rnorm(1000,as.numeric((SDDist_Y0[k_,"Mean"][[1]])), sd=as.numeric(tf$nn$softplus(SDDist_Y0[k_,"SD"][[1]]))))))
        #abline(v=as.numeric(tf$nn$softplus(as.numeric((SDDist_Y0[k_,"Mean"][[1]])))),col="red")
        #abline(v=sd(obsY[obsW==0]),lwd= 2)
      }
      CategoricalPrior <- tfd$Categorical(probs=rep(1/kClust_est,kClust_est))
      LocalMax <- tf$keras$layers$MaxPool2D(pool_size = maxPoolSize)
      LocalAve <- tf$keras$layers$AveragePooling2D(pool_size = maxPoolSize)
      #LocalPool <- function(x){tf$concat(list(LocalMax(x),LocalAve(x)),3L)}
      LocalPool <- LocalMax
    }

    if(compile == T){ tf_function_fxn <- function(x){tf_function(x,experimental_relax_shapes = F)}}
    if(compile == F){ tf_function_fxn <- function(x){x} }
    getClusterLogits <- tf_function_fxn( function(  m , training){
      if( !modelType %in% "variational_minimal_visualizer" ){
        # convolution part
        for(conv__ in 1L:nDepthHidden_conv){
          eval(parse(text = sprintf("m <-  with(tf$device( ProbLayerExecutionDevice ), { ClusterConv%s( m ) }) ",conv__)))
          if(conv__ %in% WhenPool){ m <- LocalPool( m ) }
          doLower <- (ifelse(LowerDimInputDense,yes = T, no = conv__ < nDepthHidden_conv)) & doConvLowerDimProj
          if(doLower & doBN_conv1){eval(parse(text = sprintf("m <- BNLayer_Axis3_Clust_%s(m,training=training)",conv__)))}
          if(doLower){eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { ClusterConvProj%s( m ) })",conv__))) }
          if(doBN_conv2){eval(parse(text = sprintf("m <- BNLayer_Axis3_Clust_Proj_%s(m,training=training)",conv__)))}
        }
        print2("Final convolved image dimensions: ")
        print2(dim(m))
        m <- FinalImageSummary(m)
        m <- BNLayer_Axis1_Clust(m, training = training)

        # dense part
        if(nDepthHidden_dense > 0){
        for(dense_ in 1:nDepthHidden_dense){
          m_tminus1 <- m
          eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { DenseProj_Clust_%s(m)})",dense_)))
          if(!(dense_ == nDepthHidden_dense & !BNPrePreOutput)){
            eval(parse(text = sprintf("m <- BNLayer_Axis1_Clust_%s(m,training = training)",dense_)))
          }
          if(dense_ > 1 & dense_ < nDepthHidden_dense){ m <- m + m_tminus1 }
        }
        }
      }

      # final projection layer
      m <- with(tf$device( ProbLayerExecutionDevice ), { ClusterProj(m) })
      if(BNPreOutput){m <- BNLayer_Axis1_Proj(m, training = training) }
      m <- tf$concat(list( tf$zeros(list(tf$shape(m)[1],1L)),m),1L)
      return( m  )
    })

    if(modelType == "tarnet"){
      getImageRep <- tf_function_fxn( function(m,training){
        for(conv__ in 1:nDepthHidden_conv){
          eval(parse(text = sprintf("m <-  with(tf$device( ProbLayerExecutionDevice ), { Y0Conv%s( m  ) })",conv__)))
          if(conv__ %in% WhenPool){ m <- LocalPool( m ) }
          doLower <- (ifelse(LowerDimInputDense,yes = T, no = conv__ < nDepthHidden_conv)) & doConvLowerDimProj
          if(doLower & doBN_conv1){eval(parse(text = sprintf("m <- BNLayer_Axis3_Y0_%s(m,training=training)",conv__)))}
          if(doLower){eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { Y0ConvProj%s( m ) })",conv__))) }
          if(doBN_conv2){eval(parse(text = sprintf("m <- BNLayer_Axis3_Y0_Proj_%s(m,training=training)",conv__)))}
        }
        m <- FinalImageSummary( m  )
        m <- BNLayer_Axis1_Y0(m, training = training)
      } )
      getY0 <- tf_function_fxn(function(  m , training  ){
        m <- getImageRep(m,training=training)
        return( getY0_finalStep(m,training=training) )
      })
      getY1 <- tf_function_fxn(function(  m , training  ){
        m <- getImageRep(m,training=training)
        return( getY1_finalStep(m,training=training) )
      })
      getY0_finalStep <- tf_function_fxn(function(  m , training  ){
        m <- with(tf$device( ProbLayerExecutionDevice ), { Y0Proj(m) } )
        if(BNPreOutput){m <- BNLayer_Axis1_ProjY0(m, training = training)}
        return( m  )
      } )
      getY1_finalStep <- tf_function_fxn( function(  m , training){
        m <- with(tf$device( ProbLayerExecutionDevice ), { Y1Proj(m) } )
        if(BNPreOutput){m <- BNLayer_Axis1_ProjY1(m, training = training)}
        return( m  )
      } )
    }
    if(grepl(modelType, pattern = "variational")){
      getY0 <- tf_function_fxn(function(  m , training  ){
        if(! modelType %in% "variational_minimal_visualizer"){
          # convolution part
          for(conv__ in 1:nDepthHidden_conv){
            eval(parse(text = sprintf("m <-  with(tf$device( ProbLayerExecutionDevice ), { Y0Conv%s( m )} )",conv__)))
            if(conv__ %in% WhenPool){ m <- LocalPool( m ) }
            doLower <- (ifelse(LowerDimInputDense,yes = T, no = conv__ < nDepthHidden_conv)) & doConvLowerDimProj
            if(doLower & doBN_conv1){eval(parse(text = sprintf("m <- BNLayer_Axis3_Y0_%s(m,training=training)",conv__)))}
            if(doLower){eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { Y0ConvProj%s( m ) })",conv__))) }
            if(doBN_conv2){eval(parse(text = sprintf("m <- BNLayer_Axis3_Y0_Proj_%s(m,training=training)",conv__)))}
          }
          m <- FinalImageSummary( m  )
          m <- BNLayer_Axis1_Y0(m, training = training)

          # dense part
          if(nDepthHidden_dense > 0){
          for(dense_ in 1:nDepthHidden_dense){
            m_tminus1 <- m
            eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { DenseProj_Y0_%s(m) } ) ",dense_)))
            if(!(dense_ == nDepthHidden_dense & !BNPrePreOutput)){
              eval(parse(text = sprintf("m <- BNLayer_Axis1_Y0_%s(m,training = training)",dense_)))
            }
            if(dense_ > 1 & dense_ < nDepthHidden_dense){ m <- m + m_tminus1 }
          }
          }
        }
        m <- with(tf$device( ProbLayerExecutionDevice ), { Y0Proj(m) } )
        if(BNPreOutput){m <- BNLayer_Axis1_ProjY0(m, training = training)}
        return( m  )
      } )
      getClusterProb <- tf_function_fxn(function(m , training){
        return( tf$nn$softmax(getClusterLogits(m, training = training), 1L) )
      })
      getClusterSamp_logitInput <- tf_function_fxn( function(logits_){
        #clustT_samp = tf$cast(tfd$OneHotCategorical(logits = logits_)$sample(1L),dtype = tf$float32)
        clustT_samp = tfd$RelaxedOneHotCategorical(temperature = 1/INV_TEMP_GLOBAL, logits = logits_)$sample(1L)
      })

      if(modelType == "variational_CNN"){
        getTau <- tf_function_fxn(function(  m, training  ){
          m_orig <- m
          m_ret<-list();for(k____ in 1:kClust_est){
            for(conv__ in 1:nDepthHidden_conv){
              if(conv__ == 1){ m <- m_orig }
              eval(parse(text = sprintf("m <-  TauConv%s_%s( m )",conv__,k____)))
              if(conv__ %in% WhenPool){ m <- LocalPool( m ) }
              doLower <- (ifelse(LowerDimInputDense,yes = T, no = conv__ < nDepthHidden_conv)) & doConvLowerDimProj
              if(doLower & doBN_conv1){eval(parse(text = sprintf("m <- BNLayer_Axis3_Tau_%s_%s(m,training=training)",conv__,k____)))}
              if(doLower){eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { TauConvProj%s_%s( m )})",conv__,k____))) }
              if(doBN_conv2){eval(parse(text = sprintf("m <- BNLayer_Axis3_Tau_Proj_%s_%s(m,training=training)",conv__,k____)))}
            }
            m <- FinalImageSummary( m  )
            eval(parse(text = sprintf("m <- BNLayer_Axis1_Tau%s(m, training = training)",k____)))
            eval(parse(text = sprintf("m <- with(tf$device( ProbLayerExecutionDevice ), { TauProj%s(m)} )",k____)))
            if(BNPreOutput){
              eval(parse(text = sprintf("m <- BNLayer_Axis1_ProjTau%s(m, training = training)",k____)))
            }
            m_ret[[k____]] <- m
          }
          m_ret <- tf$concat(m_ret,1L)
          return( m_ret  )
        } )
        getY1 <- tf_function_fxn( function(  m , training){
          Y0 <- getY0(m=m,training = training)
          Clust_logits <- getClusterLogits(m,training = training)
          clustT <- tf$squeeze(getClusterSamp_logitInput(Clust_logits),0L)
          tau_i <- getTau(m,training = training)
          tau_i <- tf$reduce_sum(tf$multiply(tau_i, clustT),1L,keepdims=T)
          Y1 <- Y0 + tau_i
          return(  Y1   )
        } )
      }
      if(grepl(modelType, pattern = "variational_minimal")){
        getY1 <- tf_function_fxn( function(  m , training){
          Y0 <- getY0(m=m,training = training)
          Clust_logits <- getClusterLogits(m,training = training)
          clustT <- tf$squeeze(getClusterSamp_logitInput(Clust_logits),0L)
          ETau_draw <-  (tfd$Normal(getTau_means(),
                                    tf$nn$softplus(MeanDist_tau[,"SD"])))$sample(1L)
          tau_i <- tf$reduce_sum(tf$multiply(ETau_draw, clustT),1L,keepdims=T)
          Y1 <- Y0 + tau_i
          return(  Y1   )
        } )
      }
      marginal_tau <- tf$constant(mean(obsY[obsW==1],na.rm=T)-mean(obsY[obsW==0],na.rm=T), tf$float32)
      marginal_lambda <- tf$constant(.01, tf$float32)
      TauRunning <- tf$Variable(t(rep(as.matrix(marginal_tau),times=kClust_est)),
                                dtype=tf$float32,trainable=F)
      getTau_means <- tf_function_fxn( function(){
        if(yDensity == "normal"){ ret_ <- tf$identity(MeanDist_tau[,"Mean"]) }
        if(yDensity == "lognormal"){ ret_ <- tf$identity(MeanDist_tau[,"Mean"]) }
        return(ret_)
      } )
    }

    getTrainingLikelihoodDraw <- tf_function_fxn(function(dat,treat,y){
      training <- T
      nMonte_internal <- 1L

      # cluster probabilities
      Clust_logits <- replicate(nMonte_internal,tf$expand_dims(getClusterLogits(dat, training = training),0L))
      Clust_logits <- tf$concat(Clust_logits,0L)
      clustT <- tf$squeeze(getClusterSamp_logitInput(Clust_logits),0L)
      Clust_probs <- tf$nn$softmax(Clust_logits, 2L)
      #CategoricalPost <- tfd$Categorical(probs = Clust_probs)

      EY0_i <- replicate(nMonte_internal,tf$expand_dims(getY0(dat,training = training),0L))
      EY0_i <- tf$squeeze(tf$concat(EY0_i,0L),2L)

      # enforce ATE
      if(modelType == "variational_CNN"){
        ETau_draw <- tf$concat(replicate(nMonte_internal,
                       tf$expand_dims(getTau(dat,training = training),0L)),0L)
      }
      if(grepl(modelType, pattern = "variational_minimal")){
        Tau_mean_vec <- getTau_means()
        MeanDist_Tau_post = (tfd$Normal(Tau_mean_vec, Tau_sd_vec <- tf$nn$softplus(MeanDist_tau[,"SD"])))
        ETau_draw <- tf$expand_dims(MeanDist_Tau_post$sample(batchSize),0L)
      }

      SDDist_Y1_post = (tfd$Normal(tf$identity(SDDist_Y1[,"Mean"]), tf$nn$softplus(SDDist_Y1[,"SD"])))
      EY1SD_draw <- tf$expand_dims(tf$nn$softplus(SDDist_Y1_post$sample(batchSize)),0L)

      SDDist_Y0_post = (tfd$Normal(tf$identity(SDDist_Y0[,"Mean"]), tf$nn$softplus(SDDist_Y0[,"SD"])))
      EY0SD_draw <- tf$expand_dims(tf$nn$softplus(SDDist_Y0_post$sample(batchSize)),0L)

      tau_i <- tf$reduce_sum( tf$multiply(ETau_draw, clustT), 2L )
      EY1_i <- EY0_i + tau_i
      impliedATE <- tf$reduce_mean(tau_i,1L)
      Sigma2_Y0_i <- tf$reduce_sum(tf$multiply( EY0SD_draw^2, clustT),2L)
      Sigma2_Y1_i <- tf$reduce_sum(tf$multiply( EY1SD_draw^2, clustT),2L)
      treat <- tf$expand_dims( treat , 0L)
      Y_Sigma <- (tf$multiply( 1 - treat , Sigma2_Y0_i ) +
                    tf$multiply( treat, Sigma2_Y1_i ))^0.5
      Y_Mean <- tf$multiply( 1 - treat, EY0_i ) +
        tf$multiply( treat, EY1_i)

      # some commented analyses to triple-check code correctness re: initialization
      #plot(as.numeric(tf$reduce_mean(Y_Mean,0L)),as.numeric(y),col=as.numeric(treat)+1);abline(a=0,b=1)
      #lim_<-summary(c(as.numeric(tf$reduce_mean(Y_Mean,0L)),as.numeric(y)))[c(1,6)]
      #try({plot(as.numeric(tf$reduce_mean(Y_Mean,0L)),as.numeric(y),ylim=lim_,xlim=lim_,col=as.numeric(treat)+1);abline(a=0,b=1)},T)
      #print2( 1-sum( (as.numeric(tf$reduce_mean(Y_Mean,0L))-as.numeric(y))^2)/sum((as.numeric(y)-mean(as.numeric(y)))^2))
      if(yDensity == "normal"){
        likelihood_distribution_draws <- tfd$Normal(loc = Y_Mean, scale = Y_Sigma)
      }
      if(yDensity == "lognormal"){
        likelihood_distribution_draws <- tfd$LogNormal(loc = Y_Mean, scale = Y_Sigma)
      }
      likelihood_distribution_draw <- tf$reduce_mean(likelihood_distribution_draws$log_prob( tf$expand_dims(y,0L) ),0L)
      return( likelihood_distribution_draw )
    } )

    getExpectedLikelihood <- tf_function_fxn(function(dat,treat,y){
      likelihood_distribution_expectation <- tf$zeros(list(batchSize),tf$float32)
      for(d_ in 1:nMonte_variational){
        likelihood_distribution_expectation <- likelihood_distribution_expectation +
          getTrainingLikelihoodDraw(dat=dat,treat=treat,y=y) / tf$constant(f2n(nMonte_variational),tf$float32)
      }
      return( likelihood_distribution_expectation )
    })

    getLoss <- tf_function_fxn( function(dat,treat,y,training){
      if(modelType == "tarnet"){
        m <- getImageRep(dat,training = training)
        Y0_hat <- getY0_finalStep(m,training=training)
        Y1_hat <- getY1_finalStep(m,training=training)
        Yobs_hat <- tf$multiply(Y1_hat,tf$expand_dims(treat,1L)) + tf$multiply(Y0_hat,tf$expand_dims(1-treat,1L))
        minThis <- tf$reduce_sum(tf$square(tf$expand_dims(y,1L) - Yobs_hat))
      }

      if(grepl(modelType,pattern= "variational")){
        likelihood_distribution_expectation <- getExpectedLikelihood(dat=dat,
                                                                     treat=treat,
                                                                     y=y)

        # specify some distributions
        if(grepl(modelType, pattern = "variational_minimal")){
            Tau_mean_vec <- getTau_means()
            MeanDist_Tau_post = (tfd$Normal(Tau_mean_vec, Tau_sd_vec <- tf$nn$softplus(MeanDist_tau[,"SD"])))
        }
        SDDist_Y1_post = (tfd$Normal(tf$identity(SDDist_Y1[,"Mean"]), tf$nn$softplus(SDDist_Y1[,"SD"])))
        SDDist_Y0_post = (tfd$Normal(tf$identity(SDDist_Y0[,"Mean"]), tf$nn$softplus(SDDist_Y0[,"SD"])))

        # KL terms
        {
        # generate KL components
        KLterm <- tf$zeros(list())
        if(! modelType  %in% c("variational_minimal_visualizer")){
          for(conv_ in 1:nDepthHidden_conv){
            KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(ClusterConv%s$kernel_posterior,ClusterConv%s$kernel_prior)",conv_,conv_)))
            KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(Y0Conv%s$kernel_posterior,Y0Conv%s$kernel_prior)",conv_,conv_)))
            if((ifelse(LowerDimInputDense,yes = T, no = conv_ < nDepthHidden_conv)) & doConvLowerDimProj){
              KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(Y0ConvProj%s$kernel_posterior,Y0ConvProj%s$kernel_prior)",conv_,conv_)))
              KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(ClusterConvProj%s$kernel_posterior,ClusterConvProj%s$kernel_prior)",conv_,conv_)))
            }
            if(modelType == "variational_CNN"){
              for(k_ in 1:kClust_est){
                KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(TauConv%s_%s$kernel_posterior,TauConv%s_%s$kernel_prior)",conv_,k_,conv_,k_)))
                if((ifelse(LowerDimInputDense,yes = T, no = conv_ < nDepthHidden_conv)) & doConvLowerDimProj){
                  KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(TauConvProj%s_%s$kernel_posterior,TauConvProj%s_%s$kernel_prior)",conv_,k_,conv_,k_)))
                }
              }
            }
          }
          if(nDepthHidden_dense > 0){ for(dense_ in 1:nDepthHidden_dense){
            KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(DenseProj_Y0_%s$kernel_posterior,DenseProj_Y0_%s$kernel_prior)",nDepthHidden_dense, nDepthHidden_dense)))
            KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(DenseProj_Clust_%s$kernel_posterior,DenseProj_Clust_%s$kernel_prior)",nDepthHidden_dense, nDepthHidden_dense)))
          }}
        }
        KLterm <- KLterm + tfd$kl_divergence(ClusterProj$kernel_posterior,ClusterProj$kernel_prior)
        KLterm <- KLterm + tfd$kl_divergence(Y0Proj$kernel_posterior,Y0Proj$kernel_prior)
        if(modelType == "variational_minimal"){
          KLterm <- KLterm + tf$reduce_sum(tfd$kl_divergence(MeanDist_Tau_post, (MeanDist_tau[,"Prior"][[1]])))
        }
        KLterm <- KLterm + tf$reduce_sum(tfd$kl_divergence(SDDist_Y0_post, (SDDist_Y0[,"Prior"][[1]])))
        KLterm <- KLterm + tf$reduce_sum(tfd$kl_divergence(SDDist_Y1_post, (SDDist_Y1[,"Prior"][[1]])))
        }

        #plot(as.numeric(tf$reduce_mean(likelihood_distribution_draws$log_prob( tf$expand_dims(y,0L) ),0L)),as.numeric( y  ),col = as.numeric( treat)+1)
        minThis <- tf$negative(tf$reduce_sum( likelihood_distribution_expectation )) +
          KL_wt * KLterm    #optional ATE penalty: + tf$reduce_mean(tf$multiply(marginal_lambda, tf$square(marginal_tau - impliedATE)))
        minThis <- minThis / batchSize
      }
      return( minThis )
    })

    print2("Initial forward pass...")
    for(bool_ in c(T)){ # Initialize training branch only to preserve memory
      print2(bool_)
      with(tf$GradientTape() %as% tape, {
        samp_ <- sample(1:length(obsW),batchSize)
        myLoss_forGrad <- getLoss( dat = acquireImageFxn( imageKeysOfUnits[samp_], bool_),
                                   treat = tf$constant(obsW[samp_],tf$float32),
                                   y = tf$constant(obsY[samp_],tf$float32),
                                   training = bool_ )
        if(bool_==T){ trainable_variables  <- tape$watched_variables() }
      })
    }
    print2(sprintf("nTrainableParams: %i",nTrainableParams <- sum(unlist(lapply(unlist(trainable_variables,recursive=F),function(zer){
      len_<-try(length((zer)),T);return( len_ ) })))))

    # define optimizer and training step
    #optimizer_tf = tf$optimizers$legacy$Adam(learning_rate = LEARNING_RATE_BASE) #$,clipnorm=1e1)
    optimizer_tf = tf$optimizers$legacy$Nadam(learning_rate = LEARNING_RATE_BASE) #$,clipnorm=1e1)
    if(adaptiveMomentum == T){
      BETA_1_INIT <- 0.1
      optimizer_tf = tf$optimizers$legacy$Adam(learning_rate = 0,beta_1 = BETA_1_INIT)#$,clipnorm=1e1)
      #optimizer_tf = tf$optimizers$legacy$Nadam(learning_rate=LEARNING_RATE_BASE,beta_1 = BETA_1_INIT)#$,clipnorm=1e1)
    }
    #LR_method <- "WNGrad"
    LR_method <- "constant"
    InvLR <- tf$Variable(0.,trainable  =  F)

    trainStep <-  (function(dat,y,treat, training){

      with(tf$GradientTape() %as% tape, {
        myLoss_forGrad <<- getLoss( dat = dat,
                                    treat = treat,
                                    y = y,
                                    training = training)  })
      my_grads <<- tape$gradient( myLoss_forGrad, tape$watched_variables() )

      # update LR
      #optimizer_tf$learning_rate$assign(   LEARNING_RATE_BASE*abs(cos(i/nSGD*widthCycle)  )*(i<=nSGD/2)+LEARNING_RATE_BASE*(i>nSGD/2)/(0.001+abs(i-nSGD/2)^0.2 )   )
      L2_grad_i <<- sqrt(sum((grad_i <- as.numeric(tf$concat(lapply(my_grads,function(x) tf$reshape(x,list(-1L,1L))),0L) ))^2) )
      x_i <- as.numeric( tf$concat((lapply(trainable_variables, function(zer){tf$reshape(zer,-1L)})),0L))
      if(LR_method == "WNGrad"){ if(i == 1){
        L2_grad_init <<- L2_grad_scale*L2_grad_i
        InvLR$assign(  L2_grad_init )
        print2(sprintf("Initial LR: %.3f",1/L2_grad_init))
      } }
      if(i > 1) { InvLR$assign_add( tf$divide( L2_grad_i,InvLR ) ) }
      {
        optimizer_tf$apply_gradients( rzip(my_grads, trainable_variables))
        if(LR_method == "WNGrad"){
          optimizer_tf$learning_rate$assign( tf$math$reciprocal( InvLR ) )
        }
      }

      # update momentum
      if(adaptiveMomentum == T){
        if(i >= 4){
          DENOM <- sqrt( sum((x_i-x_i_minus_1)^2))
          NUM <- sqrt( sum((grad_i-grad_i_minus_1)^2))
          LR_current <- as.numeric( optimizer_tf$learning_rate  )
          #UPPER_MOM <- 1-10^(-3)
          UPPER_MOM <- 1-10^(-2)
          MomenetumNextIter <<- max(0,min((1-sqrt(LR_current*NUM/(0.000001+DENOM)))^2,UPPER_MOM))
          #optimizer_tf$momentum$assign(MomenetumNextIter)
          optimizer_tf$beta_1$assign( MomenetumNextIter )
        }
        x_i_minus_1 <<- x_i
        grad_i_minus_1 <<- grad_i
      }
    })

    # perform training jump
    print2("Starting training...")
    if(BAYES_STEP == 2){
      eval(parse(text = sprintf("rm(%s)",paste(ls()[grepl(ls(),pattern="HASH818")] ,collapse= ',') )))
    }
    trainIndices <-sort(sample(1:length(obsW),length(obsW) - (nTest <- 0L)))
    testIndices <-sort(  (1:length(obsW))[! 1:length(obsW) %in% trainIndices] )
    if(length(testIndices)==0){testIndices <- trainIndices}
    L2grad_vec <- loss_vec <- rep(NA,times=(nSGD))
    batch_indices_list <- (sapply(1:(max(1,round((batchSize*nSGD) / length(trainIndices)))),function(zer){
      zer*length(trainIndices)+sample(1:length(trainIndices) %% ceiling(length(trainIndices)/batchSize)+1)
    }))

    # training loop
    IndicesByW <- tapply(1:length(obsW),obsW,c)
    UniqueImageKeysByW <- tapply(imageKeysOfUnits,obsW,function(zer){sort(unique(zer))})
    UniqueImageKeysByIndices <- list(tapply(which(obsW==0),imageKeysOfUnits[obsW==0],function(zer){sort(unique(zer))}),
                                     tapply(which(obsW==1),imageKeysOfUnits[obsW==1],function(zer){sort(unique(zer))}))
      #tauMeans <- c();i_<-1;for(i in i_:(n_sgd_iters <- length(unique_batch_indices <- sort(unique(c(batch_indices_list)))))){
      n_sgd_iters <- nSGD; tauMeans <- c();i_<-1;for(i in i_:nSGD){
      if(i %% 25 == 0){gc(); py_gc$collect()}
      #batch_indices <- unlist(apply(batch_indices_list == unique_batch_indices[i],2,which))
      #batch_indices_reffed <- trainIndices[batch_indices]
      #keys_SELECTED <- sample(unique(imageKeysOfUnits),batchSize)
      #batch_indices_reffed <- sapply(keys_SELECTED,function(zer){
      #f2n(sample(as.character(which(imageKeysOfUnits %in% zer)),1L)) })
      batch_indices_reffed <-  c(  sapply(1:2, function(ze){
        w_keys <- UniqueImageKeysByW[[ze]]
        samp_w_keys <- sample(unique(w_keys),max(1,round(batchSize/2)))
        unlist(  lapply(UniqueImageKeysByIndices[[ze]][as.character(samp_w_keys)],function(fa){
          f2n(sample(as.character(fa),1))
        }) ) }))
      #table(  obsW[batch_indices_reffed] )

      #table(YandW_mat$geo_long_lat_key[batch_indices_reffed])
      trainStep(dat = acquireImageFxn( imageKeysOfUnits[batch_indices_reffed], training = T ),
                y = tf$constant(obsY[batch_indices_reffed],tf$float32),
                treat = tf$constant(obsW[batch_indices_reffed],tf$float32),
                training = T)
      loss_vec[i] <- myLoss_forGrad <- as.numeric( myLoss_forGrad )
      L2grad_vec[i] <- as.numeric( L2_grad_i )
      if(is.na(myLoss_forGrad)){stop("Stopping: NA in loss function! Perhaps batchSize is too small?")}
      i_ <- i ; if(i %% 20 == 0 | i == 1){
        print2(sprintf("SGD iteration %i of %i",i,n_sgd_iters));par(mfrow = c(1,1));
        if(!quiet){
          try({plot(loss_vec,log="y",main="If Still Decreasing at End of Training, \n Try Increasing nSGD",cex.main = 0.95,ylab = "Loss Function Value",xlab="SGD Iteration Number");points(smooth.spline( na.omit(loss_vec) ),col="red",type = "l",lwd=5)},T)
        }
        if(modelType == "variational_minimal"){ print2( paste("Current estimate, tau cluster means: ", paste(round(as.numeric(getTau_means()),3L),collapse=", "), collapse =  "")) }
      }
      if(BAYES_STEP == 1){
      if(abs(i - n_sgd_iters - 1) <= (nWindow <- 20)){
        windowCounter <- windowCounter + 1
        z_counter <- 0
        for(z in tape$watched_variables()){
          z_counter <- z_counter + 1
          z_name_orig <- z_name_ <- z$name
          z_name_ <- gsub(z_name_, pattern = ":",replace = "XCOLX")
          z_name_ <- gsub(z_name_, pattern = "/",replace = "XDASHX")
          #https://monolix.lixoft.com/tasks/standard-error-using-the-fisher-information-matrix/
          if(windowCounter == 1){
            eval(parse(text = sprintf("SZ_%s <- tf$zeros(tf$shape(z))", z_name_)))
            eval(parse(text = sprintf("SZ2_%s <- tf$zeros(tf$shape(z))", z_name_)))
          }
          #eval(parse(text = sprintf("SZ_%s <- z + SZ_%s", z_name_, z_name_)))
          #eval(parse(text = sprintf("SZ2_%s <- tf$square(z) + SZ2_%s", z_name_, z_name_)))
          eval(parse(text = sprintf("SZ_%s <- my_grads[[z_counter]] + SZ_%s", z_name_, z_name_)))
          eval(parse(text = sprintf("SZ2_%s <- tf$square( my_grads[[z_counter]] ) + SZ2_%s", z_name_, z_name_)))
          if(i == n_sgd_iters){
            #eval(parse(text = sprintf("RollMean_%s <- (SZ_%s)/nWindow", z_name_, z_name_)))
            #eval(parse(text = sprintf("RollVar_%s <- tf$maximum(0.0001,SZ2_%s/nWindow - RollMean_%s^2)", z_name_,z_name_,z_name_,z_name_)))
            eval(parse(text = sprintf("RollVar_%s <- 1/tf$maximum(0.5,length(obsY)*SZ2_%s/nWindow)", z_name_,z_name_)))
          }
      }
      }
      }
    }
  }
  try(plot(L2grad_vec),T)
  #try({par(mfrow = c(1,1));plot(loss_vec, xlab = "SGD Iteration", ylab = "Loss Function Value");try(points(smooth.spline( na.omit(loss_vec) ),col="red",type = "l",lwd=5),T)},T)
  print2("Getting predicted potential outcome means...")
  for(y_t_ in c(0,1)){
    test_tab <- sort( 1:length(testIndices)%%round(length(testIndices)/max(1,round(batchFracOut*batchSize))));
    Y_test_est <-  tapply(testIndices,test_tab,function(zer){
      if(runif(1)<0.1){ gc(); py_gc$collect() }
      atP <- max(zer)/length(test_tab)
      if( any(zer %% 100 == 0) ){ print2(sprintf("Proportion done: %.3f",atP)) }
      im_zer <- acquireImageFxn(  imageKeysOfUnits[zer], training = F)
      l_ <- replicate(nMonte_predictive,
                      eval(parse(text = sprintf("list(tf$expand_dims(getY%s(m=im_zer, training = F),0L))",y_t_))))
      names(l_) <- NULL;
      l_ <- tf$concat(l_,0L)
      as.matrix(tf$reduce_mean(l_,0L))
    })
    Y_test_est <- do.call(rbind,Y_test_est)
    eval(parse(text = sprintf("Y%s_test_est <- Y_test_est",y_t_)))

    # get out of sample predictions
    Y_test_truth <- obsY[testIndices]
    W_test <- obsW[testIndices]
    yt_true <- Y_test_truth[W_test==y_t_]
    yt_est <- as.numeric(Y_test_est)[W_test==y_t_]
    yt_lims <- summary(c(yt_true,yt_est))[c(1,6)]
    if(printDiagnostics == T){
      #print((summary(lm(yt_true~yt_est))))
    }
    r2_yt_out <- 1 - sum( (yt_est - yt_true)^2 ) / sum( (yt_true - mean(yt_true))^2 )
    if(y_t_ == 0){ r2_y0_out <- r2_yt_out }
    if(y_t_ == 1){ r2_y1_out <- r2_yt_out }
    rm( Y_test_est )
  }

  try({plot(y0_true,y0_est,ylim = y0_lims,xlim=y0_lims);abline(a=0,b=1,lty=2,col="gray")},T)
  try({plot(y1_true,y1_est,ylim = y1_lims,xlim=y1_lims);abline(a=0,b=1,lty=2,col="gray")},T)

  rm(optimizer_tf);gc(); py_gc$collect()

  # get cluster probs
  print2("Getting predicted cluster probabilities....")
  batch_indices_tab <- sort( 1:length(obsY)%%round(length(obsY)/max(1,ceiling(batchFracOut*batchSize))))
  if(modelType == "tarnet" | modelType=="variational_CNN"){
    Y0_est <- do.call(rbind,tapply(1:length(batch_indices_tab),batch_indices_tab, function(indi_){
      if(runif(1)<0.1){ gc(); py_gc$collect() }
      atP <- max(indi_/length(obsY))
      if( any(zer %% 100 == 0) ){ print2(sprintf("Proportion Done: %.3f",atP)) }
      im_indi <- acquireImageFxn( imageKeysOfUnits[indi_],training = F)
      as.matrix(tf$reduce_mean(tf$concat(replicate(nMonte_predictive,getY0(im_indi,training = F)),1L),1L))
    }))
    Y1_est <- do.call(rbind,tapply(1:length(batch_indices_tab),batch_indices_tab, function(indi_){
      if(runif(1)<0.1){ gc(); py_gc$collect() }
      atP <- max(indi_/length(obsY))
      if( any(zer %% 100 == 0) ){ print2(sprintf("Proportion Done: %.3f",atP)) }
      im_indi <- acquireImageFxn( imageKeysOfUnits[indi_],training = F)
      as.matrix(tf$reduce_mean(tf$concat(replicate(nMonte_predictive,getY1(im_indi,training = F)),1L),1L))
    }))
    Y0_est <- Rescale(Y0_est, doMean = T)
    Y1_est <- Rescale(Y1_est, doMean = T)
    tau_i_est <- Rescale( Y1_est - Y0_est, doMean = F)
    impliedATE <- mean(  tau_i_est )

    clusters_info <- kmeans(tau_i_est,centers=2)
    tau_vec <- c( Rescale(clusters_info$centers, doMean = F ) )
    tau1 <- c(tau_vec[1]); tau2 <- c(tau_vec[2])
  }

  if(grepl(modelType, pattern = "variational")){
    print2("Starting estimates for cluster probabilities")
    ClusterProbs_est <- tapply(1:length(batch_indices_tab),batch_indices_tab, function(indi_){
      atP <- max(indi_/length(obsY))
      if( any(indi_ %% 100 == 0) ){ print2(sprintf("Proportion Done: %.3f",atP)) }
      if(runif(1)<0.1){ gc(); py_gc$collect() }
      im_indi <- acquireImageFxn( imageKeysOfUnits[indi_],training = F)
      ClusterProbs_est_ <- replicate(nMonte_predictive,as.matrix(getClusterProb(im_indi,training = F)))
      ClusterProbs_std_ <- apply(ClusterProbs_est_,1:2,function(re){sd(re,na.rm=T)})
      ClusterProbs_est_ <- apply(ClusterProbs_est_,1:2,function(re){mean(re,na.rm=T)})
      ClusterProbs_lower_conf_ <- ClusterProbs_est_ - 0. * ClusterProbs_std_
      return(list("ClusterProbs_est_"=ClusterProbs_est_,
                  "ClusterProbs_lower_conf_"=ClusterProbs_lower_conf_,
                  "ClusterProbs_std_"=ClusterProbs_std_))
    } )
    ClusterProbs_lower_conf <- do.call(rbind,lapply(ClusterProbs_est,function(zer) zer$ClusterProbs_lower_conf_))
    ClusterProbs_std <- do.call(rbind,lapply(ClusterProbs_est,function(zer) zer$ClusterProbs_std_))
    ClusterProbs_est <- lapply(ClusterProbs_est,function(zer) zer$ClusterProbs_est_)
    ClusterProbs_est <- (ClusterProbs_est_full <- do.call(rbind, ClusterProbs_est))[,2]
    Clust_probs_marginal_final <- colMeans( ClusterProbs_est_full )

    if(grepl(modelType,pattern="variational_minimal")){
      impliedATE <- mean(replicate(100,sum(Y_sd*as.numeric(getTau_means())*Clust_probs_marginal_final)))
    }
    gc(); py_gc$collect()

    # characterizing the treatment effects
    print2("Summarizing results...")
    SDDist_Y1_post = (tfd$Normal(tf$identity(SDDist_Y1[,"Mean"]), tf$nn$softplus(SDDist_Y1[,"SD"])))
    SDDist_Y0_post = (tfd$Normal(tf$identity(SDDist_Y0[,"Mean"]), tf$nn$softplus(SDDist_Y0[,"SD"])))
    Sigma1_sd_vec <- as.numeric(tf$reduce_mean(tf$nn$softplus(SDDist_Y1_post$sample(100L)),0L))
    Sigma0_sd_vec <- as.numeric(tf$reduce_mean(tf$nn$softplus(SDDist_Y0_post$sample(100L)),0L))

    # get uncertainties
    if(grepl(modelType,pattern="variational_minimal")){
      tau_vec <- as.numeric( getTau_means() ) * Y_sd
      for(k_ in 1:kClust_est){eval(parse(text = sprintf("tau%s <- tau_vec[k_]",k_))) }
      Tau_mean_vec <- getTau_means()
      MeanDist_Tau_post = (tfd$Normal(Tau_mean_vec, Tau_sd_vec <- tf$nn$softplus(MeanDist_tau[,"SD"])))
      Tau_sd_vec_ <- as.numeric(tf$sqrt(tf$math$reduce_variance(MeanDist_Tau_post$sample(100L),0L)))
      Tau_sd_vec <- sqrt(   Sigma1_sd_vec^2 + Sigma0_sd_vec^2 + Tau_sd_vec_^2 )
      Tau_sd_vec <- Tau_sd_vec * Y_sd
      sd_tau1 <- Tau_sd_vec[1]
      sd_tau2 <- Tau_sd_vec[2]
    }

    # obtaining the neg log likelihood if desired
    negELL <- NA; if(T == F){
      # obtaining the negative LL
      KL_wt_orig <- KL_wt
      if(! "function" %in% class(getLoss)){print2("getLoss must be R function for this part to work!")}
      KL_wt <- 0
      negELL <- tapply(1:length(batch_indices_tab),batch_indices_tab, function(indi_){
        ret_ <- as.numeric(getLoss( dat = acquireImageFxn(   imageKeysOfUnits[indi_]  , training = F),
                                    treat = tf$constant(obsW[indi_],tf$float32),
                                    y = tf$constant(obsY[indi_],tf$float32),
                                    training = F ))
        return( ret_ )
      })
      negELL <- sum(negELL)
      KL_wt <- KL_wt_orig
    }
  }

  # transportability analysis
  cluster_prob_transport_means <- NULL
  if(!is.null(transportabilityMat)){
    print2("Getting posterior predictive mean probabilities for transportability analysis...")
    {
      GetProbAndExpand <- tf_function_fxn(function(m){tf$expand_dims(getClusterProb(m,training = F),0L) })
      full_tab <- sort( 1:nrow(transportabilityMat) %% round(nrow(transportabilityMat)/max(1,round(batchFracOut*batchSize))));
      cluster_prob_transport_info <- tapply(1:nrow(transportabilityMat),full_tab,function(zer){
        if(runif(1)<0.1){ gc(); py_gc$collect() }
        atP <- max(  zer / nrow(transportabilityMat))
        if((round(atP,2)*100) %% 10 == 0){ print2(atP) }
        im_keys <- acquireImageFxn(  transportabilityMat$key[zer], training = F )
        pred_ <- replicate(nMonte_predictive,as.array(GetProbAndExpand(im_keys) ))
        list("mean"=apply(pred_[1,,,],1:2,mean),
             "var"=apply(pred_[1,,,],1:2,var))
      })
      cluster_prob_transport_info <- do.call(rbind,cluster_prob_transport_info)
      cluster_prob_transport_means <- do.call(rbind, cluster_prob_transport_info[,1])
      cluster_prob_transport_var <- do.call(rbind, cluster_prob_transport_info[,2])
      colnames(cluster_prob_transport_means) <- paste('mean_k',1:ncol(cluster_prob_transport_means), sep = "")
      colnames(cluster_prob_transport_var) <- paste('var_k',1:ncol(cluster_prob_transport_means), sep = "")
      transportabilityMat <- try(cbind(transportabilityMat,
                                   cluster_prob_transport_means,
                                   cluster_prob_transport_var),T)
      if("try-error" %in% class(transportabilityMat)){ browser() }
    }
  }


  if(!c("monti") %in% ls(envir = globalenv()) & simMode == F){
    pdf_name_key <- "RealDataFig"; monti <- NA
  }
  if( plotResults == T){
    if(modelType == "variational_minimal"){
      Tau_mean_vec_n <- as.numeric(Tau_mean_vec)
      synth_seq <- seq(min(Tau_mean_vec_n - 2 * as.numeric(Tau_sd_vec),Tau_mean_vec_n - 2 * as.numeric(Tau_sd_vec)),
                       max(Tau_mean_vec_n + 2 * as.numeric(Tau_sd_vec),Tau_mean_vec_n + 2 * as.numeric(Tau_sd_vec)),
                       length.out=1000)

      if(truthKnown <- ("ClusterProbs" %in% ls(envir = globalenv()))){
        my_density <- density(ClusterProbs)
        my_density$y <- my_density$y
        synth_seq <- seq(min(my_density$x,na.rm=T),max(my_density$x,na.rm=T),length.out=100)
      }

      for(kr_ in 1:kClust_est){
        eval(parse(text = sprintf("d%s <- dnorm(synth_seq, mean = Tau_mean_vec_n[kr_], sd = Tau_sd_vec[kr_] )",  kr_)))
      }
      pdf(sprintf("%s/HeteroSimTauDensity%s_%s_ExternalFigureKey%s.pdf",figuresPath, pdf_name_key, modelType, figuresTag))
      {
        par(mar=c(5,5,1,1))
        numbering_seq <- 1:kClust_est #c("1","1")
        col_seq <- 1:kClust_est #c("black","black")
        #col_seq[which.max(c(as.numeric(tau1),as.numeric(tau2)))] <- "red"
        #numbering_seq[which.max(c(as.numeric(tau1),as.numeric(tau2)))] <- 2
          if(!truthKnown){
            my_density <- eval(parse(text = sprintf("
                        data.frame('y'=max(c(%s)))
                        ", paste(paste("d",1:kClust_est,sep=""),collapse = ","))))
          }
          plot(my_density,
               col=ifelse(truthKnown,yes="darkgray",no="white"),
               lty = 2,
               xlim = c(min(synth_seq),max(synth_seq)),
               ylim = c(0,max(my_density$y,na.rm=T)*1.5),
               cex.lab = 2, main = "",lwd = 3,
               ylab = "Density",
               xlab = "Per Image Treatment Effect")
          if(!truthKnown){ axis(2,cex.axis = 1) }
          points(tau_vec,rep(0,times=kClust_est),
                 col = col_seq,
                 pch = "|", cex = 4)
          for(krk_ in 1:kClust_est){
            points(synth_seq, eval(parse(text = sprintf("d%s",krk_))),
                   type = "l", col= col_seq[krk_],lwd = 3)
          }
          #text(tau_vec, rep(max(my_density,na.rm=T)*0.1,2),
               #labels = sapply(numbering_seq, function(numbering_){
                 #eval(parse(text = sprintf("expression(hat(tau)(%s))", numbering_))) }),
               #col = col_seq, cex = 2)
          legends_seq_vec <- c(expression("True"~p(Y[i](1)-Y[i](0)~"|"~M[i])),
                               sapply(numbering_seq, function(numbering_){
                                    eval(parse(text=sprintf('
              expression(hat(p)~"("~Y[i](1)-Y[i](0)~"|"~Z[i]==%s~")")',numbering_)))}))
          lty_seq_vec <- c(2,1,1)
          col_seq_vec <- c("gray",col_seq)
          if(!truthKnown){
            legends_seq_vec <- legends_seq_vec[-1]
            col_seq_vec <- col_seq_vec[-1]
            lty_seq_vec <- lty_seq_vec[-1]
          }
          legend("topleft", legend = legends_seq_vec,
                 box.lwd = 0, box.lty = 0, cex = 2,
                 lty = lty_seq_vec, col = col_seq_vec, lwd = 3)
      }
      dev.off()

      if(truthKnown){
        order_ <- order(ClusterProbs)
        if(cor(ClusterProbs, ClusterProbs_est) > 0){
          # we do this so the coloring stays consistent
          col_dim <- rank(ClusterProbs_est)#gtools::quantcut(ClusterProbs_est, q = 100)
        }
        if(cor(ClusterProbs, ClusterProbs_est) < 0){
          col_dim <- rank(-ClusterProbs_est)#gtools::quantcut(ClusterProbs_est, q = 100)
        }
        pdf(sprintf("%s/HeteroSimClusterEx%s_ExternalFigureKey%s.pdf",figuresPath, pdf_name_key, figuresTag))
        {
          par(mar=c(5,5,1,1))
          plot( ClusterProbs[order_],
                1:length(order_)/length(order_),
                xlab = "Per Image Treatment Effect",
                ylab = "Empirical CDF(x)",pch = 19,
                col = viridis::magma(n=length(col_dim),alpha=0.9)[col_dim][order_],
                cex = 1.5, cex.lab = 2)
          legend("topleft",
                 box.lwd = 0, box.lty = 0,
                 pch = 19, #box.col = "white",
                 col = c(viridis::magma(5)[2],
                         viridis::magma(5)[3],
                         viridis::magma(5)[4]),
                 cex = 2,
                 legend=c("Higher Clust 1 Prob.",
                          "...",
                          "Higher Clust 2 Prob."))
        }
        dev.off()
      }
    }
  }
  if(simMode == F){
    par(mfrow=c(1,1))
    gc(); py_gc$collect()
    try(plot(ClusterProbs_est),T)
    plotting_coordinates_list <- list(); typePlot_counter <- 0
    for(typePlot in (typePlot_vec <- c("uncertainty","mean","mean_upperConf"))){
      typePlot_counter <- typePlot_counter + 1
      rows_ <- kClust_est; nExamples <- 5
      if(typePlot == "uncertainty"){rows_ <- 1L}
      plot_fxn <- function(){
        pdf(sprintf("%s/VisualizeHeteroReal_%s_%s_%s_ExternalFigureKey%s.pdf",figuresPath, modelType,typePlot,orthogonalize,figuresTag),
            height = ifelse(grepl(typePlot,pattern = "mean"), yes = 4*rows_*3, no = 4),
            width = 4*nExamples)
        {
          #
          if(grepl(typePlot,pattern = "mean")){
            par(mar=c(2, 5.9, 3, 0.5))
            layout_mat_orig <- layout_mat <- matrix(c(1:nExamples*3-3+1,
                                   1:nExamples*3-1,
                                   1:nExamples*3), nrow = 3, byrow = T)
            for(kr_ in 2:kClust_est){
              layout_mat <- rbind(layout_mat,
                                  layout_mat_orig+max(layout_mat))
            }
          }
          if(typePlot %in% c("uncertainty")){
            par(mar=c(2,2,4,2)); layout_mat <- t(1:nExamples)
          }
          layout(mat = layout_mat,
                 widths = rep(2,ncol(layout_mat)),
                 heights = rep(2,nrow(layout_mat)))
          #reNormIm <- function(ar){for(ib in 1:NBANDS){ar[,,ib] <- (ar[,,ib])*NORM_SD[ib] + NORM_MEAN[ib]  };return(ar) }
          reNormIm <- function(ar){for(ib in 1:NBANDS){ar <- ar*NORM_SD + NORM_MEAN  };return(ar) }
          plotting_coordinates_mat <- c()
          total_counter <- 0
          for(k_ in 1:rows_){
            used_coordinates <- c()
            for(i in 1:5){
              #if(k_ == 2 & typePlot == "mean"){ browser() }
              #if(k_ == 2 & i == 1){ browser() }
              print2(sprintf("Type Plot: %s; k_: %s, i: %s", typePlot, k_, i))
              total_counter <- total_counter + 1
              rfxn <- function(xer){xer}
              bad_counter <- 0;isUnique_ <- F; while(isUnique_ == F){
                BreakTies <- function(x){x + runif(length(x),-1e-3,1e-3)}
                if(typePlot == "uncertainty"){
                  main_ <- letters[  total_counter  ]

                  # plot images with largest std's
                  valBrokenTies <- BreakTies(ClusterProbs_std[,k_])
                  sorted_unique_prob_k <- sort(rfxn(unique(valBrokenTies)),decreasing=T)
                  im_i <- which(valBrokenTies == sorted_unique_prob_k[i+bad_counter])[1]
                }
                if(grepl(typePlot,pattern = "mean")){
                  main_ <- total_counter

                  # plot images with largest lower confidence
                  if(typePlot ==  "mean"){
                     valBrokenTies <- BreakTies(ClusterProbs_lower_conf[,k_])
                     sorted_unique_prob_k <- sort(rfxn(unique(valBrokenTies)),decreasing=T)
                     im_i <- which(valBrokenTies == sorted_unique_prob_k[i+bad_counter])[1]
                  }

                  # plot images with largest cluster probs
                  if(typePlot ==  "mean_upperConf"){
                    valBrokenTies <- BreakTies(ClusterProbs_est_full[,k_])
                    sorted_unique_prob_k <- sort(rfxn(unique(valBrokenTies)),decreasing=T)
                    im_i <- which(valBrokenTies == sorted_unique_prob_k[i+bad_counter])[1]
                  }
                }

                coordinate_i <- c(long[im_i], lat[im_i])
                if(bad_counter>50){browser()}
                if(i > 1){
                  isUnique_ <- T; if(!is.null(long)){
                    dist_m <- geosphere::distm(coordinate_i, used_coordinates, fun = geosphere::distHaversine)
                    bad_counter <- bad_counter + 1
                    if(all(dist_m >= 1000)){isUnique_ <- T}
                } }
                if(i == 1){isUnique_<-T}
                print2(sd_im <- sd(as.array(acquireImageFxn(  imageKeysOfUnits[im_i], training = F )[1,,,]),na.rm=T))
                if(sd_im < .5){ bad_counter <- bad_counter + 1; isUnique_ <- F }
              }

              used_coordinates <- rbind(coordinate_i,used_coordinates)
              print2(c(k_, i, im_i, long[im_i], lat[im_i]))
              if(is.na(sum((as.array(acquireImageFxn( imageKeysOfUnits[im_i] )[1,,,]))))){ browser() }

              if(length(plotBands) < 3){
                orig_scale_im_raster <-  (as.array(acquireImageFxn(
                      imageKeysOfUnits[im_i], training = F )[1,,,plotBands[1]]))
                showImage <- causalimages::image2(
                  as.matrix( orig_scale_im_raster ),
                  main = main_, cex.main = 4,
                  cex.lab = 2.5, col.lab = k_, col.main = k_,
                  xlab = ifelse(!is.null(long),
                                yes = sprintf("Long: %s, Lat: %s",
                                              fixZeroEndings(round(coordinate_i,2L)[1],2L),
                                              fixZeroEndings(round(coordinate_i,2L)[2],2L)),
                                no = ""))
              }
              if(length(plotBands) >= 3){
                orig_scale_im_raster <- raster::brick( 0.0001 + (as.array(acquireImageFxn(
                                        imageKeysOfUnits[im_i], training = F )[1,,,plotBands])) )
                showImage <- (raster::plotRGB(  orig_scale_im_raster,
                                 margins = T,
                                 r = 1, g = 2, b = 3,
                                 mar = (margins_vec <- (ep_<-1e-6)*c(1,3,1,1)),
                                 main = main_,  stretch = "lin",
                                 cex.lab = 2.5, col.lab = k_,
                                 xlab = ifelse(!is.null(long),
                                           yes = sprintf("Long: %s, Lat: %s",
                                                fixZeroEndings(round(coordinate_i,2L)[1],2L),
                                                fixZeroEndings(round(coordinate_i,2L)[2],2L)),
                                           no = ""),
                                 col.main = k_, cex.main=4))
              }
              if("try-error" %in% class(showImage)){print2("showImage broken")}
              if(grepl(typePlot,pattern = "mean")){
                # axis for plot
                ylab_ <- ""; if(i==1){
                  tauk <- eval(parse(text = sprintf("tau%s",k_)))
                  ylab_ <- eval(parse(text = sprintf("expression(hat(tau)[%s]==%.3f)",k_,tauk)))
                  if(orthogonalize == T){
                    ylab_ <- eval(parse(text = sprintf("expression(hat(tau)[%s]^{phantom() ~ symbol('\136') ~ phantom()}==%.3f)",k_, tauk)))
                  }
                  axis(side = 2,at=0.5,labels = ylab_,pos=-0.,tick=F,cex.axis=cex_tile_axis <- 4,
                       col.axis=k_)
                }

                #obtain image gradients
                {
                  take_k <- k_
                  if(i == 1){
                    ImageGrad_fxn <- (function(m){
                      m <- tf$Variable(m,trainable = T)
                      with(tf$GradientTape(watch_accessed_variables = F,persistent  = T) %as% tape, {
                        tape$watch( m )
                        PROBS_ <- tf$reduce_mean(tf$concat(
                          replicate(nMonte_salience, getClusterProb(m,training = F)),0L),0L)
                        PROBS_Smoothed <- tf$add(tf$multiply(tf$subtract(tf$constant(1), ep_LabelSmooth<-tf$constant(0.01)),PROBS_),
                                                tf$divide(ep_LabelSmooth,tf$constant(2)))
                        LOGIT_ <- tf$subtract(tf$math$log(PROBS_Smoothed),
                                              tf$math$log(tf$subtract(tf$constant(1), PROBS_Smoothed) ))
                      })
                      ImageGrad <- tape$jacobian( LOGIT_, m , experimental_use_pfor = F)
                      ImageGrad_o <- tf$gather(ImageGrad, indices = as.integer(take_k-1L), axis = 0L)
                      for(jf in 1:2){
                        if(jf == 1){ImageGrad <- tf$math$reduce_euclidean_norm(ImageGrad_o+0.0000001,3L,keepdims = T)}
                        if(jf == 2){ImageGrad <- tf$math$reduce_mean(ImageGrad_o,3L, keepdims = T)}
                        ImageGrad <- tf$gather(AveragingConv(ImageGrad),0L,axis = 0L)
                        if(jf == 1){ImageGrad_L2 <- ImageGrad}
                        if(jf == 2){ImageGrad_E <- ImageGrad}
                      }
                      return(tf$concat(list(ImageGrad_L2,ImageGrad_E),2L))
                    })
                    AveragingConv <- tf$keras$layers$Conv2D(filters=1L,
                                                            kernel_size = gradAnalysisFilterDim <- 10L,
                                                            padding = "valid")
                    AveragingConv( tf$expand_dims(tf$gather(acquireImageFxn(  imageKeysOfUnits[im_i],training = F),1L, axis = 3L),3L)  )
                    AveragingConv$trainable_variables[[1]]$assign( 1 / gradAnalysisFilterDim^2 *tf$ones(tf$shape(AveragingConv$trainable_variables[[1]])) )
                  }
                  IG <- as.array( ImageGrad_fxn( acquireImageFxn( imageKeysOfUnits[im_i], training = F) ) )
                  nColors <- 1000
                  { #if(i == 1){
                    # pos/neg breaks should be on the same scale across observation
                    pos_breaks <- sort( quantile(c(IG[,,2][IG[,,2]>=0]),probs = seq(0,1,length.out=nColors/2),na.rm=T))
                    neg_breaks <- sort(quantile(c(IG[,,2][IG[,,2]<=0]),probs = seq(0,1,length.out=nColors/2),na.rm=T))
                    gradMag_breaks <- sort(quantile((c(IG[,,1])),probs = seq(0,1,length.out = nColors),na.rm=T))
                  }

                  # magnitude
                  print2(summary(c( IG[,,1] )))
                  magPlot <- try(image(t(IG[,,1])[,nrow(IG[,,1]):1],
                            col = viridis::magma(nColors - 1),
                            breaks = gradMag_breaks, axes = F),T)
                  if("try-error" %in% class(magPlot)){
                    browser()
                    print2("magPlot broken")
                  }
                  ylab_ <- ""; if(i==1){
                    axis(side = 2,at=0.5,labels = "Salience Magnitude",
                         pos=-0.,tick=F, cex.axis=3, col.axis=k_)
                  }

                  # direction
                  dirPlot <- try(image(t(IG[,,2])[,nrow(IG[,,2]):1],
                            col = c(hcl.colors(nColors/2 - 1,"reds"),
                                    hcl.colors(nColors/2 ,"blues")),
                            breaks = c(neg_breaks,pos_breaks), axes = F),T)
                  if("try-error" %in% class(dirPlot)){print2("dirPlot broken")}
                  ylab_ <- ""; if(i==1){
                    axis(side = 2,at=0.5,labels = "Salience Direction",
                         pos=-0.,tick=F, cex.axis=3, col.axis=k_)
                  }
                }
              }
            }
            plotting_coordinates_mat <- try(rbind(plotting_coordinates_mat, used_coordinates),T)
            if("try-error" %in% class(plotting_coordinates_mat)){browser()}
            print2(used_coordinates)
          }
        }
        dev.off()
        return( plotting_coordinates_mat )
      }
      plotting_coordinates_mat <- try(plot_fxn(),T)
      if("try-error" %in% class(plotting_coordinates_mat)){ browser() }
      plotting_coordinates_list[[typePlot_counter]] <- plotting_coordinates_mat
    }
    try({ names(plotting_coordinates_list) <- typePlot_vec},T)
    par(mfrow=c(1,1))

    return( list(
                 "clusterTaus_mean" = as.numeric(tau_vec),
                 "clusterTaus_sd" = Tau_sd_vec,
                 "clusterProbs_mean" = ClusterProbs_est_full,
                 "clusterProbs_sd" = ClusterProbs_std,
                 "clusterProbs_lowerConf" = ClusterProbs_lower_conf,
                 "impliedATE" = impliedATE,
                 "individualTau_est" = tau_i_est,
                 "transportabilityMat" = transportabilityMat,
                 "plottedCoordinatesList" = plotting_coordinates_list,
                 #"negELL"=as.numeric(negELL),
                 "whichNA_dropped" = whichNA_dropped) )
  }
  if(simMode == T){
    return(list("ClusterProbs_est" = ClusterProbs_est,
                "ClusterProbs" = ClusterProbs,
                "Cluster" = as.numeric(tau1),
                "tau2" = as.numeric(tau2),
                "sd_tau1" = sd_tau1,
                "sd_tau2" = sd_tau2,
                "R2_Y0"=r2_y0_out,
                "R2_Y1"=r2_y1_out,
                "tau_i_est"=tau_i_est,
                "impliedATE" = impliedATE,
                "y0_true_out" = y0_true,
                "y1_true_out" = y1_true,
                "y0_est_out" = y0_est,
                "y1_est_out" = y1_est,
                "negELL"=as.numeric(negELL),
                "whichNA_dropped" = whichNA_dropped ))
  }
}
