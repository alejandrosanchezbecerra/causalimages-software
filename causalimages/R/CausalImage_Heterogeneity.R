#' Decompose treatment effect heterogeneity by image or image sequence
#'
#' Implements the image heterogeneity decomposition analysis of Jerzak, Johansson, and Daoud (2023). Users
#' input in treatment and outcome data, along with a function specifying how to load in images
#' using keys referenced to each unit (since loading in all image data will usually not be possible due to memory limitations).
#' This function by default performs estimation, constructs salience maps, and can optionally perform
#' estimation for new areas outside the original study sites in a transportability analysis.
#'
#' @param obsW A numeric vector where `0`'s correspond to control units and `1`'s to treated units.
#' @param obsY A numeric vector containing observed outcomes.
#' @param kClust_est Integer specifying the number of clusters used in estimation. Default is `2L`.
#' @param file Path to a tfrecord file generated by `WriteTfRecord`.
#' @param transportabilityMat An optional matrix with a column named `key` specifying keys to be used for generating treatment effect predictions for out-of-sample points in earth observation data settings.
#' @param imageKeysOfUnits A vector of length `length(obsY)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into the package to call images into memory.
#' @param long,lat Optional vectors specifying longitude and latitude coordinates for units. Used only for describing highest and lowest probability neighorhood units if specified.
#' @param X Optimal numeric matrix containing tabular information used if `orthogonalize = T`.
#' @param conda_env A `conda` environment where computational environment lives, usually created via `causalimages::BuildBackend()`. Default = `"CausalImagesEnv"`.
#' @param conda_env_required A Boolean stating whether use of the specified conda environment is required.
#' @param orthogonalize A Boolean specifying whether to perform the image decomposition after orthogonalizing with respect to tabular covariates specified in `X`.
#' @param nMonte_variational An integer specifying how many Monte Carlo iterations to use in the
#' calculation of the expected likelihood in each training step.
#' @param optimizeImageRep Boolean specifying whether to optimize over the image model representation (or only over downstream parameters).
#' @param nMonte_predictive An integer specifying how many Monte Carlo iterations to use in the calculation
#' of posterior means (e.g., mean cluster probabilities).
#' @param nMonte_salience An integer specifying how many Monte Carlo iterations to use in the calculation
#' of the salience maps (e.g., image gradients of expected cluster probabilities).
#' @param figuresTag A string specifying an identifier that is appended to all figure names.
#' @param figuresPath A string specifying file path for saved figures made in the analysis.
#' @param nSGD Number of stochastic gradient descent (SGD) iterations.
#' @param batchSize Batch size used in SGD optimization.
#' @param strides Integer specifying the strides used in the convolutional layers.=
#' @param plotResults Should analysis results be plotted?
#' @param plotBands An integer or vector specifying which band position (from the acquired image representation) should be plotted in the visual results. If a vector, `plotBands` should have 3 (and only 3) dimensions (corresponding to the 3 dimensions to be used in RGB plotting).
#' @param dataType String specifying whether to assume `"image"` or `"video"` data types.
#' @param nWidth_ImageRep Integer specifying width of image model representation.
#' @param nDepth_ImageRep Integer specifying depth of image model representation.
#' @param nWidth_Dense Integer specifying width of image model representation.
#' @param nDepth_Dense Integer specifying depth of dense model representation.
#' @param kernelSize Dimensions used in spatial convolutions.
#' @param temporalKernelSize imensions used in temporal convolutions (if `dataType = "video"``)
#' @param TfRecords_BufferScaler The buffer size used in `tfrecords` mode is `batchSize*TfRecords_BufferScaler`. Lower `TfRecords_BufferScaler` towards 1 if out-of-memory problems.
#'
#' @return Returns a list consiting of \itemize{
#'   \item `clusterTaus_mean` default
#'   \item `clusterProbs_mean`. Estimated mean image effect cluster probabilities.
#'   \item `clusterTaus_sigma`. Estimated cluster standard deviations.
#'   \item `clusterProbs_lowerConf`. Estimated lower confidence for effect cluster probabilities.
#'   \item `impliedATE`. Implied ATE.
#'   \item `individualTau_est`. Estimated individual-level image-based treatment effects.
#'   \item `transportabilityMat`. Transportability matrix withestimated cluster information.
#'   \item `plottedCoordinates`. List containing coordinates plotted in salience maps.
#'   \item `whichNA_dropped`. A vector containing observations dropped due to missingness.
#' }
#'
#' @section References:
#' \itemize{
#' \item Connor T. Jerzak, Fredrik Johansson, Adel Daoud. Image-based Treatment Effect Heterogeneity. Forthcoming in \emph{Proceedings of the Second Conference on Causal Learning and Reasoning (CLeaR), Proceedings of Machine Learning Research (PMLR)}, 2023.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @import reticulate rrapply
#' @export
#' @md
AnalyzeImageHeterogeneity <- function(obsW,
                                      obsY,
                                      X = NULL,
                                      orthogonalize = F,
                                      imageKeysOfUnits = 1:length(obsY),
                                      kClust_est = 2,
                                      file = NULL,
                                      transportabilityMat = NULL ,
                                      lat = NULL,
                                      long = NULL,
                                      conda_env = "CausalImagesEnv",
                                      conda_env_required = T,

                                      figuresTag = "",
                                      figuresPath = "./",
                                      plotBands = 1L,
                                      heterogeneityModelType = "variational_minimal",
                                      plotResults = F,
                                      optimizeImageRep = T,
                                      nWidth_ImageRep = 64L, nDepth_ImageRep = 1L,
                                      nWidth_Dense = 64L, nDepth_Dense = 1L,
                                      nDepth_TemporalRep = 1L,
                                      useTrainingPertubations = T,
                                      strides = 2L,
                                      testFrac = 0.1,
                                      kernelSize = 5L,
                                      temporalKernelSize = 2L,
                                      learningRateMax = 0.005,
                                      patchEmbedDim = 16L,
                                      nSGD  = 500L,
                                      batchSize = 16L,
                                      seed = NULL,
                                      Sys.setenv_text = NULL,

                                      imageModelClass = "VisionTransformer",
                                      nMonte_predictive = 10L,
                                      nMonte_salience = 10L,
                                      nMonte_variational = 2L,
                                      TfRecords_BufferScaler = 4L,
                                      temperature = 1,
                                      inputAvePoolingSize  = 1L,
                                      dataType = "image"){
  # create directory if needed
  if( !dir.exists(figuresPath) ){ dir.create(figuresPath) }

  {
    print2("Establishing connection to computational environment (build via causalimages::BuildBackend())")
    library(tensorflow); if(!is.null(conda_env)){
      try(reticulate::use_condaenv(conda_env, required = conda_env_required),T)
    }
    if(!is.null(Sys.setenv_text)){ eval(parse(text = Sys.setenv_text)) }
    py_gc <- reticulate::import("gc")
    jax <<- reticulate::import("jax")
    np <<- reticulate::import("numpy")
    jnp <<- reticulate::import("jax.numpy")
    jmp <<- reticulate::import("jmp")
    optax <<- reticulate::import("optax")
    oryx <<- reticulate::import("tensorflow_probability.substrates.jax")
    eq <<- reticulate::import("equinox")
    jax$config$update("jax_enable_x64", FALSE);
    # tf$config$get_visible_devices(); tf$config$experimental$set_visible_devices(list(), "GPU")
    print2(sprintf("Default device: %s",jnp$array(0.)$devices()))
    gc(); py_gc$collect()

    # image dtype management
    c2f <- jmp$cast_to_full
    image_dtype <- jnp$float16
    image_dtype_tf <- tf$float16
    variable_dtype <- jnp$float32

    cnst <- function( ar ){ jnp$array(ar, jnp$float16) }
    rzip <- function( l1,l2 ){  fl<-list(); for(aia in 1:length(l1)){ fl[[aia]] <- list(l1[[aia]], l2[[aia]]) }; return( fl  ) }
    if(is.null(seed)){ seed <- ai(runif(1,1,100000)) } 
  }
  if(!optimizeImageRep & nDepth_ImageRep > 1){ stop("Stopping: When nDepth_ImageRep = T, nDepth_imageRep must be 1L") }
  
  print2("Setting up wd logic")
  orig_wd <- getwd()
  cond1 <- substr(figuresPath, start = 0, stop = 1) == "."
  if(cond1){ figuresPath <- gsub(figuresPath, pattern = '\\.', replacement = orig_wd) }
  if(!dir.exists(figuresPath)){ dir.create(figuresPath) }

  BN_EP <- (0.001); bn_momentum <- (.90)
  if(grepl(heterogeneityModelType,pattern = "variational")){ bn_momentum <- bn_momentum^1/nMonte_variational }

  # define base tf record + train/test fxns
  changed_wd <- F; {
    print2("Establishing connection with tfrecord")
    tf_record_name <- file
    if( !grepl(tf_record_name, pattern = "/") ){
      tf_record_name <- paste("./",tf_record_name, sep = "")
    }
    tf_record_name <- strsplit(tf_record_name,split="/")[[1]]
    new_wd <- paste(tf_record_name[-length(tf_record_name)], collapse = "/")
    print2( sprintf("Temporarily re-setting the wd to %s", new_wd ) )
    changed_wd <- T; setwd( new_wd )
    tf_dataset_master <- tf$data$TFRecordDataset(  tf_record_name[length(tf_record_name)] )

    # helper functions
    useVideoIndicator <- (dataType == "video")
    getParsed_tf_dataset_inference <- function(tf_dataset){
      dataset <- tf_dataset$map( function(x){ 
                              parse_tfr_element(element = x,
                                                readVideo = useVideoIndicator,
                                                image_dtype = image_dtype_tf) } ) 
      return( dataset <- dataset$batch( ai(max(2L,round(batchSize/2L)  ))) )
    }
    
    # setup k-fold cross-fitting
    {
      getParsed_tf_dataset_train_Select <- function( tf_dataset ){
        return( tf_dataset$map( function(x){ parse_tfr_element(x, readVideo = useVideoIndicator, image_dtype = image_dtype_tf)},
                                num_parallel_calls = tf$data$AUTOTUNE) ) 
      }
      getParsed_tf_dataset_train_Shuffle <- function( tf_dataset ){
        tf_dataset <- tf_dataset$shuffle(buffer_size = tf$constant(ai(TfRecords_BufferScaler*batchSize),dtype=tf$int64),
                                         reshuffle_each_iteration = F)
      }
      getParsed_tf_dataset_train_BatchAndShuffle <- function( tf_dataset ){
        tf_dataset <- tf_dataset$shuffle(buffer_size = tf$constant(ai(TfRecords_BufferScaler*batchSize),dtype=tf$int64),
                                         reshuffle_each_iteration = T) 
        tf_dataset <- tf_dataset$batch(  ai(batchSize)   )
        tf_dataset <- tf_dataset$prefetch( tf$data$AUTOTUNE ) 
        return( tf_dataset )
      }
    }

    # setup iterator
    tf_dataset_inference <- getParsed_tf_dataset_inference( tf_dataset_master )

    # reset iterator
    ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )

    # checks
    # ds_iterator_inference$output_shapes; ds_iterator_train$output_shapes
    # ds_next_inference <- reticulate::iter_next( ds_iterator_inference )
  }

  print2("Getting channel normalization parameters...")
  tmp <- replicate(30, {
        tmp <- jnp$array( ds_iterator_inference$`next`()[[1]]  )
        if(length(tmp$shape) == 4){
          l_ <- list("NORM_MEAN" = as.array(tf$cast(jnp$mean(tmp, 0L:2L),tf$float32)),
                     "NORM_SD" = as.array(tf$cast(jnp$mean(tmp, 0L:2L),tf$float32)  ))
        }
        if(length(tmp$shape) == 5){
          l_ <- list("NORM_MEAN" = as.array(tf$cast(jnp$mean(tmp, 0L:3L),tf$float32)),
                     "NORM_SD" = as.array(tf$cast(jnp$std(tmp, 0L:3L),tf$float32))  )
        }
        return( l_  )
    })
  NORM_MEAN <- jnp$expand_dims( jnp$expand_dims(jnp$expand_dims(jnp$array(tf$constant(colMeans(do.call(rbind,tmp["NORM_MEAN",])),
                                                                           dtype = image_dtype)),0L),0L), 0L)
  NORM_SD <- jnp$expand_dims( jnp$expand_dims(jnp$expand_dims(jnp$array(tf$constant(colMeans(do.call(rbind,tmp["NORM_SD",])),
                                                                         dtype = image_dtype)),0L),0L), 0L)
  if(length(dim(tmp)) == 5){
      NORM_MEAN <- jnp$expand_dims( NORM_MEAN, 0L)
      NORM_SD <- jnp$expand_dims( NORM_SD, 0L)
  }
  
  # reset inference iterator 
  ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )
  
  # clear memory 
  rm(  tmp  ) 

  if(useTrainingPertubations){
    trainingPertubations_OneObs <- function(im_, key){
        AB <- ifelse(dataType == "video", yes = 1L, no = 0L)
        which_path <- jnp$squeeze(jax$random$categorical(key = key, logits = jnp$array(t(rep(0, times = 4)))),0L)# generates random # from 0L to 3L
        im_ <- jax$lax$cond(jnp$equal(which_path,jnp$array(0L)), true_fun = function(){ jnp$flip(im_, AB+1L) } , false_fun = function(){im_})
        im_ <- jax$lax$cond(jnp$equal(which_path,jnp$array(2L)), true_fun = function(){ jnp$flip(im_, AB+2L) }, false_fun = function(){im_})
        im_ <- jax$lax$cond(jnp$equal(which_path,jnp$array(3L)), true_fun = function(){ jnp$flip(jnp$flip(im_, AB+1L),AB+2L) }, false_fun = function(){im_})
        return( im_ )
    } 
    trainingPertubations <- jax$vmap(function(im_, key){return( trainingPertubations_OneObs(im_,key) )  }, in_axes = list(0L,0L))
  }
  InitImageProcessFn <- jax$jit(function(im, key, inference){
      # expand dims if needed
      if(length(imageKeysOfUnits) == 1){ im <- jnp$expand_dims(im,0L) }

      # normalize
      im <- (im - NORM_MEAN) / NORM_SD

      # training pertubations
      if(useTrainingPertubations){
        im <- jax$lax$cond(inference, 
                           true_fun = function(){ im }, 
                           false_fun = function(){ trainingPertubations(im, 
                                                        jax$random$split(key,im$shape[[1]])) } )
      }

      # downshift resolution if desired
      if(inputAvePoolingSize > 1){ im <- jax$vmap(function(im){ AvePoolingDownshift(im)}, 0L) }

      # return normalized (& pertubed if inference = F) image/image seq
      return( im  )
  })

  # set up placeholders + start loop 
  kFolds <- 3L
  cf_iters <- 2L
  Loss_out_baseline_vec <- Loss_out_vec <- Y0_est_mat <- Y1_est_mat <- tau_est_mat <- c()  
  TestIndices_list <- TrainIndices_list <- replicate(list(), n = cf_iters*kFolds)
  
  # set up loop values 
  nUniqueKeys <- length( unique( imageKeysOfUnits ) )
  cf_keys_split <- 1*as.numeric(cut(1:nUniqueKeys,kFolds))
  cf_keys_split <- sapply( 1:kFolds, function(l_){ list(which(cf_keys_split==l_))})
  cf_keys_toSkip_bounds <- lapply(cf_keys_split,function(l_){c(min(l_), max(l_))})
  
  # start outer CF iteration 
  trainCounter <- 0; for(cf_iter in 1:cf_iters){ 
    
    # shuffle for outer CF iteration 
    tf_dataset_master_ <- getParsed_tf_dataset_train_Shuffle( tf_dataset_master )
    
    # start inner CF iteration 
    for(kf_ in 1:kFolds){
      trainCounter <- trainCounter + 1
      print2(sprintf("Starting training at k = %s of %s (%s of %s)...",kf_, kFolds, cf_iter, cf_iters))
      # setup iterator 
      {
        # select a tf record indexed to (1:kFolds([!1:kFolds %in% kf_] (skip indices bounded by cf_keys_toSkip_bounds)
        # 1 2 3 4 * 5 6 7 8 * 9 10 11 12, K = 3 
        if(kf_ == 1){ 
          tf_dataset_train <- getParsed_tf_dataset_train_Select(
            tf_dataset_master_$skip( ai(cf_keys_toSkip_bounds[[kf_]][2]) ) )$`repeat`(-1L) 
            # skip 1:4
        }
        if(kf_ == kFolds){ 
          tf_dataset_train <- getParsed_tf_dataset_train_Select(
            tf_dataset_master_$take( ai(cf_keys_toSkip_bounds[[kf_]][1]-1L) ) )$`repeat`(-1L) 
          # take 1 2 3 4 * 5 6 7 8
        }
        if(kf_ > 1 & kf_ < kFolds){ 
           tf_dataset_train <- getParsed_tf_dataset_train_Select(
             tf_dataset_master_$take( ai(cf_keys_toSkip_bounds[[kf_]][1]-1L) )$concatenate(
               tf_dataset_master_$skip( ai(cf_keys_toSkip_bounds[[kf_]][2]) ) ))$`repeat`(-1L) # repeat to avoid out of sequence errors 
           # 1:4, skip 1:8
        }
        tf_dataset_train <- getParsed_tf_dataset_train_BatchAndShuffle( tf_dataset_train )
        ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
      }
      
      # obtain image representation function
      {
        print2("Initializing image representation functions...")
        SharedImageRepresentation <- T;
        setwd(orig_wd); ImageRepresentations <- GetImageRepresentations(
            file = file, conda_env = conda_env,
            dataType = dataType,
            nWidth_ImageRep = nWidth_ImageRep,
            nDepth_ImageRep = nDepth_ImageRep,
            strides = strides,
            nDepth_TemporalRep = nDepth_TemporalRep,
            patchEmbedDim = patchEmbedDim,
            batchSize = batchSize,
            imageModelClass = imageModelClass,
            temporalKernelSize = temporalKernelSize,
            kernelSize = kernelSize,
            TfRecords_BufferScaler = 3L,
            imageKeysOfUnits = (UsedKeys <- sample(unique(imageKeysOfUnits),min(c(length(unique(imageKeysOfUnits)),2*batchSize)))), getRepresentations = T,
            returnContents = T,
            bn_momentum = bn_momentum,
            InitImageProcess = InitImageProcessFn,
            seed = seed + ai(trainCounter) # seed 
            ); setwd(new_wd)
            ImageModel_And_State_And_MPPolicy_List <- ImageRepresentations[["ImageModel_And_State_And_MPPolicy_List"]]
            ImageRepArm_OneObs <- ImageRepresentations[["ImageRepArm_OneObs"]]
            ImageRepArm_batch_R <- ImageRepresentations[["ImageRepArm_batch_R"]]
    
            print2("Done initializing image representation function...")
            rm(ImageRepresentations); gc(); py_gc$collect()
      }
    
      # set environment of image sampling functions
      figuresPath <- paste(strsplit(figuresPath,split="/")[[1]],collapse = "/")
      windowCounter <- 0
    
      # orthogonalize if specified
      whichNA_dropped <- c()
      if(orthogonalize){
        print2("Orthogonalizing Potential Outcomes...")
        if(is.null(X)){stop("orthogonalize set to TRUE, but no X specified to perform orthogonalization!")}
    
        # drop observations with NAs in their orthogonalized outcomes
        whichNA_dropped <- which( is.na(  rowSums( X ) ) )
        if(length(whichNA_dropped) > 0){
          # note: transportabilityMat doesn't need to drop dropNAs
          obsW <- obsW[-whichNA_dropped]
          obsY <- obsY[-whichNA_dropped]
          X <- X[-whichNA_dropped,]
          imageKeysOfUnits <- imageKeysOfUnits[-whichNA_dropped]
          lat <- lat[ -whichNA_dropped ]
          long <- long[ -whichNA_dropped ]
        }
        Yobs_ortho <- resid(temp_lm <- lm(obsY ~ X))
        if(length(Yobs_ortho) != length(obsY)){
          stop("length(Yobs_ortho) != length(obsY)")
        }
        plot(obsY,Yobs_ortho)
        obsY <- Yobs_ortho
      }
    
      # set up holders 
      Tau_sd_vec <- plotting_coordinates_list <- Tau_mean_sd_vec <- loss_vec <- NULL
      Tau_mean_return_vec <- Tau_mean_return_sd_vec <- jnp$array(1.)
    
      # specify some training parameters + helper functions
      batchFracOut <- max(1/3*batchSize,3) / batchSize
      nMonte_variational <- ai( nMonte_variational  )
      widthFreq <- 20
      WhenPool <- c(1,2)
      #as the temperature goes to 0 the RelaxedOneHotCategorical becomes discrete with a distribution described by the logits or probs parameters
      #plot(as.matrix2(do.call(rbind,replicate(100,tfd$RelaxedOneHotCategorical(temperature = temperature, probs = c(0.1,0.9))$sample(1L))))[,2],ylim = c(0,1));abline(h=0.5)
    
      # set up some placeholders
      y0_true <- r2_y1_out <- r2_y0_out <- ClusterProbs_est <- NULL
      tau_est <- negELL <- y1_est <- y0_est <- y1_true <- y0_true <- NULL
      if(!"ClusterProbs" %in% ls() &
         !"ClusterProbs" %in% ls(envir=globalenv())){ClusterProbs<-NULL}
    
      # normalize outcomes for stability (estimates are re-normalized after training)
      obsY_orig <- obsY
      Y_mean <- mean(obsY); Y_sd <- sd(obsY)
      obsY <- (obsY - Y_mean)  /  Y_sd
      Rescale <- function(x,doMean = F){ return( x*Y_sd + ifelse(doMean, yes = Y_mean, no = 0) ) }
      Tau_mean_init_prior <- Tau_mean_init <- mean(obsY[obsW==1]) - mean(obsY[obsW==0])
      Tau_sd_init <- sqrt( var(obsY[obsW==1]) + var( obsY[obsW==0]) )
      Y0_sd_vec <- na.omit(replicate(10000,{ top_ <- sample(1:length(obsY),batchSize); return(sd(obsY[top_][obsW[top_]==0])) }))
      Y1_sd_vec <- na.omit(replicate(10000,{ top_ <- sample(1:length(obsY),batchSize); sd(obsY[top_][obsW[top_]==1]) }))
      tau_vec <- na.omit(replicate(10000,{ top_ <- sample(1:length(obsY),batchSize); mean(obsY[top_][obsW[top_]==1]) - mean(obsY[top_][obsW[top_]==0]) }))
      Y0_mean_init_prior <- Y0_mean_init <- mean(obsY[obsW==0]); Y0_sd_raw <- max(0.01, mean(Y0_sd_vec,na.rm=T))
      Y1_mean_init_prior <- Y1_mean_init <- mean(obsY[obsW==1]); Y1_sd_raw <- max(0.01, mean(Y1_sd_vec,na.rm=T))
      softplus_inverse <- function(x){ jnp$log(jnp$exp(x) - cnst(1.)) }
      softplus_inverse2 <- function(x){ jnp$log(jnp$exp(x) - jnp$array(1)) }
      Y0_sd_priorMean <- softplus_inverse(cnst(Y0_sd_raw))
      Y1_sd_priorMean <-softplus_inverse(cnst(Y1_sd_raw))
      Y0_sd_initMean <-softplus_inverse(cnst((SD_scaling <- 1)*Y0_sd_raw))#<-SD_scaling*sd(obsY[obsW==0])))
      Y1_sd_initMean <-softplus_inverse(cnst(SD_scaling*Y1_sd_raw))#<-SD_scaling*sd(obsY[obsW==1])))
    
      #for(BAYES_STEP in c(1,2)){if(BAYES_STEP == 1){ print2(ifelse(BAYES_STEP==1,yes="Empirical Bayes Calibration Step (see  Krishnan et al. (2020))...", no="Empirical Bayes Estimation Step...")) }
      for(BAYES_STEP in c(1)){
        if(BAYES_STEP == 1){
          nSGD_ORIG <- nSGD
          L2_grad_scale <- cnst( 0.5 )
          SD_PRIOR_MODEL <- cnst(.0001); KL_wt <- cnst(0)
          SD_INIT_MODEL <- cnst(.000001)
          PRIOR_MODEL_FXN <- function(name_){
            eval(parse(text = 'function(dtype, shape, name, trainable, add_variable_fn){
          d_prior <- oryx$distributions$Normal(loc = jnp$zeros(shape), scale = cnst(SD_PRIOR_MODEL))
          tfd$Independent(d_prior, reinterpreted_batch_ndims = tf$size(d_prior$batch_shape_tensor())) }'))
          }
          PRIOR_MODEL_FXN("hap")
          PosteriorInitializer <- function(){
            eval(parse(text = 'tfp$layers$default_mean_field_normal_fn(
            is_singular = F,
            loc_initializer = tf$keras$initializers$GlorotUniform(seed = ai(runif(1,1,100000))),
            untransformed_scale_initializer = tf$keras$initializers$random_normal(mean = -9.0, stddev = 0.001, seed = ai(runif(1,1,100000))),
            loc_regularizer = NULL,
            untransformed_scale_regularizer = NULL,
            loc_constraint = NULL,
            untransformed_scale_constraint = NULL )')) }
        }
        if(BAYES_STEP == 2){
          nSGD <- nSGD_ORIG
          L2_grad_scale <- cnst( 0.5 )
          KL_wt <- cnst( batchSize / length(obsY) )
          PRIOR_MODEL_FXN <- function(name_){
            prior_loc_name <- sprintf("%s_PRIOR_MEAN_HASH818",name_ )
            prior_SD_name <- sprintf("%s_PRIOR_SD_HASH818",name_ )
            ZERO_LEN_IN <- length( eval(parse(text = sprintf("%s$variables",name_)))) == 0
            if( ZERO_LEN_IN){
              prior_loc_name <- "jnp$zeros(shape)"; prior_SD_name <- "1"
            }
            if( !ZERO_LEN_IN){
              z_name_ref <- eval(parse(text = sprintf("%s$variables[[1]]$name",name_)))
              z_name_ref <- gsub(z_name_ref, pattern = ":",replacement = "XCOLX")
              z_name_ref <- gsub(z_name_ref, pattern = "/",replacement = "XDASHX")
    
              # set mean
              eval.parent(parse(text = sprintf("%s <- jnp$array(%s$variables[[1]],variable_dtype)",prior_loc_name,name_)))
    
              # set sd
              eval.parent(parse(text = sprintf("%s <- tf$maximum(jnp$array(0.001,dtype = variable_dtype),
                                                            jnp$array(0.1*tf$sqrt(tf$math$reduce_variance(%s$variables[[1]])),variable_dtype))",prior_SD_name,name_)))
            }
            eval(parse(text = sprintf('function(dtype, shape, name, trainable, add_variable_fn){
                  d_prior <- oryx$distributions$Normal(loc = (%s),
                                      scale = (%s))
                  tfd$Independent(d_prior, reinterpreted_batch_ndims = tf$size(d_prior$batch_shape_tensor())) }',
                                      prior_loc_name, prior_SD_name)))
          }
    
          # set other priors
          Tau_mean_init_prior <- as.vector(MeanDist_tau[k_,"Mean"][[1]])
          Y0_sd_priorMean <- as.vector(SDDist_Y0[k_,"Mean"][[1]])
          Y1_sd_priorMean <- as.vector(SDDist_Y1[k_,"Mean"][[1]])
        }
    
        print2("Building clustering model...")
        {
          batch_axis_name <- "batch"; bn_momentum <- 0.9; ep_BN <- 0.001
          DenseList_Prior <- DenseStateList <- DenseList <- list()
          InvSoftPlus <- function(x){ jnp$log(jnp$exp(x) - 1) }
          for(arm_ in c("Tau","EY0")){
            for(dense_ in 1L:nDepth_Dense){
                  # arm_ <- "Tau"; dense_ <- 1L
                  InputDim <- ai( ifelse(dense_==1, yes = nWidth_ImageRep, no = nWidth_Dense) )
                  HiddenDim <- ai( ifelse(dense_ < nDepth_Dense, yes = ai( nWidth_Dense*(HiddenWidthDense <- 3.) ), no = nWidth_Dense) )
                  OutputDim <- ai( ifelse(dense_==nDepth_Dense,
                                             yes = ifelse(arm_ == "Tau", yes = kClust_est-1L, no = 1L),
                                             no = nWidth_Dense))
                  BiasInit <- ifelse(dense_ == nDepth_Dense & arm_ == "EY0", 
                                     yes = mean(obsY[obsW == 0]), 
                                     no = ifelse(dense_ == nDepth_Dense & arm_ == "Tau" & heterogeneityModelType == "tarnet", 
                                                 yes = mean(obsY[obsW == 1]) - mean(obsY[obsW == 0]), 
                                                 no = 0.))
                  NonLinearScalerDense <- 3^2
                  eval(parse(text = sprintf("DenseList$%s_d%s <- list(
                  
                                          list(eq$nn$BatchNorm( input_size = InputDim, axis_name = batch_axis_name, momentum = bn_momentum, eps = ep_BN, channelwise_affine = F),
                                               jnp$array(rep(1., times = InputDim))), 
                                          list(jnp$array(matrix(rnorm(InputDim*HiddenDim)*sqrt(2/InputDim), nrow = InputDim)), # swiglu proj wts (1) 
                                               jnp$array(matrix(rnorm(InputDim*HiddenDim,sd=0.0001, mean = -8), nrow = InputDim))), 
                                               
                                          list(jnp$array(matrix(rnorm(InputDim*HiddenDim)*sqrt(2/InputDim), nrow = InputDim)), # swiglu proj wts (2) 
                                               jnp$array(matrix(rnorm(InputDim*HiddenDim,sd=0.0001, mean = -8), nrow = InputDim))),
                                          list(jnp$array(matrix(rnorm(HiddenDim*OutputDim)*sqrt(2/InputDim), nrow = HiddenDim)), # output proj wts
                                               jnp$array(matrix(rnorm(HiddenDim*OutputDim,sd=0.0001, mean = -8), nrow = HiddenDim))),
                                               
                                          jnp$array(rep(BiasInit, times = OutputDim)), # bias 
                                          list(jnp$array(matrix(rnorm(InputDim*OutputDim)*sqrt(2/InputDim), nrow = InputDim)), # resid proj wts
                                               jnp$array(matrix(rnorm(InputDim*OutputDim,sd=0.0001, mean = -8), nrow = InputDim))),
                                               
                                          InvSoftPlus(jnp$array(1./sqrt( NonLinearScalerDense * nDepth_Dense ))) # residual wt 
                                          )", arm_, dense_ )))
                  eval(parse(text = sprintf("DenseList_Prior$%s_d%s <- list(
                                          jnp$array(matrix(rnorm(OutputDim*InputDim)*sqrt(2/InputDim), nrow = InputDim)),
                                          jnp$array(matrix(rnorm(OutputDim*InputDim,sd=0.0001, mean = -8), nrow = InputDim)))", arm_, dense_ )))
                  eval(parse(text = sprintf("DenseStateList$BNState_%s_d%s <- eq$nn$State(  DenseList$%s_d%s[[1]][[1]]  )", arm_, dense_, arm_, dense_ )))
            }; rm( dense_ )
          }
          print2("Initializing cluster projection...")
          Tau_mean_init <- mean(obsY[obsW==1]) - mean(obsY[obsW==0])
          Tau_means_init <- Tau_mean_init + .01*seq(-1,1,length.out=kClust_est)*max(0.01,abs(Tau_mean_init))
    
          print2("Initializing cluster centers...")
          base_mat <- as.data.frame( matrix(list(),nrow=kClust_est,ncol=3L) ); colnames( base_mat ) <- c("Mean","SD","Prior")
          SDDist_Y1 <- SDDist_Y0 <- MeanDist_tau <- base_mat
          as.numeric2 <- function(x){ as.numeric(tf$cast(x,tf$float32)) }
          as.matrix2 <- function(x){ as.matrix(tf$cast(x,tf$float32)) }
          for(k_ in 1:kClust_est){
            sd_init_trainableParams <- as.numeric2(softplus_inverse2(0.00001)) # set this to a small number so network starts off as nearly deterministic
    
            # tau's
            MeanDist_tau[k_,"Mean"][[1]] <- list( jnp$array(cnst(Tau_means_init[k_]) ) )
            MeanDist_tau[k_,"SD"][[1]] <- list( jnp$array(cnst(sd_init_trainableParams) ) )
            MeanDist_tau[k_,"Prior"][[1]] <- list( oryx$distributions$Normal(cnst(Tau_mean_init_prior), cnst(2*sd(tau_vec) )))
    
            # Y0
            SDDist_Y0[k_,"Mean"][[1]] <- list( jnp$array(cnst(as.numeric2(Y0_sd_initMean)) ) )
            SDDist_Y0[k_,"SD"][[1]] <- list( jnp$array(cnst(sd_init_trainableParams)) )
            SDDist_Y0[k_,"Prior"][[1]] <- list( oryx$distributions$Normal(cnst(as.numeric2(Y0_sd_priorMean)), cnst(2*sd(Y0_sd_vec))))
    
            # Y0
            SDDist_Y1[k_,"Mean"][[1]] <- list( jnp$array(cnst(Y1_sd_initMean) ) )
            SDDist_Y1[k_,"SD"][[1]] <- list( jnp$array(cnst(sd_init_trainableParams)) )
            SDDist_Y1[k_,"Prior"][[1]] <- list( oryx$distributions$Normal(cnst(Y1_sd_priorMean),cnst(2*sd(Y1_sd_vec))))
          }
        }
        MeanDist_tau <- unlist(  MeanDist_tau  )
        SDDist_Y1 <- unlist(  SDDist_Y1  )  ;  SDDist_Y0 <- unlist(  SDDist_Y0  )
        names(MeanDist_tau) <- paste("Tau_", names(MeanDist_tau), sep = "")
        names(SDDist_Y0) <- paste("Y0_", names(SDDist_Y0),sep = "")
        names(SDDist_Y1) <- paste("Y1_", names(SDDist_Y1),sep = "")
        CausalList <- c(MeanDist_tau, SDDist_Y0, SDDist_Y1)
        PriorCausalList <- CausalList[grepl(names(CausalList),pattern = "Prior")]
        CausalList <- CausalList[!grepl(names(CausalList),pattern = "Prior")]
    
        GetDenseNet <- function(ModelList, ModelList_fixed, m,
                                vseed, StateList, seed, MPList, inference, type){
            m <- jnp$expand_dims(m, 0L)
            for(d__ in 1L:nDepth_Dense){
              print(sprintf("Depth %s of %s (type = %s)", d__, nDepth_Dense, type))
              mtm1 <- m; 
              if(d__ < nDepth_Dense){
                if(T == F){ # if using bnorm
                  m <- LE(ModelList, sprintf("%s_d%s",type, d__))[[1]][[1]](m, state = StateList[[2]][[d__]], inference = inference)
                  StateIndex <- LE_index(StateList, sprintf("%s_d%s",type, d__))
                  StateIndex <- paste(sapply(StateIndex, function(zer){ paste("[[", zer, "]]") }), collapse = "")
                  eval(parse(text = sprintf("StateList%s <- m[[2]]", StateIndex))); m <- m[[1]]
                }
              
                if(grepl(heterogeneityModelType,pattern="variational")){  
                  Wts <- MPList[[1]]$cast_to_compute( oryx$distributions$Normal( 
                                            c2f( LE(ModelList, sprintf("%s_d%s",type, d__))[[1]]),
                           jax$nn$softplus( c2f( LE(ModelList, sprintf("%s_d%s",type, d__))[[2]]) ) )$sample(seed = jnp$add(40L, seed) ) )
                }
                if(grepl(heterogeneityModelType,pattern="tarnet")){  # just take mean 
                  m <- jax$nn$swish( jnp$matmul(m, LE(ModelList, sprintf("%s_d%s",type, d__))[[2]] [[1]] ) ) * 
                              jnp$matmul( m, LE(ModelList, sprintf("%s_d%s",type, d__))[[3]][[1]] )
                }
              }
              
              # main projection 
              m <- jnp$matmul( m, LE(ModelList, sprintf("%s_d%s",type, d__))[[4]] [[1]] ) +  LE(ModelList, sprintf("%s_d%s",type, d__))[[5]]
              
              # resid path 
              if(d__ < nDepth_Dense){
                m <- jnp$matmul( mtm1, LE(ModelList, sprintf("%s_d%s",type, d__))[[6]] [[1]] ) +
                        m*jax$nn$softplus( LE(ModelList, sprintf("%s_d%s",type, d__))[[7]]$astype(jnp$float32) )$astype(mtm1$dtype)
              }
          }
    
          if( type == "Tau" & grepl(heterogeneityModelType,pattern="variational") ){  
            m <- jnp$concatenate(list( MPList[[1]]$cast_to_compute( jnp$zeros(list(1L,1L)) ), m), 1L)
          }
          return( list(jnp$squeeze( m, 0L), StateList)  )
        }
        for(type_ in c("EY0","Tau")){
          eval(parse(text =  sprintf("Get%s_batch <- jax$vmap( Get%s <- function(
            ModelList, ModelList_fixed,
            m, vseed,
            StateList, seed, MPList, inference){
            GetDenseNet(ModelList, ModelList_fixed, m, vseed, StateList, seed, MPList, inference, type = '%s')
          }, in_axes = list(NULL, NULL,
                            0L, 0L,
                            NULL, NULL, NULL, NULL),
             axis_name = batch_axis_name,
             out_axes = list(0L, NULL))", type_, type_ , type_)))
        }
        if(grepl(heterogeneityModelType, pattern = "variational")){
          getClusterSamp_logitInput <- function(logits_, seed_){
            return( oryx$distributions$RelaxedOneHotCategorical(temperature = c2f(temperature),
                      logits =  logits_ )$sample(seed = jnp$add(452L, seed_)) )
          }
          if(grepl(heterogeneityModelType, pattern = "variational_minimal")){
            GetEY1_batch <-  function(ModelList, ModelList_fixed, m, vseed, StateList, seed, MPList, inference){
              EY0 <- GetEY0_batch(ModelList, ModelList_fixed,
                                  m, vseed, StateList, seed, MPList, inference)[[1]]
              Clust_logits <- GetTau_batch(ModelList,ModelList_fixed,
                                           m, vseed, StateList, seed, MPList, inference)[[1]]
              clustT <- getClusterSamp_logitInput(Clust_logits, seed)
              ETau_draw <-  oryx$distributions$Normal(
                              c2f(getTau_means(ModelList)),
                        jax$nn$softplus(c2f(getTau_sds(ModelList))))$sample(seed = jnp$add(10L,seed))
              Etau_ <- jnp$sum(jnp$multiply(ETau_draw, clustT), axis = 1L, keepdims=T)
              return( EY0 + Etau_ )
            }
          }
          getTau_means <- function(ModelList){ return(  jnp$stack(list(LE(ModelList, "Tau_Mean1"), LE(ModelList, "Tau_Mean2") )) )  }
          getTau_sds <- function(ModelList){ return(  jnp$stack(list(LE(ModelList, "Tau_SD1"), LE(ModelList, "Tau_SD2") )) )  }
        }
        getSDY_params <- function(ModelList, qname, pname){ return(
          jnp$stack(list(LE(ModelList, sprintf("%s_%s1", qname, pname)),
                         LE(ModelList, sprintf("%s_%s2", qname, pname)) )) ) }
        if(grepl(heterogeneityModelType, pattern = "tarnet")){
          GetEY1_batch <-  function(ModelList, ModelList_fixed, m, vseed, StateList, seed, MPList, inference){
            EY0 <- GetEY0_batch(ModelList, ModelList_fixed,
                                m, vseed, StateList, seed, MPList, inference)[[1]]
            Etau_ <- GetTau_batch(ModelList,ModelList_fixed,
                                   m, vseed, StateList, seed, MPList, inference)[[1]]
            return( EY0 + Etau_ )
          }
        }
    
        GetLikelihoodDraw_batch <- function(
                                            ModelList, ModelList_fixed,
                                            m, treat, y, vseed,
                                            StateList, seed, MPList, inference){
          # image representation model
          if(SharedImageRepresentation){
            print2("Getting image representation...")
            m <- ImageRepArm_batch_R(ifelse(optimizeImageRep, yes = list(ModelList), no = list(ModelList_fixed))[[1]],
                                    m, StateList, seed, MPList, inference)
            StateList <- m[[2]]; m <- m[[1]]
          }
          
          print2("Getting baseline outcome...")
          EY0_i <- GetEY0_batch(ModelList, ModelList_fixed, m, vseed, StateList, seed, MPList, inference)
          StateList <- EY0_i[[2]]; EY0_i <- EY0_i[[1]]
          
          print2("Getting cluster logits...")
          clustT <- GetTau_batch(ModelList, ModelList_fixed, m, vseed, StateList, seed, MPList, inference)
          StateList <- clustT[[2]]; clustT <- clustT[[1]]
          
          if(grepl(heterogeneityModelType,pattern="tarnet")){ 
            Etau_ <- clustT
            EY0Uncert_draw <- jnp$take(jax$nn$softplus( c2f(getSDY_params(ModelList, "Y0", "Mean"))),0L)
            EY1Uncert_draw <- jnp$take(jax$nn$softplus( c2f(getSDY_params(ModelList, "Y1", "Mean"))),0L)
          }
          if(grepl(heterogeneityModelType,pattern="variational")){ 
            clustT <- MPList[[1]]$cast_to_compute( getClusterSamp_logitInput(c2f(clustT), seed) )
            
            # note: use vseed if vmapping and seed if pre-batched
            ETau_draw <- oryx$distributions$Normal(
              c2f(getTau_means( ModelList )),
              jax$nn$softplus( c2f(getTau_sds( ModelList ) )))$sample( seed = jnp$add(seed,111L) )
            ETau_draw <- MPList[[1]]$cast_to_compute( ETau_draw )
      
            # get SD draws
            EY0Uncert_draw <- oryx$distributions$Normal(
                            c2f( getSDY_params(ModelList,"Y0", "Mean") ),
                           jax$nn$softplus( c2f(getSDY_params(ModelList, "Y0","SD") ) ))$sample(  seed = jnp$add(seed,324L) )
            EY0Uncert_draw <- MPList[[1]]$cast_to_compute( jax$nn$softplus( EY0Uncert_draw ) )
      
            EY1Uncert_draw <- oryx$distributions$Normal(
                            c2f( getSDY_params(ModelList, "Y1", "Mean")) ,
              jax$nn$softplus( c2f(getSDY_params(ModelList, "Y1", "SD"))) )$sample(seed = jnp$add(seed,3234L))
            EY1Uncert_draw <- MPList[[1]]$cast_to_compute( jax$nn$softplus( EY1Uncert_draw ) )
      
            # setup likelihood
            #Etau_ <- jnp$sum( jnp$multiply(ETau_draw, clustT), 1L , keepdims=F)
            #Sigma2_Y0_i <- jnp$sum(jnp$multiply( EY0SD_draw^cnst(2), clustT),1L,keepdims=F)
            #Sigma2_Y1_i <- jnp$sum(jnp$multiply( EY1SD_draw^cnst(2), clustT),1L,keepdims=F)
            Etau_ <- jnp$sum( jnp$multiply(ETau_draw, clustT), keepdims=F)
            EY0Uncert_draw <- jnp$sum(jnp$multiply( EY0Uncert_draw, clustT),keepdims=F)
            EY1Uncert_draw <- jnp$sum(jnp$multiply( EY1Uncert_draw, clustT),keepdims=F)
          } 
          
          Y_Sigma <- ( cnst(1) - treat) * EY0Uncert_draw  + treat * EY1Uncert_draw 
          Y_Mean <- ( cnst(1) - treat) * EY0_i  + treat * (EY0_i + Etau_)
    
          # some commented analyses to triple-check code correctness re: initialization
          # lik_dist_draw <- jnp$mean( oryx$distributions$Normal(loc = c2f(Y_Mean), scale = c2f(Y_Sigma) )$log_prob(c2f(y)) )
    
          # simplify by uncommenting
          lik_dist_draw <- jnp$negative(  jnp$mean( (Y_Mean - y)^2) ) # later we - to min the sum of squares
    
          # return
          return(list(lik_dist_draw, StateList));
        }
    
        # get KL terms
        getKL <- (function(ModelList){
          # specify some distributions
          if(grepl(heterogeneityModelType, pattern = "variational_minimal")){
            Tau_mean_vec <- getTau_means(ModelList)
            MeanDist_Tau_post = oryx$distributions$Normal(Tau_mean_vec, jax$nn$softplus(c2f(jnp$stack(MeanDist_tau[,"SD"]))))
          }
          SDDist_Y1_post = oryx$distributions$Normal(jnp$stack(SDDist_Y1[,"Mean"]), jax$nn$softplus(jnp$stack(SDDist_Y1[,"SD"])))
          SDDist_Y0_post = oryx$distributions$Normal(jnp$stack(SDDist_Y0[,"Mean"]), jax$nn$softplus(jnp$stack(SDDist_Y0[,"SD"])))
    
          # generate KL components
          KLterm <- jnp$zeros(list(), dtype = ComputeDtype)
          if(! heterogeneityModelType  %in% c("variational_minimal_visualizer")){
            if(nDepth_Dense > 0){ for(dense_ in 1:nDepth_Dense){
              KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(DenseProj_Y0_%s_a$kernel_posterior,DenseProj_Y0_%s_a$kernel_prior)",nDepth_Dense, nDepth_Dense)))
              KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(DenseProj_Clust_%s_a$kernel_posterior,DenseProj_Clust_%s_a$kernel_prior)",nDepth_Dense, nDepth_Dense)))
    
              # comment if not using _b
              KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(DenseProj_Y0_%s_b$kernel_posterior,DenseProj_Y0_%s_b$kernel_prior)",nDepth_Dense, nDepth_Dense)))
              KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(DenseProj_Clust_%s_b$kernel_posterior,DenseProj_Clust_%s_b$kernel_prior)",nDepth_Dense, nDepth_Dense)))
            }}
          }
          if(heterogeneityModelType == "variational_minimal"){
            KLterm <- KLterm + jnp$sum(tfd$kl_divergence(MeanDist_Tau_post, (MeanDist_tau[,"Prior"][[1]])))
          }
          KLterm <- KLterm + jnp$sum(tfd$kl_divergence(SDDist_Y0_post, (SDDist_Y0[,"Prior"][[1]])))
          KLterm <- KLterm + jnp$sum(tfd$kl_divergence(SDDist_Y1_post, (SDDist_Y1[,"Prior"][[1]])))
          return( KLterm  )
        })
    
        GetExpectedLikelihood <-  function(ModelList, ModelList_fixed,
                                            m, treat, y, vseed,
                                            StateList, seed, MPList, inference){
          Elik <- jnp$zeros(list(), dtype = jnp$float16)
          for(mi_ in 1L:nMonte_variational){
            LikContrib <-  GetLikelihoodDraw_batch(ModelList, ModelList_fixed,
                                                    m, treat, y, jnp$add(vseed, mi_),
                                                    StateList, jnp$add(seed, mi_), MPList, inference)
            StateList <- LikContrib[[2]]; LikContrib <- LikContrib[[1]]
            Elik <- Elik + LikContrib / jnp$array(f2n(nMonte_variational), jnp$float16)
          }
          return( list(Elik, StateList) )
        }
    
        GetLoss <- function(ModelList, ModelList_fixed,
                            m, treat, y, vseed,
                            StateList, seed, MPList){
            ModelList <- MPList[[1]]$cast_to_compute( ModelList )
            ModelList_fixed <- MPList[[1]]$cast_to_compute( ModelList_fixed )
            StateList <- MPList[[1]]$cast_to_compute( StateList )
    
            # likelihood and state updates
            Elik <- GetExpectedLikelihood(ModelList, ModelList_fixed,
                                          m, treat, y, vseed,
                                          StateList, seed, MPList, F) # note: inference = F
            StateList <- Elik[[2]]; Elik <- Elik[[1]]
    
            # minimize negative log likelihood and positive KL term
            if(BAYES_STEP == 1){ minThis <- jnp$negative( Elik ) }
            if(BAYES_STEP == 2){ minThis <- CombineLikelihoodWithKL( Elik, klContrib ) }
    
            print2("Returning loss + state...")
            minThis <- MPList[[1]]$cast_to_output( minThis ) # compute to output dtype
            minThis <- MPList[[2]]$scale( minThis ) # scale loss
    
            StateList <- MPList[[1]]$cast_to_param( StateList ) # cast to param dtype
            return( list(minThis, StateList)  )
        }
    
        CombineLikelihoodWithKL <- ( function(lik_, kl_){
          minThis <- tf$negative(jnp$sum( lik_ ))
          if(BAYES_STEP == 2){
            minThis <- minThis + KL_wt * kl_ # combine likelihood with prior
          }
          minThis <- minThis / cnst(as.numeric(batchSize)) # normalize
        })
    
        # set state and model lists
        gc(); py_gc$collect()
        GradAndLossAndAux <-  eq$filter_jit( eq$filter_value_and_grad( GetLoss, has_aux = T) )
        if(!optimizeImageRep){
          ModelList <- list(DenseList, CausalList)
          ModelList_fixed <- ImageModel_And_State_And_MPPolicy_List[[1]]
        }
        if(optimizeImageRep){
          ModelList <- list(ImageModel_And_State_And_MPPolicy_List[[1]],
                            DenseList, CausalList)
          ModelList_fixed <- jnp$array(0.)
        }
        StateList <- list(ImageModel_And_State_And_MPPolicy_List[[2]], DenseStateList)
        MPList <- list(jmp$Policy(compute_dtype = (ComputeDtype <- jnp$float16), 
                                  param_dtype = jnp$float32,
                                  output_dtype= (OutputDtype <- jnp$float16)),
                       jmp$DynamicLossScale(loss_scale = jnp$array(2^12,dtype = OutputDtype),
                                            min_loss_scale = jnp$array(1.,dtype = OutputDtype),
                                            period = 20L))
        ModelList <- MPList[[1]]$cast_to_param( ModelList )
        ModelList_fixed <- MPList[[1]]$cast_to_param( ModelList_fixed )
        rm( ImageModel_And_State_And_MPPolicy_List, DenseStateList, DenseList, CausalList )
    
        # define optimizer and training step
        {
          LR_schedule <- optax$warmup_cosine_decay_schedule(warmup_steps = (nWarmup <- min(c(50, nSGD))),
                                                            decay_steps = max(c(51, nSGD-nWarmup)),
                                                            init_value = learningRateMax/100, 
                                                            peak_value = learningRateMax, 
                                                            end_value =  learningRateMax/100)
          optax_optimizer <-  optax$chain(
            optax$clip(1), 
            optax$adaptive_grad_clip(clipping = 0.10),
            optax$adabelief( learning_rate = LR_schedule, eps=1e-8, eps_root=1e-8, b1 = 0.90, b2 = 0.999)
          )
    
          # model partition, setup state, perform parameter count
          opt_state <- optax_optimizer$init( eq$partition(ModelList, eq$is_array)[[1]] )
          print2(sprintf("Total trainable parameter count: %s", nParams <- sum(unlist(lapply(jax$tree_leaves( eq$partition(ModelList, eq$is_array)[[1]]), function(zer){zer$size})))))
    
          # jit update fxns
          jit_apply_updates <- eq$filter_jit(optax$apply_updates)
          jit_get_update <- eq$filter_jit(optax_optimizer$update)
        }
    
        keys2indices_list <- tapply(1:length(imageKeysOfUnits), imageKeysOfUnits, c)
        if(BAYES_STEP == 2){
          eval(parse(text = sprintf("rm(%s)",paste(ls()[grepl(ls(),pattern="HASH818")] ,collapse= ',') )))
        }
    
        # training loop
        gc(); py_gc$collect()
        IndicesByW <- tapply(1:length(obsW),obsW,c)
        UniqueImageKeysByW <- tapply(imageKeysOfUnits,obsW,function(zer){sort(unique(zer))})
        L2grad_vec <- loss_vec <- rep(NA,times=(nSGD))
        keysUsedInTraining <- tauMeans <- c();
        justCheckCrossFitter <- F
        if(DoTraining <- T){ 
        n_sgd_iters <- nSGD; i_<-1L ; DoneUpdates <- 0L; for(i in i_:nSGD){
            t0 <- Sys.time(); if(i %% 5 == 0 | i == 1){gc(); py_gc$collect()}
    
            ds_next_train <- try(ds_iterator_train$`next`(),T)
            if("try-error" %in% class(ds_next_train)){browser()}
    
            # if we run out of observations, reset iterator
            RestartedIterator <- F; if( is.null(ds_next_train) ){
                print2("Re-setting iterator! (type 1)")
                ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
                ds_next_train <-  ds_iterator_train$`next`(); py_gc$collect()
            }
    
              # get a new batch if size mismatch - size mismatches generate new cached compiled fxns
            if(!RestartedIterator){ if(length(ds_next_train[[3]]) != batchSize){
                  print2("Re-setting iterator! (type 2)")
                  ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
                  ds_next_train <-  ds_iterator_train$`next`(); gc(); py_gc$collect()
            } }
    
            # select batch indices based on keys
            batch_keys <- unlist(  lapply( p2l(ds_next_train[[3]]$numpy()), as.character) )
            batch_indices <- sapply(batch_keys, function(key_){
              f2n( sample(as.character( keys2indices_list[[key_]] ), 1) ) })
            ds_next_train <- ds_next_train[[1]]
            if(any(!batch_indices %in% keysUsedInTraining)){ keysUsedInTraining <- c(keysUsedInTraining, batch_keys[!batch_keys %in% keysUsedInTraining]) }
    
        # training step
        if( justCheckCrossFitter ){ print("Just checking cross-fitter...") }
        if( !justCheckCrossFitter ){ 
            t1 <- Sys.time();
            # rm(GradAndLossAndAux); GradAndLossAndAux <-  eq$filter_jit( eq$filter_value_and_grad( GetLoss, has_aux = T) )
            GradientUpdatePackage <- GradAndLossAndAux(
                                                     MPList[[1]]$cast_to_compute(ModelList), MPList[[1]]$cast_to_compute(ModelList_fixed),
                                                     InitImageProcessFn(jnp$array(ds_next_train),  jax$random$PRNGKey(600L+i), inference = F), # m
                                                     jnp$array(as.matrix(obsW[batch_indices]),dtype = jnp$float16), # treat
                                                     jnp$array(as.matrix(obsY[batch_indices]),dtype = jnp$float16), # y
                                                     jax$random$split(jax$random$PRNGKey( 500L+i ),batchSize),  # vseed
                                                     StateList, # StateList
                                                     jax$random$PRNGKey( 123L+i ), # seed
                                                     MPList # MPlist
                                                     )
    
            if(!"try-error" %in% class(GradientUpdatePackage)){
              # get updated state
              StateList_tmp <- GradientUpdatePackage[[1]][[2]] # state
    
              # get loss + grad
              loss_vec[i] <- myLoss_fromGrad <- np$array( MPList[[2]]$unscale( GradientUpdatePackage[[1]][[1]] ) )# value
              GradientUpdatePackage <- GradientUpdatePackage[[2]] # grads
              GradientUpdatePackage <- eq$partition(GradientUpdatePackage, eq$is_inexact_array)
              GradientUpdatePackage_aux <- GradientUpdatePackage[[2]]; GradientUpdatePackage <- GradientUpdatePackage[[1]]
    
              # unscale + adjust loss scale is some non-finite or NA
              if(i == 1){
                Map2Zero <- eq$filter_jit(function(input){
                  jax$tree_map(function(x){ jnp$where(jnp$isnan(x),
                                                      jnp$array(0), x)}, input) })
                AllFinite <- jax$jit( jmp$all_finite )
              }
              GradientUpdatePackage <- Map2Zero( MPList[[2]]$unscale( GradientUpdatePackage ) )
              AllFinite_DontAdjust <- AllFinite( GradientUpdatePackage )  & jnp$squeeze(jnp$array(!is.infinite(myLoss_fromGrad)))
              MPList[[2]] <- MPList[[2]]$adjust( AllFinite_DontAdjust  )
    
              # update parameters if finite gradients
              DoUpdate <- !is.na(myLoss_fromGrad) & np$array(AllFinite_DontAdjust) & !is.infinite(myLoss_fromGrad)
              if(! DoUpdate ){ print2("Note: Not updating parameters due to non-finite gradients in mixed-precision training...") }
              if( DoUpdate ){
                DoneUpdates <- DoneUpdates + 1
    
                # get updates
                GradientUpdatePackage <- MPList[[1]]$cast_to_param( GradientUpdatePackage )
                GradientUpdatePackage <- jit_get_update( updates = GradientUpdatePackage,
                                                         state = opt_state,
                                                         params =  eq$partition(ModelList, eq$is_array)[[1]] )
    
                # separate updates from state
                opt_state <- GradientUpdatePackage[[2]]
                GradientUpdatePackage <- eq$combine(GradientUpdatePackage[[1]], GradientUpdatePackage_aux)
    
                # perform update
                ModelList <- eq$combine( jit_apply_updates(
                              params = GlobalPartition( eq$partition(ModelList, eq$is_array)[[1]], PartFxn)[[1]],
                              updates = GlobalPartition(GradientUpdatePackage,PartFxn)[[1]] ),
                          eq$partition(ModelList, eq$is_array)[[2]])
    
                # feed in updates for state and model
                StateList <- StateList_tmp
                rm(GradientUpdatePackage, StateList_tmp)
              }
           } #
    
          # print diagnostics
          i_ <- i ; if(i %% 10 == 0 | i < 10 ){
            print2(sprintf("SGD iteration %s of %s - Loss: %.2f (%.1f%%) - Total time (s): %.2f - Grad time (s): %.2f",
                           i, n_sgd_iters,
                           loss_vec[i], 100*mean(loss_vec[i] <= loss_vec[1:i],na.rm=T),
                           (Sys.time() - t0)[[1]], (Sys.time() - t1)[[1]] ))
            loss_vec <- f2n(loss_vec); loss_vec[is.infinite(loss_vec)] <- NA
            try(plot(rank(na.omit(loss_vec)), cex.main = 0.95,ylab = "Loss Function Rank",xlab="SGD Iteration Number"), T)
            if(i > 10){ try_ <- try(points(smooth.spline( rank(na.omit(loss_vec) )), col="red",type = "l",lwd=5),T) }
          }
          if(BAYES_STEP == 1){
          if(abs(i - n_sgd_iters - 1) <= (nWindow <- 20)){
            windowCounter <- windowCounter + 1; z_counter <- 0
            if(T == F){
             for(z in trainable_variables){
              z_counter <- z_counter + 1
              z_name_orig <- z_name_ <- z$name
              z_name_ <- gsub(z_name_, pattern = ":",replacement = "XCOLX")
              z_name_ <- gsub(z_name_, pattern = "/",replacement = "XDASHX")
              if(windowCounter == 1){
                eval(parse(text = sprintf("SZ_%s <- jnp$zeros(jnp$shape(z), dtype = jnp$float32)", z_name_)))
                eval(parse(text = sprintf("SZ2_%s <- jnp$zeros(jnp$shape(z), dtype = jnp$float32)", z_name_)))
              }
              try(eval(parse(text = sprintf("SZ_%s <- my_grads[[z_counter]] + SZ_%s", z_name_, z_name_))), T)
              try(eval(parse(text = sprintf("SZ2_%s <- jnp$square( my_grads[[z_counter]] ) + SZ2_%s", z_name_, z_name_))), T)
              if(i == n_sgd_iters){
                #eval(parse(text = sprintf("RollMean_%s <- (SZ_%s)/nWindow", z_name_, z_name_)))
                try(eval(parse(text = sprintf("RollVar_%s <- tf$cast(1/tf$maximum(0.5,length(obsY)*SZ2_%s/nWindow),dtype=jnp$float32)", z_name_,z_name_))), T)
              }
            }
            }
          }
          }
        }
        } # end for(i in i_:nSGD){
        }
      }
      
      # remember training sequence 
      trainIndices <- which( imageKeysOfUnits %in% keysUsedInTraining )
      testIndices <- which( !imageKeysOfUnits %in% keysUsedInTraining )
    
    if(!justCheckCrossFitter){ 
      print2("Getting predicted quantities...")
      #print("DEBUG MODE IS ON IN GetSummaries()");GetSummaries <- (function(ModelList, ModelList_fixed,
      GetSummaries <- eq$filter_jit(function(ModelList, ModelList_fixed,
                                               m, vseed,
                                               StateList, seed, MPList){
          # image representation model
          if(SharedImageRepresentation){
            m <- ImageRepArm_batch_R(ifelse(optimizeImageRep, yes = list(ModelList), no = list(ModelList_fixed))[[1]],
                                              m, StateList, seed, MPList, T)
            StateList <- m[[2]] ; m <- m[[1]]
          }
          y0_ <- sapply(1L:nMonte_predictive, function(iter){ list(jnp$expand_dims(
                           GetEY0_batch(ModelList, ModelList_fixed,
                                        m,  jnp$add(vseed,iter), StateList, jnp$add(seed,iter), MPList, T)[[1]], 0L)) })
          y1_ <- sapply(1L:nMonte_predictive, function(iter){ list(jnp$expand_dims(
                          GetEY1_batch(ModelList, ModelList_fixed,
                                        m, jnp$add(vseed,iter), StateList, jnp$add(seed,iter), MPList, T), 0L)) })
          y0_ <- jnp$concatenate(y0_,0L); y0_ <- jnp$mean(y0_,0L)
          y1_ <- jnp$concatenate(y1_,0L); y1_ <- jnp$mean(y1_,0L)
    
          # get predictions
          ClusterProbs_est_ <- jnp$concatenate(sapply(1L:nMonte_predictive,function(iter){
                              jnp$expand_dims(jax$nn$softmax(
                                      GetTau_batch(ModelList, ModelList_fixed,
                                                   m, jnp$add(vseed,iter),
                                                   StateList, jnp$add(seed,iter), MPList, T)[[1]]), 0L) } ),0L)
          ClusterProbs_std_ <- jnp$std(ClusterProbs_est_,0L)
          ClusterProbs_est_ <- jnp$mean(ClusterProbs_est_,0L)
          ClusterProbs_lower_conf_ <- ClusterProbs_est_ - ClusterProbs_std_
    
          return( list(y0_, y1_,
                      ClusterProbs_est_,
                      ClusterProbs_lower_conf_,
                      ClusterProbs_std_) )
        })
    
        inference_counter <- 0; nUniqueKeys <- length(unique(imageKeysOfUnits))
        KeyQuantCuts <- gtools::quantcut(1:nUniqueKeys, q = ceiling( nUniqueKeys / (batchSize*0.5) ))
        #passedIterator <- NULL; Results_by_keys <- replicate(nUniqueKeys,list());for(zer in 1:nUniqueKeys){ # use when incorporating X's
        passedIterator <- NULL; Results_by_keys <- replicate(length(unique(KeyQuantCuts)),list());
        for(cut_ in unique(KeyQuantCuts)){ # use when not incorporating X's
          inference_counter <- inference_counter + 1
          zer <- which(cut_ == KeyQuantCuts)
          atP <- max(zer) / nUniqueKeys
          if( any(zer %% 50 == 0) | 1 %in% zer ){ print2(sprintf("Proportion done (inference): %.3f", atP)) }
            {
              setwd(orig_wd);ds_next_in <- GetElementFromTfRecordAtIndices(
                                                             uniqueKeyIndices = which(unique(imageKeysOfUnits) %in% unique(imageKeysOfUnits)[zer]),
                                                             filename = file,
                                                             iterator = passedIterator,
                                                             readVideo = useVideoIndicator,
                                                             image_dtype = image_dtype_tf,
                                                             nObs = length(unique(imageKeysOfUnits)),
                                                             return_iterator = T );setwd(new_wd)
              passedIterator <- ds_next_in[[2]]
              key_ <- try(unlist(  lapply( p2l(ds_next_in[[1]][[3]]$numpy()) , as.character) ), T)
              ds_next_in <- ds_next_in[[1]]
    
              # deal with batch 1 case here
              if(length(ds_next_in[[1]]$shape) == 3 & dataType == "image"){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }
              if(length(ds_next_in[[1]]$shape) == 4 & dataType == "video"){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }
              ds_next_in <- ds_next_in[[1]]
          }
    
          # get summaries and save
          GottenSummaries <- GetSummaries(ModelList, ModelList_fixed,
                                          InitImageProcessFn(jnp$array(ds_next_in),  jax$random$PRNGKey(ai(runif(1,0, 10000))), inference = T),
                                          jax$random$split(jax$random$PRNGKey(ai(runif(1,0, 10000))), ds_next_in$shape[[1]]),
                                          StateList, jax$random$PRNGKey(ai(runif(1,0,100000))), MPList)
          ret_list <- list("y0_" = as.matrix2(GottenSummaries[[1]]),
                           "y1_" = as.matrix2(GottenSummaries[[2]]),
                           "ClusterProbs_est_" = as.matrix2(GottenSummaries[[3]]),
                           "ClusterProbs_lower_conf_" = as.matrix2(GottenSummaries[[4]]),
                           "ClusterProbs_std_" = as.matrix2(GottenSummaries[[5]]),
                           "key" = as.matrix( key_) )
          Results_by_keys[[inference_counter]] <- ret_list
        }
        print2("Done getting predicted quantities...")
        Results_by_keys_list <- Results_by_keys
        Results_by_keys <- apply(do.call(rbind, Results_by_keys_list), 2, function(zer){(do.call(rbind,zer))})
    
        # re-order data
        if(!"matrix" %in% class(Results_by_keys)){
          Results_by_keys <- lapply(Results_by_keys, function(zer){
                                          zer[match(as.character(imageKeysOfUnits), 
                                                    as.character(unlist(  Results_by_keys[["key"]] )) ),]})
        }
        if("matrix" %in% class(Results_by_keys)){
          Results_by_keys <- as.data.frame( Results_by_keys[match(as.character(imageKeysOfUnits), 
                                as.character(unlist(  Results_by_keys[,"key"] ))),] ) 
          Results_by_keys[,1:4] <- apply(Results_by_keys[,1:4], 2, f2n)
        }
  
        # process all outcomes
        Y0_est_mat <- cbind(Y0_est_mat, Y0_est <- Rescale(unlist(Results_by_keys$y0_), doMean = T))
        Y1_est_mat <- cbind(Y1_est_mat, Y1_est <- Rescale(unlist(Results_by_keys$y1_), doMean = T))
        Yobs_est <- Y1_est * obsW + Y0_est * (1-obsW )
        tau_est_mat <- cbind(tau_est_mat,  tau_est <- ( Y1_est - Y0_est))# Y(w)_est already re-scaled 
        browser()
        # hist( Results_by_keys$y0_ ) 
        # hist(obsY[obsW==0])
        # cor(Results_by_keys$y0_[obsW==0],obsY[obsW==0])
        # LE(ModelList, sprintf("%s_d%s","EY0", nDepth_Dense))[[3]]
        # LE(ModelList, sprintf("%s_d%s","EY0", nDepth_Dense))[[1]]
        
        # hist( Results_by_keys$y1_ ) 
        # cor(Results_by_keys$y1_[obsW==1],obsY[obsW==1])
        # hist(obsY[obsW==1])
        
        # hist(tau_est)
        # hist(Results_by_keys$y1_-Results_by_keys$y0_)
        # cor(tau_est,SwappedRowsIndicator)
        # summary(lm(SwappedRowsIndicator~tau_est))
        # LE(ModelList, sprintf("%s_d%s","Tau", nDepth_Dense))[[3]]
        # mean((obsY[obsW==1])) - mean((obsY[obsW==0]))
        # mean(Rescale(obsY[obsW==1],doMean = T)) - mean(Rescale(obsY[obsW==0],doMean = T))
    
        Yobs_est_out <- Yobs_est[testIndices]
        Loss_out_baseline_vec <- c(Loss_out_baseline_vec,
                                   Loss_baseline_out <- mean( (Yobs_est_out - mean(Yobs_est[trainIndices]))^2 )^0.5)
        Loss_out_vec <- c(Loss_out_vec, Loss_out <- mean( (Yobs_est_out - mean(Yobs_est_out))^2)^0.5)
    }
  
        # save test indices 
        TestIndices_list[[trainCounter]] <- testIndices
        TrainIndices_list[[trainCounter]] <- trainIndices
    } # end k fold 
  }
  
  if(justCheckCrossFitter){
    return(list("CF_info" = list("TrainIndices_list"=TrainIndices_list, "TestIndices_list"=TestIndices_list)))
  }
  
    tau_est_cf <- Y0_est_cf <- Y1_est_cf <- replicate(rep(NA,times=nrow(Y0_est_mat)), n = trainCounter)
    for(theIt in 1:trainCounter){
      Y0_est_cf[TestIndices_list[[theIt]],theIt] <- (Y0_est_mat[TestIndices_list[[theIt]],theIt] )
      Y1_est_cf[TestIndices_list[[theIt]],theIt] <- (Y1_est_mat[TestIndices_list[[theIt]],theIt] )
      tau_est_cf[TestIndices_list[[theIt]],theIt] <- (tau_est_mat[TestIndices_list[[theIt]],theIt] )
    }
    print("Sanity 1?")
    print( mean( 1:length(obsY) %in% unlist(TestIndices_list) ))  # sanity value = 1 
    print( sprintf("Sanity %s?",cf_iters) )
    print( table( table(unlist(TestIndices_list)) ) ) 
    Y0_est_se <- (rowMeans(Y0_est_cf^2,na.rm=T) - rowMeans(Y0_est_cf,na.rm=T)^2)^0.5
    Y1_est_se <- (rowMeans(Y1_est_cf^2,na.rm=T) - rowMeans(Y1_est_cf,na.rm=T)^2)^0.5
    tau_est_se <- (rowMeans(tau_est_cf^2,na.rm=T) - rowMeans(tau_est_cf,na.rm=T)^2)^0.5
    
    tau_est <- rowMeans(tau_est_cf, na.rm = T) 
    # plot(tau_est-tau_est_se);abline(h=0)
    # hist(tau_est)
    # plot(tau_est, SwappedRowsIndicator); cor(tau_est, SwappedRowsIndicator)
    Y0_est <- rowMeans(Y0_est_cf, na.rm = T) 
    Y1_est <- rowMeans(Y1_est_cf, na.rm = T) 
    # plot(tau_est, Y1_est-Y0_est,col=obsW+1);abline(a=0,b=1)
    Yobs_est <- Y1_est * obsW + Y0_est * (1-obsW )
    
    # process outcome predictions
    W_test <- obsW[testIndices]
    Y_test_truth <- obsY[testIndices]
    Y_test_est <- Yobs_est[testIndices]
    rm( Results_by_keys_list )

    # process cluster data
    ClusterProbs_lower_conf <- Results_by_keys$ClusterProbs_lower_conf_
    ClusterProbs_std <- Results_by_keys$ClusterProbs_std_
    ClusterProbs_est_full <-  Results_by_keys$ClusterProbs_est_
    ClusterProbs_est <- ClusterProbs_est_full[,2]
    Clust_probs_marginal_final <- colMeans( ClusterProbs_est_full )
    gc(); py_gc$collect()

  # get cluster probs
  if(grepl(heterogeneityModelType, pattern = "variational")){
    print2("Summarizing results...")
    SDDist_Y1_post <- oryx$distributions$Normal(getSDY_params(ModelList, "Y1", "Mean"),
                                jax$nn$softplus(getSDY_params(ModelList, "Y1", "SD")))
    SDDist_Y0_post <- oryx$distributions$Normal(getSDY_params(ModelList, "Y0", "Mean"),
                                 jax$nn$softplus(getSDY_params(ModelList, "Y0", "SD")))
    Sigma1_sd_vec <- as.numeric2(jnp$mean(jax$nn$softplus(SDDist_Y1_post$sample(100L, seed = jax$random$PRNGKey(4L))),0L))
    Sigma0_sd_vec <- as.numeric2(jnp$mean(jax$nn$softplus(SDDist_Y0_post$sample(100L, seed = jax$random$PRNGKey(4L))),0L))

    # get uncertainties
    if(  grepl(heterogeneityModelType, pattern="variational_minimal")  ){
      tau_vec <- as.numeric2( getTau_means(ModelList) ) * Y_sd
      Tau_mean_return_vec <- Rescale( as.numeric2(Tau_mean_vec <- getTau_means(ModelList)), doMean = F)
      Tau_mean_return_sd_vec <- Rescale(as.numeric2(Tau_mean_sd_vec <- jax$nn$softplus(getTau_sds(ModelList))),
                                      doMean = F)
      MeanDist_Tau_post = (oryx$distributions$Normal(Tau_mean_vec, Tau_sd_vec <- jax$nn$softplus(getTau_sds(ModelList))))
      Tau_sd_vec_ <- as.numeric2(tf$sqrt(tf$math$reduce_variance(MeanDist_Tau_post$sample(
                              100L, seed = jax$random$PRNGKey(45L)),0L)))
      Tau_sd_vec <- sqrt(   Sigma1_sd_vec^2 + Sigma0_sd_vec^2 + Tau_sd_vec_^2 )
      Tau_sd_vec <- Tau_sd_vec * Y_sd
    }
  }

  # transportability analysis
  cluster_prob_transport_means <- NULL; if(!is.null(transportabilityMat)){
    print2("Getting posterior predictive mean probabilities for transportability analysis...")
    {
      if(grepl(heterogeneityModelType, pattern = "variational")){ GetProbAndExpand <- function(m){jnp$expand_dims( jax$nn$softmax(GetTau(m,inference = T)),0L) }}
      if(grepl(heterogeneityModelType, pattern = "tarnet")){ GetProbAndExpand <- function(m){jnp$expand_dims( GetTau(m,inference = T),0L) }}
      full_tab <- sort( 1:nrow(transportabilityMat) %% round(nrow(transportabilityMat)/max(1,round(batchFracOut*batchSize))));
      cluster_prob_transport_info <- tapply(1:nrow(transportabilityMat),full_tab,function(zer){
        gc(); py_gc$collect()
        atP <- max(  zer / nrow(transportabilityMat))
        if((round(atP,2)*100) %% 10 == 0){ print2(atP) }
        {
          setwd(orig_wd); ds_next_in <- GetElementFromTfRecordAtIndices(
                                                         uniqueKeyIndices = which(unique(imageKeysOfUnits) %in% imageKeysOfUnits[zer]),
                                                         filename = file,
                                                         readVideo = useVideoIndicator,
                                                         image_dtype = image_dtype_tf,
                                                         nObs = nrow(transportabilityMat) ); setwd(new_wd)
          if(length(ds_next_in[[1]]$shape) == 3 & dataType == "image"){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }
          if(length(ds_next_in[[1]]$shape) == 4 & dataType == "video"){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }
          ds_next_in <- ds_next_in[[1]]
        }
        im_keys <-  InitImageProcessFn( jnp$array(ds_next_in),  jax$random$PRNGKey(600L), inference = T)
        pred_ <- replicate(nMonte_predictive,as.array(GetProbAndExpand(im_keys) ))
        list("mean"=apply(pred_[1,,,],1:2,mean),
             "var"=apply(pred_[1,,,],1:2,var))
      })
      cluster_prob_transport_info <- do.call(rbind,cluster_prob_transport_info)
      cluster_prob_transport_means <- do.call(rbind, cluster_prob_transport_info[,1])
      cluster_prob_transport_var <- do.call(rbind, cluster_prob_transport_info[,2])
      colnames(cluster_prob_transport_means) <- paste('mean_k',1:ncol(cluster_prob_transport_means), sep = "")
      colnames(cluster_prob_transport_var) <- paste('var_k',1:ncol(cluster_prob_transport_means), sep = "")
      }
    if(TRUE %in% transportabilityMat){
        transportabilityMat <- as.data.frame(cbind(
                                     "key"=imageKeysOfUnits,
                                     "long"=long,
                                     "lat"=lat))
        cluster_prob_transport_means <- Results_by_keys$ClusterProbs_est_
        cluster_prob_transport_var <-  Results_by_keys$ClusterProbs_std_^2
      }
    transportabilityMat <- try(cbind(transportabilityMat,
                                   cluster_prob_transport_means,
                                   cluster_prob_transport_var),T)
  }

  # perform plots
  if( changed_wd ){ setwd(  orig_wd  ) }
  if( plotResults == T){
    CausalImageHeterogeneity_plot <- paste(deparse(CausalImageHeterogeneity_plot),collapse="\n")
    CausalImageHeterogeneity_plot <- gsub(CausalImageHeterogeneity_plot,pattern="function \\(\\)",replace="")
    eval( parse( text = CausalImageHeterogeneity_plot ), envir = environment() )
  }
  try(setwd(orig_wd), T)
  
  return( list(
                 "clusterTaus_mean" = as.numeric2(Tau_mean_return_vec),
                 "clusterTaus_sd" = as.numeric2(Tau_mean_return_sd_vec),
                 "clusterSigmas" = Tau_sd_vec,
                 "clusterProbs_mean" = ClusterProbs_est_full,
                 "clusterProbs_sd" = ClusterProbs_std,
                 "clusterProbs_lowerConf" = ClusterProbs_lower_conf,
                 "impliedATE" = mean(  tau_est ),
                 "individualTau_est" = tau_est,
                 "Y0_est" = Y0_est,
                 "Y1_est" = Y1_est,
                 "Yobs_est" = Yobs_est,
                 
                 "Y0_est_se" = Y0_est_se, 
                 "Y1_est_se" = Y1_est_se, 
                 "tau_est" = tau_est, 
                 "tau_est_se" = tau_est_se, 
                 
                 "loss_vec" = loss_vec,
                 "Loss_baseline_out" = Loss_baseline_out,
                 "Loss_out" = Loss_out,
                 "CF_info" = list("cf_keys_split" = cf_keys_split,
                                  "Loss_out_vec" = Loss_out_vec,
                                  "Loss_out_baseline_vec" = Loss_out_baseline_vec,
                                  "Y0_est_mat" = Y0_est_mat,
                                  "Y1_est_mat" = Y1_est_mat, 
                                  "tau_est_mat" = tau_est_mat,
                                  "TestIndices_list" = TestIndices_list,
                                  "TrainIndices_list" = TrainIndices_list),
                 "transportabilityMat" = transportabilityMat,
                 "plottedCoordinatesList" = plotting_coordinates_list,
                 "whichNA_dropped" = whichNA_dropped) )
}

