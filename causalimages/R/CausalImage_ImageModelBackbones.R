#!/usr/bin/env Rscript
#' Generates image and video representations useful in earth observation tasks for casual inference.
#'
#' Generates image and video representations useful in earth observation tasks for casual inference, following the approach in Rolf, Esther, et al.  (2021).
#'
#' @param file Path to a tfrecord file generated by `causalimages::WriteTfRecord`.
#' @param imageKeysOfUnits A vector of length `length(imageKeysOfUnits)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into the package to call images into memory.
#' @param conda_env A `conda` environment where computational environment lives, usually created via `causalimages::BuildBackend()`. Default = `"CausalImagesEnv"`
#' @param conda_env_required A Boolean stating whether use of the specified conda environment is required.
#' @param kernelSize Dimensif(ions used in the convolution kernels.
#' @param temporalKernelSize Dimensions used in the temporal part of the convolution kernels if using image sequences.
#' @param nWidth_ImageRep Number of embedding features output.
#' @param strides  Integer specifying the strides used in the convolutional layers.
#' @param InitImageProcess (default = `NULL`) Initial image processing function. Usually left `NULL`.
#' @param batchSize Integer specifying batch size in obtaining representations.
#' @param dataType String specifying whether to assume `"image"` or `"video"` data types. Default is `"image"`.
#' @param TfRecords_BufferScaler The buffer size used in `tfrecords` mode is `batchSize*TfRecords_BufferScaler`. Lower `TfRecords_BufferScaler` towards 1 if out-of-memory problems.
#' @param seed Integer specifying the seed for pseudo random number generation.
#'
#' @return A list containing two items:
#' \itemize{
#' \item `Representations` (matrix) A matrix containing image/video representations, with rows corresponding to observations.
#' \item `ImageRepArm_OneObs,ImageRepArm_batch_R, ImageRepArm_batch` (functions) Image modeling functions.
#' \item `ImageModel_And_State_And_MPPolicy_List` List containing image model parameters fed into functions.
#' }
#'
#' @section References:
#' \itemize{
#' \item Rolf, Esther, et al. "A generalizable and accessible approach to machine learning with global satellite imagery." *Nature Communications* 12.1 (2021): 4392.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @export
#' @md


GetImageRepresentations <- function(
    imageKeysOfUnits = NULL,
    file = NULL,
    conda_env = "CausalImagesEnv",
    conda_env_required = T,
    returnContents = T,
    getRepresentations = T,
    imageModelClass = "VisionTransformer",
    Sys.setenv_text = NULL,

    InitImageProcess = NULL,
    nWidth_ImageRep = 64L,
    nDepth_ImageRep = 1L,
    nDepth_TemporalRep = 1L,
    batchSize = 16L,
    optimizeImageRep = T,
    strides = 1L,
    temporalKernelSize = 2L,
    kernelSize = 3L,
    patchEmbedDim = 16L,
    TfRecords_BufferScaler = 10L,
    dropoutRate,
    dataType = "image",
    bn_momentum = 0.99,
    inputAvePoolingSize = 1L, # set > 1L if seeking to downshift the image resolution
    seed = NULL){

  # initialize tensorflow if not already initialized
  if(   !"logical" %in% class(try(as.numeric(np$array(jnp$square(1.)))==1,T))  | !all(c("gc", "jax", "numpy", "numpy", "jax.numpy", "jmp", "optax", "equinox") %in% ls()) ){
    print2("Establishing connection to computational environment (build via causalimages::BuildBackend())")
    library(tensorflow); if(!is.null(conda_env)){
      try(reticulate::use_condaenv(conda_env, required = conda_env_required),T)
    }
    if(!is.null(Sys.setenv_text)){ eval(parse(text = Sys.setenv_text)) }
    py_gc <- reticulate::import("gc")
    jax <<- reticulate::import("jax")
    jnp <<- reticulate::import("jax.numpy")
    np <<- reticulate::import("numpy")
    jmp <<- reticulate::import("jmp")
    optax <<- reticulate::import("optax")
    eq <<- reticulate::import("equinox")
    jax$config$update("jax_enable_x64", FALSE);
  }
  gc(); try(py_gc$collect(), T)
  if(is.null(seed)){ seed <- as.integer(runif(1,1,10000)) }

  # image dtype
  image_dtype <- jnp$float16
  image_dtype_tf <- tf$float16

  if( batchSize > length( unique(imageKeysOfUnits) )){
    batchSize <- length( unique(imageKeysOfUnits)  )
  }

  # define base tf record + train/test fxns
  orig_wd <- getwd()
  if(  !is.null(  file  )  ){
    # established tfrecord connection
    tf_record_name <- file
    if( !grepl(tf_record_name, pattern = "/") ){
      tf_record_name <- paste("./",tf_record_name, sep = "")
    }
    tf_record_name <- strsplit(tf_record_name,split="/")[[1]]
    setwd( new_wd <- paste(tf_record_name[-length(tf_record_name)],collapse = "/") )
    tf_dataset = tf$data$TFRecordDataset(  tf_record_name[length(tf_record_name)] )

    # helper functions
    useVideo <- (dataType == "video")
    getParsed_tf_dataset_inference <- function(tf_dataset){
        dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo, image_dtype = image_dtype_tf)} ) # return
      return( dataset <- dataset$batch( as.integer(max(2L, round(batchSize/2L)  ))) )
    }

    getParsed_tf_dataset_train <- function(tf_dataset){
      dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo, image_dtype = image_dtype_tf)} ) # return
      dataset <- dataset$shuffle(tf$constant(as.integer(TfRecords_BufferScaler*batchSize), dtype=tf$int64),
                                 reshuffle_each_iteration = T)
      dataset <- dataset$batch(as.integer(batchSize))
    }

    # setup iterators
    tf_dataset_train <- getParsed_tf_dataset_train( tf_dataset )
    tf_dataset_inference <- getParsed_tf_dataset_inference( tf_dataset )

    # reset iterators
    ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
    ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )
  }

  # acquire image to set dimensions
  setwd(orig_wd); test_ <- tf$expand_dims(GetElementFromTfRecordAtIndices( uniqueKeyIndices = 1L,
                                                             filename = file,
                                                             readVideo = useVideo,
                                                             image_dtype = image_dtype_tf,
                                                             nObs = length(unique(imageKeysOfUnits)))[[1]],0L); setwd(new_wd)
  imageDims <- ai( length( dim(test_) ) - 2L )
  rawChannelDims <- ai( dim(test_)[length(dim(test_))] )
  rawSpatialDims <- ai( dim(test_)[length(dim(test_))-1] )

  # setup jax model
  {
    print2("Setting up image representation model...")
    NonLinearScaler <- 3^2
    MPList <- list(jmp$Policy(compute_dtype="float16", 
                              param_dtype="float32", 
                              output_dtype="float32"),
                   jmp$DynamicLossScale(jnp$array(2^15), period = 1000L))


    # coerce to integer for safety
    kernelSize <- ai(kernelSize); strides <- ai(strides)
    rawChannelDims <- ai(rawChannelDims); nWidth_ImageRep <- ai(nWidth_ImageRep)

    # set batch name
    batch_axis_name <- "batch";
    if(!"bn_momentum" %in% ls()){ bn_momentum <- 0.99 }

    # transformer preliminaries
    ffmap <- jax$vmap(function(L_, x){ L_(x) }, in_axes = list(NULL,0L))
    RMS_norm <- function(x_){ jnp$divide( x_, jnp$sqrt(0.001+jnp$mean(jnp$square(x_), -1L, keepdims=T))) }
    LayerNorm <- function(x_){ jax$nn$standardize( x_, -1L, epsilon = 1e-5) }
    InvSoftPlus <- function(x){ jnp$log(jnp$exp(x) - 1) }

    # set up model
    if(imageModelClass == "VisionTransformer"){
      # Calculate number of patches
      nPatches_side = ai(rawSpatialDims / patchEmbedDim)
      nPatches = ai(nPatches_side^2)
      InitialPatchDims <- ai( (nPatches_side*patchEmbedDim*nPatches_side*patchEmbedDim)/nPatches * rawChannelDims )

      # rotary embedding setup
      theta_vals_patch <-  10000^( -(2*( 1:(nWidth_ImageRep/2) )) / nWidth_ImageRep ) # p. 5
      theta_vals_patch <- unlist(sapply(theta_vals_patch,function(z){list(c(z,z))}))
      theta_vals_patch <- jnp$expand_dims(jnp$array(theta_vals_patch), 0L)
      nTimeSteps_patch <- ai(nPatches_side^2)
      position_patch <- jnp$expand_dims( jnp$arange(1L, nTimeSteps_patch+3L), 1L)  # + 3L for stop, start
      cos_terms_patch <- jnp$cos( pos_times_theta_patch <- (position_patch *  theta_vals_patch) ) # p. 7
      sin_terms_patch <- jnp$sin( pos_times_theta_patch )
      WideMultiplicationFactor <- 3.5
      nTransformerOutputWidth <- nWidth_ImageRep

    StateList <- ModelList <- replicate(nDepth_ImageRep+1, jnp$array(0.)) # initialize with 0's
    for(d_ in 1L:nDepth_ImageRep){
        SpatialTransformerRenormer_d <- list(jnp$array( t(rep(1,times=nWidth_ImageRep) ) ),
                                             jnp$array( t(rep(1,times=nWidth_ImageRep) ) ))
        SpatialMultihead_d <- eq$nn$MultiheadAttention(
                            query_size = nWidth_ImageRep,
                            output_size = nWidth_ImageRep,
                            num_heads = 12L,
                            use_output_bias = F,
                            key = jax$random$PRNGKey( 23453355L + seed + d_) )
        SpatialFF_d <- list(eq$nn$Linear(in_features = nWidth_ImageRep,
                                         out_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                                         use_bias = F, # hidden bias
                                         key = jax$random$PRNGKey(ai(3340L + seed + d_))),
                            eq$nn$Linear(in_features = nWidth_ImageRep,
                                         out_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                                         use_bias = F, # swiglu bias
                                         key = jax$random$PRNGKey(ai(3311L + seed + d_))),
                            eq$nn$Linear(in_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                                         out_features = nWidth_ImageRep,
                                         use_bias = F, # final bias
                                         key = jax$random$PRNGKey(ai(33324L + seed +  d_))))
        StateList[[d_]] <- eval(parse(text = sprintf("list('BNState_ImRep_d%s'= jnp$array(0.))", d_)))
        ModelList[[d_]] <- eval(parse(text = sprintf('list("SpatialMultihead_d%s" = SpatialMultihead_d,
                                                          "SpatialFF_d%s" = SpatialFF_d,
                                                          "SpatialResidualWts_d%s" = list(InvSoftPlus(jnp$array(1./sqrt( NonLinearScaler * nDepth_ImageRep ))),InvSoftPlus(jnp$array(1./sqrt( NonLinearScaler * nDepth_ImageRep )))),
                                                          "SpatialTransformerRenormer_d%s" = SpatialTransformerRenormer_d)', d_, d_, d_, d_ )))
    }
    ModelList[[d_+1]] <- list("SpatialTransformerSupp" = list(
        jnp$array(t(runif(nWidth_ImageRep,-sqrt(6/nWidth_ImageRep),sqrt(6/nWidth_ImageRep)))), # Start
        jnp$array(t(runif(nWidth_ImageRep,-sqrt(6/nWidth_ImageRep),sqrt(6/nWidth_ImageRep)))), # Stop
        jnp$array(rep(1,times = nWidth_ImageRep)),  # RMS weighter
        eq$nn$Conv(kernel_size = c(patchEmbedDim, patchEmbedDim),
                   num_spatial_dims = 2L, stride = c(patchEmbedDim,patchEmbedDim),
                   in_channels = rawChannelDims, use_bias = F,
                   out_channels = nWidth_ImageRep, key = jax$random$PRNGKey(4L+1040L+seed)), # patch embed
        eq$nn$Linear(in_features = nWidth_ImageRep, out_features =  nTransformerOutputWidth,
                      use_bias = F, key = jax$random$PRNGKey(999L + seed  ) ) # final dense proj
      ))
    }
    TransformerBackbone <- function(ModelList, m, StateList, seed, MPList, inference, type){
      if(type == "Spatial"){ # patch embed
          m <- LE(ModelList,"SpatialTransformerSupp")[[4]](jnp$transpose(m, c(2L, 0L, 1L)))
          m <- jnp$transpose(jnp$reshape(m, list(m$shape[[1]],-1L)))
          print2(sprintf("Transformer dims: [%s]", paste(unlist(m$shape),collapse=",")))
      }

      # append start and stop condons
      m <- jnp$concatenate(list(LE(ModelList,sprintf("%sTransformerSupp",type))[[1]], m), 0L)
      m <- jnp$concatenate(list(m, LE(ModelList,sprintf("%sTransformerSupp",type))[[2]]), 0L)

      # start neural block
      DepthOfThisTransformer <- ifelse(type=="Spatial", yes = nDepth_ImageRep, no = nDepth_TemporalRep)
      print2(sprintf("Starting Transformer block [depth %s, %s]...", DepthOfThisTransformer, type))
      mtm1 <- m; for(d_ in 1L:DepthOfThisTransformer){
          # standardize
          m <- RMS_norm(m)*LE(ModelList,sprintf("%sTransformerRenormer_d%s",type, d_))[[1]]

          # rotary embeddings
          m_pos <- MPList[[1]]$cast_to_compute( jnp$zeros_like( m ) )
          for( IDX in seq(0L, nWidth_ImageRep, by = 2L)){
            m_pos <- m_pos$at[,jnp$array(IDX)]$add(  jnp$negative(jnp$take(m, IDX+1L, axis = 1L) ) )
            m_pos <- m_pos$at[,jnp$array(IDX+1L)]$add( jnp$take(m, IDX, axis = 1L) )
          }
          m_pos <- m*jnp$take( MPList[[1]]$cast_to_compute(cos_terms_patch), jnp$array(0L:(m$shape[[1]]-1L)), 0L) +
                    m_pos*jnp$take(MPList[[1]]$cast_to_compute(sin_terms_patch), jnp$array(0L:(m$shape[[1]]-1L)), 0L) # p. 7

          # multihead attention block
          m <- try(LE(ModelList,sprintf("%sMultihead_d%s",type,d_))(
                      query = m_pos,
                      key_  = m_pos,
                      value = m), T) #key = seed, # breaks in GPU mode

          # residual connection
          mtm1 <- m <- mtm1 + m*jax$nn$softplus( LE(ModelList,sprintf("%sResidualWts_d%s",type,d_))[[1]]$astype(jnp$float32) )$astype(mtm1$dtype)

          # normalize
          m <- RMS_norm(m) * LE(ModelList,sprintf("%sTransformerRenormer_d%s",type,d_))[[2]]

          # feed forward
          m <- jax$nn$swish(ffmap(LE(ModelList,sprintf("%sFF_d%s",type,d_))[[1]], m)) *
                        ffmap(LE(ModelList,sprintf("%sFF_d%s",type,d_))[[2]], m) # swiglu proj
          m <- ffmap(LE(ModelList,sprintf("%sFF_d%s",type,d_))[[3]], m) # final proj

          # residual connection to previous state
          mtm1 <- m <- mtm1 + m*jax$nn$softplus( LE(ModelList,sprintf("%sResidualWts_d%s",type,d_))[[2]]$astype(jnp$float32) )$astype(mtm1$dtype)
      }
      print2(sprintf("Done with Transformer block [depth %s, %s]...", DepthOfThisTransformer, type))

      # path control -- only executed in final pass over sequences
      TransformerOutputPathControl <- (dataType == "image" & type == "Spatial") | 
                                            (dataType == "video" & type == "Temporal")

      # take CLS embedding from position 0 
      m <- jnp$take(m, indices = 0L, axis = 0L)

      if( TransformerOutputPathControl ){
        # final norm
        # m <- jnp$squeeze(LayerNorm( jnp$expand_dims(m,0L) )*LE(ModelList,sprintf("%sTransformerSupp",type))[[3]])

        # linear proj, note: dense starts with linear projection  
        # m <- LE(ModelList,sprintf("%sTransformerSupp", type))[[5]]( m )  
      }
    return( list(m, StateList) )
    }
    if(imageModelClass == "CNN"){
    StateList <- ModelList <- replicate(nDepth_ImageRep, list())
    for(d_ in 1L:nDepth_ImageRep){
        if(d_ > 1){ strides <- 1L }
      SeperableSpatial_jax <- eq$nn$Conv(kernel_size = c(kernelSize, kernelSize),
                                         num_spatial_dims = 2L, stride = c(strides, strides),
                                         in_channels = dimsSpatial <- ai(ifelse(d_ == 1L, yes = rawChannelDims, no = nWidth_ImageRep)),
                                         out_channels = dimsSpatial, use_bias = F,
                                         groups = dimsSpatial,
                                         key = jax$random$PRNGKey(4L+d_+seed))
      SeperableFeature_jax <- eq$nn$Conv(in_channels = dimsSpatial, 
                                         out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
                                         groups = 1L, 
                                         num_spatial_dims = 2L,stride = c(1L,1L), use_bias = T,
                                         key = jax$random$PRNGKey(50L+d_+seed))
      SeperableFeature2_jax <- eq$nn$Conv(in_channels = nWidth_ImageRep, 
                                         out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
                                         groups = 1L, 
                                         num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                         key = jax$random$PRNGKey(530L+d_+seed))
      SeperableFeature3_jax <- eq$nn$Conv(in_channels = nWidth_ImageRep, 
                                          out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
                                          groups = 1L, 
                                          num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                          key = jax$random$PRNGKey(5340L+d_+seed))
      SeperableFeature4_jax <- eq$nn$Conv(in_channels = nWidth_ImageRep, 
                                          out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
                                          groups = 1L, 
                                          num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                          key = jax$random$PRNGKey(53140L+d_+seed))
      ResidualTm1Path_jax <- eq$nn$Conv(in_channels = dimsSpatial, 
                                        out_channels = nWidth_ImageRep,
                                        groups = 1L,
                                        kernel_size = c(1L,1L),
                                        num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                        key = jax$random$PRNGKey(3250L+d_+seed))
      ResidualTPath_jax <- eq$nn$Conv(in_channels = nWidth_ImageRep, 
                                      out_channels = nWidth_ImageRep,
                                      groups = 1L,
                                      kernel_size = c(1L,1L),
                                      num_spatial_dims = 2L,stride = c(1L,1L), use_bias = F,
                                      key = jax$random$PRNGKey(32520L+d_+seed))

        # reset weights with Xavier/Glorot
        if(T == F){ 
        SeperableSpatial_jax <- eq$tree_at(function(l){l$weight}, SeperableSpatial_jax,
                                           jax$random$uniform(key=jax$random$PRNGKey(5L+d_+seed),
                                                              minval = -sqrt(6/(dimsSpatial+dimsSpatial)),
                                                              maxval = sqrt(6/(dimsSpatial+dimsSpatial)),
                                                              shape = jax$tree_util$tree_leaves(SeperableSpatial_jax)[[1]]$shape) )
        }

        # setup bn for CNN block
        LayerBN1 <- eq$nn$BatchNorm(
          #input_size = (BatchNormDim1 <- dimsSpatial), # input norm
          input_size = (BatchNormDim1 <- nWidth_ImageRep), # post-process norm
          axis_name = batch_axis_name,
          momentum = bn_momentum, eps = (BN_ep <- 0.01^2), channelwise_affine = F)
        LayerBN2 <- eq$nn$BatchNorm(
          input_size = (BatchNormDim2 <- nWidth_ImageRep), 
          axis_name = batch_axis_name,
          momentum = bn_momentum, eps = BN_ep, channelwise_affine = F)
        StateList[[d_]] <- eval(parse(text = sprintf("list('BNState_ImRep1_d%s'=list(eq$nn$State( LayerBN1 )),
                                                        'BNState_ImRep2_d%s'=list(eq$nn$State( LayerBN2 )))", d_, d_)))
        ModelList[[d_]] <- eval(parse(text = sprintf('list("SeperableSpatial_jax_d%s" = SeperableSpatial_jax,
                                  "SeperableFeature_jax_d%s" = SeperableFeature_jax,
                                  "SeperableFeature2_jax_d%s" = SeperableFeature2_jax, 
                                  "SeperableFeature3_jax_d%s" = SeperableFeature3_jax, 
                                  "SeperableFeature4_jax_d%s" = SeperableFeature4_jax, 
                                  "ResidualTm1Path_jax_d%s" = ResidualTm1Path_jax,
                                  "ResidualTPath_jax_d%s" = ResidualTPath_jax,
                                  "SpatialResidualWts_d%s" = list(InvSoftPlus(jnp$array(1./sqrt( NonLinearScaler * nDepth_ImageRep ))),InvSoftPlus(jnp$array(1./ sqrt( NonLinearScaler * nDepth_ImageRep )))),
                                  "BN_ImRep1_d%s" = list(LayerBN1, jnp$array(rep(1.,times=BatchNormDim1)) ),
                                  "BN_ImRep2_d%s" = list(LayerBN2, jnp$array(rep(1.,times=BatchNormDim2)) ))', d_, d_, d_, d_, d_, d_, d_, d_, d_, d_ )))
    }
    ModelList[[d_+1]] <- list("SpatialTransformerSupp" = list(
      "FinalCNNProj"=eq$nn$Linear(in_features = nWidth_ImageRep, out_features =  nWidth_ImageRep, # final dense proj
                   use_bias = F, key = jax$random$PRNGKey(9989L + seed  ) )  ),
      "FinalCNNBN"=list(LayerBN <- eq$nn$BatchNorm(input_size = nWidth_ImageRep,
                            axis_name = batch_axis_name,
                            momentum = bn_momentum, eps = BN_ep, channelwise_affine = F), 
                            jnp$array(rep(1.,times=nWidth_ImageRep)) ))
    StateList[[d_+1]] <- eval(parse(text = "list('BNState_ImRep_FinalCNNBN'=eq$nn$State( LayerBN ))"))
    }
    if(dataType == "video"){
      for(dt_ in 1L:nDepth_TemporalRep){
        TemporalTransformerRenormer_d <- list(jnp$array( t(rep(1,times=nWidth_ImageRep) ) ),
                                              jnp$array( t(rep(1,times=nWidth_ImageRep) ) ))
        TemporalMultihead_d <- eq$nn$MultiheadAttention(
                                    query_size = nWidth_ImageRep,
                                    output_size = nWidth_ImageRep,
                                    num_heads = 8L,
                                    use_output_bias = F,
                                    key = jax$random$PRNGKey( 23453355L + seed+dt_) )
        TemporalFF_d  <- list(eq$nn$Linear(in_features = nWidth_ImageRep,
                          out_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                          use_bias = F, # hidden bias
                          key = jax$random$PRNGKey(ai(33430L + 1L+dt_ + seed  ))),
             eq$nn$Linear(in_features = nWidth_ImageRep,
                          out_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                          use_bias = F, # swiglu bias
                          key = jax$random$PRNGKey(ai(33311L + 1L+dt_ + seed ))),
             eq$nn$Linear(in_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                          out_features = nWidth_ImageRep,
                          use_bias = F, # final bias
                          key = jax$random$PRNGKey(ai(333324L + 1L + dt_ + seed ))))
        ModelList[[length(ModelList) + 1]] <- eval(parse(text = sprintf('list("TemporalTransformerRenormer_d%s" = TemporalTransformerRenormer_d,
                                                "TemporalMultihead_d%s" = TemporalMultihead_d,
                                               "TemporalResidualWts_d%s" = list(InvSoftPlus(jnp$array(1./sqrt( NonLinearScaler * nDepth_TemporalRep ))),InvSoftPlus(jnp$array(1./sqrt( NonLinearScaler * nDepth_TemporalRep )))),
                                               "TemporalFF_d%s" = TemporalFF_d)', dt_,dt_,dt_,dt_)))
      }
      ModelList[[length(ModelList)+1]] <- list("TemporalTransformerSupp" = list(
                            jnp$array(t(runif(nWidth_ImageRep,-sqrt(6/nWidth_ImageRep),sqrt(6/nWidth_ImageRep)))), # Start
                            jnp$array(t(runif(nWidth_ImageRep,-sqrt(6/nWidth_ImageRep),sqrt(6/nWidth_ImageRep)))), # Stop
                            jnp$array( t(rep(1,times=nWidth_ImageRep) ) ),
                            jnp$array(0.), # unused  in temporal
                            eq$nn$Linear(in_features = nWidth_ImageRep, out_features =  nTransformerOutputWidth,
                                         use_bias = F, key = jax$random$PRNGKey(999L+d_+seed  ))
                            ))
    }

    # m <- InitImageProcess( jnp$array( batch_inference[[1]]),T)[0,0,,,];  d__ <- 1L; inference <- F
    # m <- InitImageProcess( jnp$array( batch_inference[[1]]), T);  d__ <- 1L; inference <- F
    ImageRepArm_SpatialArm <- function(ModelList, m, StateList, seed, MPList, inference){
      if(imageModelClass == "VisionTransformer"){
          m <- TransformerBackbone(ModelList, m, StateList, seed, MPList, inference, type = "Spatial")
          StateList <- m[[2]]; m <- m[[1]]
      }
      if(imageModelClass == "CNN"){
          m <- jnp$transpose(m, c(2L, 0L, 1L)) # transpose to CWH
          for(d__ in 1:nDepth_ImageRep){
            # residual path 
            mtm1 <- m
            
            # seperable convolution   
            m <- LE(ModelList,sprintf("SeperableFeature_jax_d%s",d__))(
                    LE(ModelList,sprintf("SeperableSpatial_jax_d%s",d__))(m))

            if(! optimizeImageRep){
              #m <- jax$nn$swish( 
                #LE(ModelList,sprintf("SeperableFeature2_jax_d%s",d__))(m) )  * LE(ModelList,sprintf("SeperableFeature3_jax_d%s",d__))(m)
              # m <- LE(ModelList,sprintf("SeperableFeature3_jax_d%s",d__))(m)
              # hist(np$array(m$val)) 
              
              m <- jnp$concatenate(list(mx_ <- jnp$max(m, c(1L:2L)), mn_ <- jnp$mean(m, c(1L:2L))), 0L)
              return( list(m, StateList)  )
            }
            
            if( optimizeImageRep ){
              # bnorm
              if( optimizeImageRep &  d__ > 1 & T == T){ 
                m <- LE(ModelList, sprintf("BN_ImRep1_d%s",d__))[[1]](m, state = LE(StateList, sprintf("BNState_ImRep1_d%s",d__))[[1]], inference = inference)
                StateIndex <- paste(sapply(LE_index( StateList, sprintf("BNState_ImRep1_d%s",d__) ),
                                           function(zer){ paste("[[", zer, "]]") }), collapse = "")
                eval(parse(text = sprintf("StateList%s <- m[[2]]", StateIndex))); m <- m[[1]]
                if(dataType == "image"){ m <- m * jnp$expand_dims(jnp$expand_dims(LE(ModelList, sprintf("BN_ImRep1_d%s",d__))[[2]],1L),1L) }
                if(dataType == "video"){ m <- m * jnp$expand_dims(jnp$expand_dims(jnp$expand_dims(LE(ModelList, sprintf("BN_ImRep1_d%s",d__))[[2]],1L),1L),1L)  }
              }
              
              # swiglu act fxn
              m <- jax$nn$swish( 
                    LE(ModelList,sprintf("SeperableFeature2_jax_d%s",d__))(m)
                )  * LE(ModelList,sprintf("SeperableFeature3_jax_d%s",d__))(m)
              m <- LE(ModelList,sprintf("SeperableFeature3_jax_d%s",d__))(m)
              
              if( optimizeImageRep & T == T ){ 
                m <- LE(ModelList, sprintf("BN_ImRep2_d%s",d__))[[1]](m, state = LE(StateList, sprintf("BNState_ImRep2_d%s",d__))[[1]], inference = inference)
                StateIndex <- paste(sapply(LE_index( StateList, sprintf("BNState_ImRep2_d%s",d__) ),
                                           function(zer){ paste("[[", zer, "]]") }), collapse = "")
                eval(parse(text = sprintf("StateList%s <- m[[2]]", StateIndex))); m <- m[[1]]
                if(dataType == "image"){ m <- m * jnp$expand_dims(jnp$expand_dims(LE(ModelList, sprintf("BN_ImRep2_d%s",d__))[[2]],1L),1L) }
                if(dataType == "video"){ m <- m * jnp$expand_dims(jnp$expand_dims(jnp$expand_dims(LE(ModelList, sprintf("BN_ImRep2_d%s",d__))[[2]],1L),1L),1L)  }
              }
              
              # adaptive max pooling 
              if(d__ %% 2 %in% c(0,1)){ 
                m <- eq$nn$AdaptiveMaxPool2d(list(ai(m$shape[[2]]*(SpatialShrinkRate <- 0.75)), ai(m$shape[[3]]*SpatialShrinkRate)))( m ) 
                m <- LE(ModelList,sprintf("ResidualTPath_jax_d%s",d__))(m)
              }
              
              if(T == F){ 
                hist(c(np$array(mtm1$val))); apply(np$array(mtm1$val),2,sd)
                hist(c(np$array(m$val))); apply(np$array(m$val),2,sd)
                hist(c(np$array( eq$nn$AdaptiveAvgPool2d(list(m$shape[[2]],m$shape[[3]]))( LE(ModelList,sprintf("ResidualTm1Path_jax_d%s",d__))(mtm1))$val)))
                # hist(c(np$array(LE(ModelList,sprintf("ResidualTm1Path_jax_d%s",d__))(mtm1)$val)))
              }
              
              # residual connection 
              m <- eq$nn$AdaptiveAvgPool2d(list(m$shape[[2]],m$shape[[3]]))(
                            LE(ModelList,sprintf("ResidualTm1Path_jax_d%s",d__))(mtm1)) +  
                         m * jax$nn$softplus( LE(ModelList,sprintf("SpatialResidualWts_d%s",d_))[[2]]$astype(jnp$float32) )$astype(mtm1$dtype)
            }
          }
          
          # final pooling, final norm (if used), and projection 
          m <- jnp$max(m, c(1L:2L))
          #m <- jnp$mean(m, c(1L:2L))
          if(optimizeImageRep & T == T){
              print2("Performing final CNN normalization...")
              m <- LE(ModelList, "FinalCNNBN")[[1]](m, state = LE(StateList, "BNState_ImRep_FinalCNNBN"), 
                                                    inference = inference)
              StateIndex <- paste(sapply(LE_index( StateList, "BNState_ImRep_FinalCNNBN" ), function(zer){ paste("[[", zer, "]]") }), collapse = "")
              eval(parse(text = sprintf("StateList%s <- m[[2]]", StateIndex))); m <- m[[1]]
              if(dataType == "image"){ m <- m * LE(ModelList, "FinalCNNBN")[[2]] }
              if(dataType == "video"){ m <- m * jnp$expand_dims(jnp$expand_dims(LE(ModelList, "FinalCNNBN")[[2]],1L),1L) }
          }
          # m <- LE(ModelList,"FinalCNNProj")(m) # note: projection done in next dense layer
      }
      return( list(m, StateList)  )
    }
    ImageRepArm_batch <- eq$filter_jit(ImageRepArm_batch_R <- function(ModelList, m, StateList, seed, MPList, inference){
    #ImageRepArm_batch <- (ImageRepArm_batch_R <- function(ModelList, m, StateList, seed, MPList, inference){ print("DEBUG MODE IS ON IN IMAGE BACKBONE"); Sys.sleep(5L); 
      ModelList <- MPList[[1]]$cast_to_compute( ModelList )
      StateList <- MPList[[1]]$cast_to_compute( StateList )

      # squeeze temporal dim if needed
      if(dataType == "video"){ m <- jnp$reshape(m, c(-1L, (orig_shape_m <- jnp$shape(m))[3:5])) }

      print2(sprintf("Image stack dims: [%s]", paste(unlist(m$shape),collapse=",")))

      # get spatial representation
      # m <- m[1,,,]
      m <- jax$vmap(function(ModelList, m,
                             StateList, seed, MPList, inference){
            ImageRepArm_SpatialArm(ModelList, m,
                                   StateList, seed, MPList, inference) },
               in_axes = list(NULL, 0L, NULL, NULL, NULL, NULL),
               axis_name = batch_axis_name,
               out_axes = list(0L,NULL))(ModelList, m, StateList, seed, MPList, inference)
      StateList <- m[[2]]; m <- m[[1]]

      # unsqueeze temporal dim if needed
      if(dataType == "video"){
        m <- jnp$reshape(m, c(orig_shape_m[1:2], -1L))
        m <- jax$vmap(function(ModelList, m,
                               StateList, seed, MPList, inference){
                    TransformerBackbone(ModelList, m,
                                        StateList, seed, MPList, inference, type = "Temporal")},
                      in_axes = list(NULL, 0L, NULL, NULL, NULL, NULL),
                      axis_name = batch_axis_name,
                      out_axes = list(0L,NULL))(ModelList, m,
                                                StateList, jnp$add(seed,42L), MPList, inference)
        StateList <- m[[2]]; m <- m[[1]]
      }
      return( list(m, StateList) )
    })
  }

  if(is.null(InitImageProcess)){
    InitImageProcess <- (function(im, seed, inference =  F){ return( im  ) })
  }

  Representations <- NULL; if(getRepresentations){
  Representations <- matrix(NA,nrow = length(unique(imageKeysOfUnits)), 
                            ncol = ifelse(optimizeImageRep, yes = nWidth_ImageRep, 
                                                            no = nWidth_ImageRep+nWidth_ImageRep*(imageModelClass=="CNN") ))
  usedImageKeys <- c(); last_i <- 0; ok_counter <- 0; ok<-F; while(!ok){
      ok_counter <- ok_counter + 1

      batch_indices <- (last_i+1):(last_i+batchSize)
      batch_indices <- batch_indices[batch_indices <= length(unique(imageKeysOfUnits))]
      last_i <- batch_indices[ length(batch_indices) ]

      # checks for last / batch size corrections
      if(last_i == length(unique(imageKeysOfUnits))){ ok <- T }
      batchSizeOneCorrection <- F; if(length(batch_indices) == 1){ batchSizeOneCorrection <- T }

      # get the data
      setwd(orig_wd); batch_inference <- GetElementFromTfRecordAtIndices( uniqueKeyIndices = batch_indices,
                                                            filename = file,
                                                            nObs = length(unique(imageKeysOfUnits)),
                                                            return_iterator = T,
                                                            readVideo = useVideo,
                                                            image_dtype = image_dtype_tf,
                                                            iterator = ifelse(ok_counter > 1,
                                                                              yes = list(saved_iterator),
                                                                              no = list(NULL))[[1]] ); setwd(new_wd)
      if(batchSizeOneCorrection){
          batch_inference[[1]][[1]] <- tf$concat(list(tf$expand_dims(batch_inference[[1]][[1]],0L),
                                                      tf$expand_dims(batch_inference[[1]][[1]],0L)), 0L)
      }
      saved_iterator <- batch_inference[[2]]
      batch_inference <- batch_inference[[1]]
      batch_keys <- unlist(  lapply( p2l(batch_inference[[3]]$numpy()), as.character) )

      gc(); try(py_gc$collect(), T) # collect memory
      #representation_ <- try( np$array( ImageRepArm_batch_R(ModelList, # uncomment for debugging
      representation_ <- try( np$array( ImageRepArm_batch(ModelList,
                                                          InitImageProcess(jnp$array(batch_inference[[1]]),
                                                                           jax$random$PRNGKey(ai(2L+ok_counter + seed)), inference = T),
                                                          StateList,
                                                          jax$random$PRNGKey(ai(last_i + seed)),
                                                          MPList, 
                                                          T # inference 
                                                          )[[1]]  ), T)
      # plot(representation_)
      # hist(as.matrix(representation_)); apply(as.matrix(representation_),2,sd)
      # plot(representation_[,sample(1:20)])
      # plot(representation_[1,],np$array(LE(ModelList,"SpatialTransformerSupp")[[1]]))
      # plot(representation_[sample(1:4,1),],np$array(LE(ModelList,"TemporalTransformerSupp")[[1]]))
      if("try-error" %in% class(representation_)){ browser() }
      if(batchSizeOneCorrection){ representation_ <- representation_[1,] }
      usedImageKeys <- c(usedImageKeys, batch_keys)
      Representations[batch_indices,] <- representation_
      print2(sprintf("%.2f%% done getting image/video representations", 100*last_i / length(unique(imageKeysOfUnits))))
  }
  Representations <- Representations[match(imageKeysOfUnits,usedImageKeys),]
  print2(sprintf("Done getting image/video representations!"))
  }

  # reset wd (may have been changed via tfrecords use)
  setwd(  orig_wd  )

  ImageModel_And_State_And_MPPolicy_List = list(ModelList, StateList, MPList);
  rm(ModelList, StateList, MPList); gc()

  if(returnContents){
   return( list( "ImageRepresentations"= Representations,
                 "ImageRepArm_batch_R" = ImageRepArm_batch_R,
                 "ImageRepArm_batch" = ImageRepArm_batch,
                 "ImageModel_And_State_And_MPPolicy_List" = ImageModel_And_State_And_MPPolicy_List ) )
  }
}
