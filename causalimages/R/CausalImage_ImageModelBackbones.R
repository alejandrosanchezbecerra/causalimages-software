#!/usr/bin/env Rscript
#' Generates image and video representations useful in earth observation tasks for casual inference.
#'
#' Generates image and video representations useful in earth observation tasks for casual inference, following the approach in Rolf, Esther, et al.  (2021).
#'
#' @param file Path to a tfrecord file generated by `causalimages::WriteTfRecord`.
#' @param imageKeysOfUnits A vector of length `length(imageKeysOfUnits)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into the package to call images into memory.
#' @param conda_env A `conda` environment where computational environment lives, usually created via `causalimages::BuildBackend()`. Default = `"CausalImagesEnv"`
#' @param conda_env_required A Boolean stating whether use of the specified conda environment is required.
#' @param kernelSize Dimensif(ions used in the convolution kernels.
#' @param temporalKernelSize Dimensions used in the temporal part of the convolution kernels if using image sequences.
#' @param nWidth_ImageRep Number of embedding features output.
#' @param strides  Integer specifying the strides used in the convolutional layers.
#' @param InitImageProcess (default = `NULL`) Initial image processing function. Usually left `NULL`.
#' @param batchSize Integer specifying batch size in obtaining representations.
#' @param dataType String specifying whether to assume `"image"` or `"video"` data types. Default is `"image"`.
#' @param TfRecords_BufferScaler The buffer size used in `tfrecords` mode is `batchSize*TfRecords_BufferScaler`. Lower `TfRecords_BufferScaler` towards 1 if out-of-memory problems.
#' @param seed Integer specifying the seed for pseudo random number generation.
#'
#' @return A list containing two items:
#' \itemize{
#' \item `Representations` (matrix) A matrix containing image/video representations, with rows corresponding to observations.
#' \item `ImageRepArm_OneObs,ImageRepArm_batch_R, ImageRepArm_batch` (functions) Image modeling functions.
#' \item `ImageModel_And_State_And_MPPolicy_List` List containing image model parameters fed into functions.
#' }
#'
#' @section References:
#' \itemize{
#' \item Rolf, Esther, et al. "A generalizable and accessible approach to machine learning with global satellite imagery." *Nature Communications* 12.1 (2021): 4392.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @export
#' @md


GetImageRepresentations <- function(
    imageKeysOfUnits = NULL,
    file = NULL,
    conda_env = "CausalImagesEnv",
    conda_env_required = T,
    returnContents = T,
    getRepresentations = T,

    InitImageProcess = NULL,
    nWidth_ImageRep = 64L,
    nDepth_ImageRep = 1L,
    batchSize = 16L,
    strides = 1L,
    temporalKernelSize = 2L,
    kernelSize = 3L,
    TfRecords_BufferScaler = 10L,
    dataType = "image",
    bn_momentum = 0.9,
    bn_epsilon = 0.01,
    inputAvePoolingSize = 1L, # set > 1L if seeking to downshift the image resolution
    seed = NULL){

  # initialize tensorflow if not already initialized
  if(   !"logical" %in% class(try(as.numeric(np$array(jnp$square(1.)))==1,T))  | !all(c("gc", "jax", "numpy", "numpy", "jax.numpy", "jmp", "optax", "equinox") %in% ls()) ){
    print2("Establishing connection to computational environment (build via causalimages::BuildBackend())")
    library(tensorflow);
    if(!is.null(conda_env)){
      try(tensorflow::use_condaenv(conda_env, required = conda_env_required),T)
    }
    Sys.sleep(1.); try(tf$square(1.),T); Sys.sleep(1.);

    # import python garbage collectors
    py_gc <- reticulate::import("gc")
    jax <<- reticulate::import("jax")
    np <<- reticulate::import("numpy")
    jnp <<- reticulate::import("jax.numpy")
    jmp <<- reticulate::import("jmp")
    optax <<- reticulate::import("optax")
    eq <<- reticulate::import("equinox")
    jax$config$update("jax_enable_x64", FALSE);
  }
  gc(); try(py_gc$collect(), T)
  if(is.null(seed)){ seed <- as.integer(runif(1,1,10000)) }

  # image dtype
  image_dtype <- jnp$float16
  image_dtype_tf <- tf$float16

  if( batchSize > length( unique(imageKeysOfUnits) )){
    batchSize <- length( unique(imageKeysOfUnits)  )
  }

  # define base tf record + train/test fxns
  orig_wd <- getwd()
  if(  !is.null(  file  )  ){
    # established tfrecord connection
    tf_record_name <- file
    if( !grepl(tf_record_name, pattern = "/") ){
      tf_record_name <- paste("./",tf_record_name, sep = "")
    }
    tf_record_name <- strsplit(tf_record_name,split="/")[[1]]
    setwd( new_wd <- paste(tf_record_name[-length(tf_record_name)],collapse = "/") )
    tf_dataset = tf$data$TFRecordDataset(  tf_record_name[length(tf_record_name)] )

    # helper functions
    useVideo <- (dataType == "video")
    getParsed_tf_dataset_inference <- function(tf_dataset){
        dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo, image_dtype = image_dtype_tf)} ) # return
      return( dataset <- dataset$batch( as.integer(max(2L, round(batchSize/2L)  ))) )
    }

    getParsed_tf_dataset_train <- function(tf_dataset){
      dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo, image_dtype = image_dtype_tf)} ) # return
      dataset <- dataset$shuffle(tf$constant(as.integer(TfRecords_BufferScaler*batchSize), dtype=tf$int64),
                                 reshuffle_each_iteration = T)
      dataset <- dataset$batch(as.integer(batchSize))
    }

    # setup iterators
    tf_dataset_train <- getParsed_tf_dataset_train( tf_dataset )
    tf_dataset_inference <- getParsed_tf_dataset_inference( tf_dataset )

    # reset iterators
    ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
    ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )
  }

  # acquire image
  {
    setwd(orig_wd); test_ <- tf$expand_dims(GetElementFromTfRecordAtIndices( uniqueKeyIndices = 1L,
                                                             filename = file,
                                                             readVideo = useVideo,
                                                             image_dtype = image_dtype_tf,
                                                             nObs = length(unique(imageKeysOfUnits)))[[1]],0L); setwd(new_wd)
  }
  imageDims <- ai( length( dim(test_) ) - 2L )
  RawChannelDims <- ai( dim(test_)[length(dim(test_))] )

  # setup jax model
  {
    print2("Setting up image representation model...")
    MPList <- list(jmp$Policy(compute_dtype="float16", param_dtype="float32", output_dtype="float32"),
                   jmp$DynamicLossScale(jnp$array(2^15), period = 1000L))

    batch_axis_name <- "batch";
    if(!"bn_momentum" %in% ls()){ bn_momentum <- 0.90 }
    StateList <- ModelList <- replicate(nDepth_ImageRep, list())
    for(d_ in 1L:nDepth_ImageRep){
      if(d_ > 1){ strides <- 1L }
      SeperableSpatial_jax <- eq$nn$Conv(kernel_size = c(kernelSize, kernelSize),
                                         num_spatial_dims = 2L,stride = c(strides,strides),
                                         out_channels = (dimsSpatial <- ifelse(d_ == 1, yes = RawChannelDims, no = nWidth_ImageRep)),
                                         groups = dimsSpatial,
                                         in_channels = dimsSpatial,
                                         key = jax$random$PRNGKey(4L+d_+seed))
      SeperableFeature_jax <- eq$nn$Conv(out_channels = nWidth_ImageRep, kernel_size = c(1L,1L),
                                         num_spatial_dims = 2L,stride = c(1L,1L),
                                         in_channels = dimsSpatial, key = jax$random$PRNGKey(5L+d_+seed))

      # reset weights with Xavier/Glorot
      SeperableSpatial_jax <- eq$tree_at(function(l){l$weight}, SeperableSpatial_jax,
                                         jax$random$uniform(key=jax$random$PRNGKey(5L+d_+seed),
                                                            minval = -sqrt(6/(dimsSpatial+dimsSpatial)),
                                                            maxval = sqrt(6/(dimsSpatial+dimsSpatial)),
                                                            shape = jax$tree_util$tree_leaves(SeperableSpatial_jax)[[1]]$shape) )
      SeperableFeature_jax <- eq$tree_at(function(l){l$weight}, SeperableFeature_jax,
                                         jax$random$uniform(key=jax$random$PRNGKey(45L+d_+seed),
                                                            minval = -sqrt(6/(dimsSpatial+nWidth_ImageRep)),
                                                            maxval = sqrt(6/(dimsSpatial+nWidth_ImageRep)),
                                                            shape = jax$tree_util$tree_leaves(SeperableFeature_jax)[[1]]$shape))
      SeperableTemporal_jax <- jnp$array(0.); if(dataType == "video"){
        if(T == F){
          SeperableTemporal_jax <- eq$nn$Conv(out_channels = nWidth_ImageRep, num_spatial_dims = 3L,
                                                  kernel_size = c(temporalKernelSize,1L,1L),
                                                  stride = c(1L,1L,1L),
                                                  in_channels = nWidth_ImageRep,
                                                  key = jax$random$PRNGKey(43L+d_+seed) )
        }
        SeperableTemporal_jax <- eq$nn$Conv(out_channels = nWidth_ImageRep, # only num_spatial_dims = 2L supported on GPU
                                            kernel_size = c(temporalKernelSize,1L),
                                            num_spatial_dims = 2L, stride = c(1L, 1L),
                                            in_channels = nWidth_ImageRep,
                                            key = jax$random$PRNGKey(43L+d_+seed) )
        SeperableTemporal_jax <- eq$tree_at(function(l){l$weight}, SeperableTemporal_jax,
                                            jax$random$uniform(key=jax$random$PRNGKey(62L+d_+seed),
                                                               minval = -sqrt(6/(nWidth_ImageRep+nWidth_ImageRep)),
                                                               maxval = sqrt(6/(nWidth_ImageRep+nWidth_ImageRep)),
                                                               shape = jax$tree_util$tree_leaves(SeperableTemporal_jax)[[1]]$shape))
      }

      # setup bn
      LayerBN <- eq$nn$BatchNorm(
        input_size = nWidth_ImageRep,
        axis_name = batch_axis_name,
        momentum = bn_momentum, eps = 0.001, channelwise_affine = T)
      StateList[[d_]] <- eval(parse(text = sprintf("list('BNState_ImRep_d%s'=eq$nn$State( LayerBN ))", d_)))
      ModelList[[d_]] <- eval(parse(text = sprintf('list("SeperableSpatial_jax_d%s" = SeperableSpatial_jax,
                                "SeperableFeature_jax_d%s" = SeperableFeature_jax,
                                "SeperableTemporal_jax_d%s" = SeperableTemporal_jax,
                                "BN_ImRep_d%s" = LayerBN)', d_, d_, d_, d_ )))
    }
    if(dataType == "video"){
      ModelList[[nDepth_ImageRep+1]] <- list("MultiheadAttn"=  eq$nn$MultiheadAttention(
                                                               query_size = nWidth_ImageRep,
                                                               output_size = nWidth_ImageRep,
                                                               num_heads = 3L,
                                                               use_output_bias = F,
                                                               key = jax$random$PRNGKey( 23453355L + seed) ) )
    }

    # zzz
    # m <- jax_array4d <- jnp$array( tf$gather(batch_inference[[1]],0L,0L));  d__ <- 1L
    ImageRepArm_OneObs <- ( function(ModelList, m, StateList, MPList, inference){
      ModelList <- MPList[[1]]$cast_to_compute( ModelList )
      StateList <- MPList[[1]]$cast_to_compute( StateList )

      # transpose to C,T,W,H
      if(dataType == "image"){  m <- jnp$transpose(m, c(2L, 0L, 1L)) }
      if(dataType == "video"){  m <- jnp$transpose(m, c(3L, 0L, 1L, 2L)) }
      for(d__ in 1:nDepth_ImageRep){
        if(dataType == "image"){ # spatial block
          m <-  LE(ModelList,sprintf("SeperableFeature_jax_d%s",d__)) (
                    LE(ModelList,sprintf("SeperableSpatial_jax_d%s",d__))(m) )
        }

        if(dataType == "video"){
          # spatial block
          m <- jax$vmap(function(ModelList, m){
            LE(ModelList,sprintf("SeperableFeature_jax_d%s",d__)) (
              LE(ModelList,sprintf("SeperableSpatial_jax_d%s",d__))(m) )
          }, in_axes = list(NULL, 1L), out_axes = 1L)( ModelList, m )

          # temporal block
          ## fails on METAL backend
          # m <- LE(ModelList, sprintf("SeperableTemporal_jax_d%s", d__))( m )

          ## succeeds on GPU
          orig_shape <- m$shape
          m <- jnp$expand_dims(jnp$transpose(jnp$reshape(m, c(m$shape[1:2], -1L)), c(2L, 0L, 1L)),3L)
          m <- jax$vmap(function(tm){ LE(ModelList, sprintf("SeperableTemporal_jax_d%s", d__))( tm ) }, 0L)(  m  )
          m <- jnp$transpose(jnp$squeeze(m,3L), c(1L, 2L, 0L))
          m <- jnp$reshape(m, c(m$shape[1:2], orig_shape[3:4]))
        }

        # batchnorm block
        if( nDepth_ImageRep > 1 ){
          m <- LE(ModelList, sprintf("BN_ImRep_d%s",d__))(m, state = LE(StateList, sprintf("BNState_ImRep_d%s",d__)), inference = inference)
          StateIndex <- LE_index( StateList, sprintf("BNState_ImRep_d%s",d__) )
          StateIndex <- paste(sapply(StateIndex, function(zer){ paste("[[", zer, "]]") }), collapse = "")
          eval(parse(text = sprintf("StateList%s <- m[[2]]", StateIndex))); m <- m[[1]]

          # act fxn block
          m <- jax$nn$swish( m )
        }

        if(dataType == "image"){
          m <- eq$nn$AdaptiveMaxPool2d(list(ai(m$shape[[2]]/2), ai(m$shape[[3]]/2)))( m ) # succeeds on METAL backend
        }
        # note: MaxPool3D and vmapped MaxPool2D( m ) fail on METAL backend; MaxPool2D fails on Metal backend
        if(dataType == "video"){
            m_orig <- m$shape
            m <- jnp$reshape(m, c(-1L, m$shape[3:4]))
            # m <- MaxPool2D(  m ) fails with METAL backend
            m <- eq$nn$AdaptiveMaxPool2d(list(ai(m$shape[[2]]/2), ai(m$shape[[3]]/2)))( m )
            m <- jnp$reshape(m, c(m_orig[1:2],m$shape[2:3]))
        }
      }

      if(dataType == "image"){ m <- jnp$max(m, c(1L:2L))  } # # spatial pooling only

      if(dataType == "video"){
        # spatial pooling
        m <- jnp$max(m, c(2L:3L))

        # temporal pooling
        m <- jnp$transpose(m, c(1L,0L))
        {
          swap_m <- jnp$zeros_like( m )
          for( idx in seq(0L, nWidth_ImageRep, by = 2L)){
            swap_m <- swap_m$at[,jnp$array(idx)]$add( jnp$take(m, idx+1L, axis = 1L) )
            swap_m <- swap_m$at[,jnp$array(idx+1L)]$add( jnp$take(m, idx, axis = 1L) )
          }
          position <- jnp$expand_dims(jnp$arange(0, m$shape[[1]]), 1L)
          theta_vals <- jnp$expand_dims( 10000^((-2*jnp$arange(1,nWidth_ImageRep+1L)  - 1)/nWidth_ImageRep), 0L)
          cos_terms <- MPList[[1]]$cast_to_compute( jnp$cos( m_theta <- (position *  theta_vals) ))
          sin_terms <- MPList[[1]]$cast_to_compute( jnp$sin( m_theta ) )
          m <- m*cos_terms + swap_m*sin_terms
          m <- LE( ModelList, "MultiheadAttn" )(query = swap_m, key_ = swap_m, value = m, inference = inference)
        }
        m <- jnp$mean( m, 0L)
      }
      return( list(m, StateList) )
    })
    ImageRepArm_batch <- eq$filter_jit( ImageRepArm_batch_R <- jax$vmap(
      function(ModelList, m, StateList, MPPolicy, inference){
        ImageRepArm_OneObs(ModelList, m, StateList, MPPolicy, inference)
      }, in_axes = list(NULL, 0L, NULL, NULL, NULL),
      out_axes = list(0L, NULL),
      axis_name = batch_axis_name) )
  }

  if(is.null(InitImageProcess)){
    InitImageProcess <- (function(im, training = F){
      if(inputAvePoolingSize > 1){ im <- AvePoolingDownshift(im) }
      return( im  )
    })
  }

  Representations <- NULL; if(getRepresentations){
  Representations <- matrix(NA,nrow = length(unique(imageKeysOfUnits)), ncol = nWidth_ImageRep)
  usedImageKeys <- c(); last_i <- 0; ok_counter <- 0; ok<-F; while(!ok){
      ok_counter <- ok_counter + 1

      batch_indices <- (last_i+1):(last_i+batchSize)
      batch_indices <- batch_indices[batch_indices <= length(unique(imageKeysOfUnits))]
      last_i <- batch_indices[length(batch_indices)]

      # checks for last / batch size corrections
      if(last_i == length(unique(imageKeysOfUnits))){ ok <- T }
      batchSizeOneCorrection <- F; if(length(batch_indices) == 1){ batchSizeOneCorrection <- T }

      # get the data
      setwd(orig_wd); batch_inference <- GetElementFromTfRecordAtIndices( uniqueKeyIndices = batch_indices,
                                                            filename = file,
                                                            nObs = length(unique(imageKeysOfUnits)),
                                                            return_iterator = T,
                                                            readVideo = useVideo,
                                                            image_dtype = image_dtype_tf,
                                                            iterator = ifelse(ok_counter > 1,
                                                                              yes = list(saved_iterator),
                                                                              no = list(NULL))[[1]] ); setwd(new_wd)
      if(batchSizeOneCorrection){
          batch_inference[[1]][[1]] <- tf$concat(list(tf$expand_dims(batch_inference[[1]][[1]],0L),
                                                 tf$expand_dims(batch_inference[[1]][[1]],0L)), 0L)
      }
      saved_iterator <- batch_inference[[2]]
      batch_inference <- batch_inference[[1]]
      batch_keys <- unlist(  lapply( batch_inference[[3]]$numpy(), as.character) )

      gc(); try(py_gc$collect(), T) # collect memory
      representation_ <- try( np$array( ImageRepArm_batch(ModelList,
                                                          InitImageProcess( jnp$array( batch_inference[[1]]) ),
                                                          StateList, MPList, T)[[1]]  ), T)
      # hist(as.matrix(representation_)); apply(as.matrix(representation_),2,sd)
      if("try-error" %in% class(representation_)){ browser() }
      if(batchSizeOneCorrection){
          batch_indices <- batch_indices[-1];
          batch_keys <- batch_keys[-1];
          representation_ <- representation_[1,]
      }
      usedImageKeys <- c(usedImageKeys, batch_keys)
      Representations[batch_indices,] <- representation_
      print2(sprintf("%.2f%% done getting image/video representations", 100*last_i / length(unique(imageKeysOfUnits))))
  }
  Representations <- Representations[match(imageKeysOfUnits,usedImageKeys),]
  print2(sprintf("Done getting image/video representations!"))
  }

  # reset wd (may have been changed via tfrecords use)
  setwd(  orig_wd  )

  ImageModel_And_State_And_MPPolicy_List = list(ModelList, StateList, MPList);
  rm(ModelList,StateList,MPList)

  if(returnContents){
   return( list( "ImageRepresentations"= Representations,
                 "ImageRepArm_OneObs" = ImageRepArm_OneObs,
                 "ImageRepArm_batch_R" = ImageRepArm_batch_R,
                 "ImageRepArm_batch" = ImageRepArm_batch,
                 "ImageModel_And_State_And_MPPolicy_List" = ImageModel_And_State_And_MPPolicy_List ) )
  }
}
